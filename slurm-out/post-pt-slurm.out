06/24/2024 22:20:42 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/24/2024 22:20:42 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
[INFO|tokenization_utils_base.py:2082] 2024-06-24 22:20:42,178 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2082] 2024-06-24 22:20:42,178 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2082] 2024-06-24 22:20:42,178 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2082] 2024-06-24 22:20:42,178 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2082] 2024-06-24 22:20:42,178 >> loading file tokenizer_config.json
06/24/2024 22:20:42 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/24/2024 22:20:42 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/24/2024 22:20:42 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/24/2024 22:20:42 - INFO - llmtuner.hparams.parser - Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/24/2024 22:20:42 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/24/2024 22:20:42 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/24/2024 22:20:42 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/24/2024 22:20:42 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/24/2024 22:20:42 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/24/2024 22:20:42 - INFO - llmtuner.hparams.parser - Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/24/2024 22:20:43 - INFO - llmtuner.data.loader - Loading dataset post-pre-train-data.json...
06/24/2024 22:20:43 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
06/24/2024 22:21:00 - INFO - llmtuner.data.loader - Loading dataset post-pre-train-data.json...
06/24/2024 22:21:00 - INFO - llmtuner.data.loader - Loading dataset post-pre-train-data.json...
06/24/2024 22:21:00 - INFO - llmtuner.data.loader - Loading dataset post-pre-train-data.json...
06/24/2024 22:21:00 - INFO - llmtuner.data.loader - Loading dataset post-pre-train-data.json...
06/24/2024 22:21:00 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
06/24/2024 22:21:00 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
06/24/2024 22:21:00 - INFO - llmtuner.data.loader - Loading dataset post-pre-train-data.json...
06/24/2024 22:21:00 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
06/24/2024 22:21:00 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
06/24/2024 22:21:00 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
input_ids:
[15923, 10636, 59601, 12280, 1629, 55140, 23404, 59631, 144, 144, 15923, 33784, 59601, 13214, 22411, 14284, 1006, 59631, 144, 144, 15923, 14750, 59601, 960, 21187, 10970, 1830, 97, 7915, 46606, 597, 8799, 50084, 1289, 719, 815, 961, 5247, 1744, 8602, 651, 567, 12280, 1629, 3978, 3968, 97, 59638, 567, 18707, 4987, 1067, 98, 707, 2766, 1715, 593, 567, 5439, 7973, 29603, 717, 728, 1035, 21911, 592, 5407, 97, 796, 962, 42771, 651, 19143, 1328, 25044, 98, 2624, 567, 952, 4731, 97, 567, 2165, 620, 2160, 562, 6138, 4987, 639, 3986, 631, 7541, 594, 7216, 97, 20996, 597, 3688, 98, 8369, 4671, 2685, 6689, 33142, 97, 562, 2672, 7478, 632, 567, 1645, 597, 4944, 8788, 97, 597, 1074, 2894, 10958, 632, 567, 1999, 597, 1288, 29570, 593, 567, 3062, 98, 707, 1074, 59594, 942, 2876, 3091, 5883, 748, 629, 1316, 631, 3225, 3062, 98, 2, 15923, 10636, 59601, 1008, 80, 80, 59631, 144, 144, 15923, 33784, 59601, 12870, 28772, 59631, 144, 144, 15923, 14750, 59601, 707, 1008, 80, 80, 34803, 1915, 583, 2744, 5490, 17269, 1374, 2728, 597, 2743, 2875, 651, 562, 4506, 14818, 21228, 597, 5353, 5269, 5925, 5523, 12271, 98, 2174, 6992, 97, 2011, 2744, 14787, 597, 567, 7117, 593, 2744, 2920, 97, 748, 629, 5814, 737, 562, 2376, 1514, 98, 707, 14321, 59629, 59575, 1402, 994, 9480, 567, 2942, 594, 786, 932, 5126, 97, 878, 6556, 17691, 597, 13103, 567, 4930, 593, 562, 2744, 4032, 737, 2160, 567, 3239, 1621, 863, 6180, 98, 13821, 59594, 59481, 5730, 8795, 21611, 14964, 59594, 10994, 2743, 2875, 98, 707, 18707, 31940, 567, 23533, 1715, 97, 878, 40399, 786, 2953, 16341, 866, 4671, 594, 8948, 593, 2778, 98, 2, 15923, 10636, 59601, 15686, 44105, 47126, 59568, 82, 77, 77, 77, 59594, 83, 77, 77, 77, 597, 764, 84, 77, 77, 59594, 59613, 85, 77, 77, 10093, 38231, 5158, 59631, 144, 144, 15923, 33784, 59601, 19615, 59594, 14562, 18745, 1997, 59631, 144, 144, 15923, 14750, 59601, 707, 15686, 44105, 47126, 59568, 82, 77, 77, 77, 592, 59568, 83, 77, 77, 77, 10093, 597, 764, 84, 77, 77, 592, 764, 85, 77, 77, 10093, 41579, 678, 46545, 3124, 586, 5288, 631, 1768, 1149, 98, 1648, 1856, 645, 624, 831, 97, 16253, 59594, 3365, 4625, 44983, 828, 592, 59568, 78, 77, 594, 2361, 98, 707, 5288, 726, 562, 1374, 2837, 7143, 597, 25198, 6355, 639, 748, 1621, 8008, 4125, 98, 707, 18707, 717, 41546, 737, 567, 1715, 878, 620, 2676, 2057, 737, 663, 1596, 1499, 3350, 651, 14325, 17528, 97, 960, 18234, 719, 2317, 567, 41579, 663, 11613, 5597, 59594, 8530, 7230, 594, 567, 3831, 59638, 98, 1117, 620, 5854, 737, 567, 1502, 639, 567, 3091, 12394, 748, 629, 2948, 2057, 98, 1648, 748, 1224, 1376, 17528, 705, 1212, 2728, 36130, 59575, 639, 5204, 567, 9100, 7542, 98, 2, 15923, 10636, 59601, 3418, 21799, 5447, 767, 2836, 59568, 79, 77, 59620, 1864, 25697, 619, 6450, 59631, 144, 144, 15923, 33784, 59601, 1214, 1162, 597, 5610, 31027, 583, 11037, 59631, 144, 144, 15923, 14750, 59601, 707, 3418, 21799, 5447, 767, 2836, 1864, 25697, 619, 6450, 620, 2676, 2057, 737, 15658, 9842, 597, 21686, 13913, 639, 26836, 2674, 567, 1774, 651, 53730, 1253, 659, 810, 45203, 597, 46240, 98, 707, 2127, 10131, 620, 2616, 592, 5628, 97, 5867, 592, 567, 2014, 20691, 632, 567, 2266, 97, 597, 748, 629, 37593, 1267, 1480, 592, 567, 1999, 597, 592, 567, 1288, 98, 6016, 592, 663, 26106, 97, 8584, 46275, 3379, 562, 9051, 36198, 620, 962, 2117, 98, 4611, 567, 1122, 1269, 728, 1049, 592, 629, 11458, 97, 567, 4593, 748, 629, 11955, 4074, 1877, 567, 4274, 592, 4441, 2176, 98, 707, 18707, 717, 2955, 651, 567, 960, 649, 614, 1616, 1715, 597, 567, 2942, 59594, 14655, 4731, 593, 37593, 3948, 567, 2127, 10131, 59638, 98, 2, 15923, 10636, 59601, 601, 60464, 1765, 1852, 22649, 45275, 631, 9722, 984, 45688, 22556, 59631, 144, 144, 15923, 33784, 59601, 9936, 3706, 730, 689, 59631, 144, 144, 15923, 14750, 59601, 707, 1715, 597, 9658, 583, 593, 567, 601, 60464, 1765, 1852, 22649, 45275, 631, 9722, 984, 45688, 22556, 46508, 786, 2732, 592, 567, 1376, 15835, 885, 593, 567, 7616, 13369, 98, 4055, 853, 620, 12183, 2876, 594, 9658, 97, 7534, 2365, 37833, 9796, 7516, 97, 567, 1064, 874, 2864, 717, 4060, 594, 5553, 592, 37194, 16933, 584, 9796, 98, 1117, 40049, 962, 17269, 567, 10546, 42449, 10853, 593, 9796, 18194, 592, 4414, 567, 13369, 98, 707, 1715, 620, 2084, 632, 5583, 6837, 966, 744, 1463, 98, 960, 1263, 601, 60464, 1765, 1852, 22649, 45275, 631, 9722, 984, 45688, 22556, 46508, 2431, 562, 14652, 3195, 97, 5867, 592, 881, 2920, 97, 59638, 1067, 567, 18707, 98, 960, 7998, 748, 962, 629, 1483, 2940, 659, 2347, 659, 9330, 705, 6084, 659, 39253, 5887, 98, 59638, 2, 15923, 10636, 59601, 14759, 28079, 4589, 17752, 59631, 144, 144, 15923, 33784, 59601, 9536, 35539, 25331, 59631, 144, 144, 15923, 14750, 59601, 707, 29719, 14759, 28079, 4589, 17752, 27196, 4318, 2096, 24169, 592, 2792, 567, 43082, 98, 707, 1961, 4539, 620, 1067, 592, 3163, 567, 43082, 3247, 97, 5478, 4973, 28945, 97, 3364, 567, 5043, 15517, 6347, 592, 567, 43082, 14590, 597, 36396, 3205, 5218, 97, 15960, 12177, 39489, 567, 4588, 4098, 7603, 98, 707, 27196, 962, 3849, 2672, 97, 29719, 11705, 4414, 1006, 651, 9855, 26241, 27155, 98, 707, 18707, 717, 14611, 737, 567, 960, 30957, 3249, 97, 18717, 27736, 1164, 597, 6981, 21976, 3912, 631, 1374, 2942, 4273, 59638, 98, 2, 15923, 10636, 59601, 752, 59660, 78, 59631, 144, 144, 15923, 33784, 59601, 25071, 3102, 20113, 59585, 59631, 144, 144, 15923, 14750, 59601, 752, 59660, 78, 620, 562, 46545, 3124, 586, 97, 25265, 11914, 21928, 651, 9855, 53312, 606, 97, 20840, 36715, 568, 97, 29407, 38616, 597, 17537, 40131, 937, 4170, 1491, 98, 707, 3091, 5054, 593, 567, 21928, 11278, 659, 562, 2324, 4598, 651, 34750, 584, 97, 51966, 1438, 15371, 98, 707, 2706, 4671, 678, 19465, 632, 597, 965, 737, 19900, 567, 4600, 7534, 632, 567, 21928, 98, 707, 29407, 38616, 4155, 3722, 3912, 702, 719, 1330, 97, 631, 2107, 567, 34528, 922, 98, 707, 18707, 31940, 567, 1186, 837, 5369, 3095, 597, 9203, 1715, 97, 567, 1374, 13314, 1830]
inputs:
Product Name: Biamp Rack Products;

Product Category: Digital Audio Processors;

Product Description: “High recognition value, uniform aesthetics and practical scalability – this has been impressively achieved with the Biamp brand language,” the jury statement said. The previous design of the digital audio processors was not only costly to produce, but also incompatible with newer system architectures. With the new concept, the company is making a visual statement that allows for differences in dimension, connectivity and application. Design elements include consistent branding, a soft curve on the top and bottom edges, and two red bars on the left and right margins of the products. The two-part black front panel can be used for various products.<|endoftext|>Product Name: V33;

Product Category: Video Camera;

Product Description: The V33 livestreaming video camera ensures high image and sound quality with a tenfold zoom and eight linear array microphones. All tasks, including video editing and the sharing of video material, can be performed by a single person. The manufacturer’s own app supports the user in all work steps, which increases productivity and reduces the costs of a video production by making the learning process more efficient. AI-assisted noise reduction guarantees crystal-clear sound quality. The jury praised the simplified design, which eliminates all superfluous elements in favour of performance.<|endoftext|>Product Name: HP LaserJet 5000-6000 and E700-E800 Series MFPs;

Product Category: Multi-Function Printers;

Product Description: The HP LaserJet 5000 to 6000 Series and E700 to E800 Series printers are multifunctional devices for business use. They feature stowable, tablet-like touchscreens up to 10 in size. The devices have a high paper capacity and scanning units that can process documents quickly. The jury was fascinated by the design which is characterised by an edgy style with bold colours, “because this makes the printers an attractive eye-catcher in the office”. This is helped by the fact that the front panels can be personalised. They can show different colours or even image motifs that reflect the corporate identity.<|endoftext|>Product Name: Meaco Arete One 20L Dehumidifier;

Product Category: Heating and Air Conditioning Technology;

Product Description: The Meaco Arete One Dehumidifier is characterised by gentle curves and matt surfaces that visually connect the product with furnishings such as sofas and curtains. The water tank is easy to remove, thanks to the large handles on the side, and can be emptied both to the left and to the right. Thanks to an adapter, continuous drainage via a garden hose is also possible. Since the back does not need to be accessible, the unit can be pushed directly against the wall to save space. The jury was taken with the “seamless design and the user-friendly concept of emptying the water tank”.<|endoftext|>Product Name: théATRE Glass Container for Loose Leaf Tea;

Product Category: Food Containers;

Product Description: The design and colouring of the théATRE Glass Container for Loose Leaf Tea jars allude to the different manifestations of the intended contents. While one is jet black in colour, symbolising roasted tea leaves, the amber version was designed in reference to freshly brewed tea. This differentiation also ensures the crucial opacity characteristics of tea packaging to protect the contents. The design is based on traditional Chinese teapots. “The théATRE Glass Container for Loose Leaf Tea jars represent a sustainable solution, thanks to their material,” said the jury. “They can also be refilled as often as desired or serve as decorative objects.”<|endoftext|>Product Name: LED Hair Regrowth;

Product Category: Light Therapy Device;

Product Description: The adjustable LED Hair Regrowth helmet uses phototherapy to treat the scalp. The light treatment is said to improve the scalp environment, reduce skin inflammation, increase the overall oxygen supply to the scalp tissue and stimulate cell activity, thereby reactivating the hair growth cycle. The helmet also includes soft, adjustable ear protectors with integrated Bluetooth headphones. The jury was impressed by the “exciting, futuristic look and variable adjustment options for high user comfort”.<|endoftext|>Product Name: N+1;

Product Category: Bathroom Shelf;

Product Description: N+1 is a multifunctional, modular bathroom shelf with integrated faucet, soap dispenser, towel dryer and toothbrush disinfection. The front edge of the shelf serves as a control element with recessed, illuminated function buttons. The individual elements are switched on and off by pressing the corresponding symbol on the shelf. The towel dryer offers additional options at this point, for example the drying time. The jury praised the emphatically clear and functional design, the high utility value
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
[INFO|configuration_utils.py:724] 2024-06-24 22:21:10,717 >> loading configuration file Yi-9B/config.json
[INFO|configuration_utils.py:789] 2024-06-24 22:21:10,718 >> Model config LlamaConfig {
  "_name_or_path": "Yi-9B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 48,
  "num_key_value_heads": 4,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 64000
}

The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
[INFO|modeling_utils.py:3280] 2024-06-24 22:21:13,225 >> loading weights file Yi-9B/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-06-24 22:21:13,276 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-06-24 22:21:13,277 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:49<01:49, 109.51s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:49<01:49, 109.78s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:49<01:49, 109.89s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:49<01:49, 109.91s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:49<01:49, 109.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:49<01:49, 109.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [03:13<00:00, 94.37s/it] Loading checkpoint shards: 100%|██████████| 2/2 [03:13<00:00, 96.65s/it]
06/24/2024 22:24:28 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.
06/24/2024 22:24:28 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.
06/24/2024 22:24:28 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
06/24/2024 22:24:29 - INFO - llmtuner.model.loader - trainable params: 2457600 || all params: 8831864832 || trainable%: 0.0278
Loading checkpoint shards: 100%|██████████| 2/2 [03:14<00:00, 94.78s/it] Loading checkpoint shards: 100%|██████████| 2/2 [03:14<00:00, 97.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [03:14<00:00, 94.77s/it] Loading checkpoint shards: 100%|██████████| 2/2 [03:14<00:00, 97.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [03:14<00:00, 94.80s/it] Loading checkpoint shards: 100%|██████████| 2/2 [03:14<00:00, 97.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [03:14<00:00, 94.80s/it] Loading checkpoint shards: 100%|██████████| 2/2 [03:14<00:00, 97.07s/it]
06/24/2024 22:24:29 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.
06/24/2024 22:24:29 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.
06/24/2024 22:24:29 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [03:14<00:00, 94.80s/it] Loading checkpoint shards: 100%|██████████| 2/2 [03:14<00:00, 97.07s/it]
[INFO|modeling_utils.py:4024] 2024-06-24 22:24:29,131 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-06-24 22:24:29,131 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at Yi-9B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
06/24/2024 22:24:29 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.
06/24/2024 22:24:29 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.
06/24/2024 22:24:29 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
[INFO|configuration_utils.py:881] 2024-06-24 22:24:29,156 >> loading configuration file Yi-9B/generation_config.json
[INFO|configuration_utils.py:928] 2024-06-24 22:24:29,156 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

06/24/2024 22:24:29 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.
06/24/2024 22:24:29 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.
06/24/2024 22:24:29 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
06/24/2024 22:24:29 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.
06/24/2024 22:24:29 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.
06/24/2024 22:24:29 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
06/24/2024 22:24:29 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.
06/24/2024 22:24:29 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.
06/24/2024 22:24:29 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
06/24/2024 22:24:29 - INFO - llmtuner.model.loader - trainable params: 2457600 || all params: 8831864832 || trainable%: 0.0278
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
06/24/2024 22:24:29 - INFO - llmtuner.model.loader - trainable params: 2457600 || all params: 8831864832 || trainable%: 0.0278
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
06/24/2024 22:24:29 - INFO - llmtuner.model.loader - trainable params: 2457600 || all params: 8831864832 || trainable%: 0.0278
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
06/24/2024 22:24:29 - INFO - llmtuner.model.loader - trainable params: 2457600 || all params: 8831864832 || trainable%: 0.0278
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
06/24/2024 22:24:29 - INFO - llmtuner.model.loader - trainable params: 2457600 || all params: 8831864832 || trainable%: 0.0278
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:607] 2024-06-24 22:24:29,338 >> Using auto half precision backend
[INFO|trainer.py:1969] 2024-06-24 22:24:32,559 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-06-24 22:24:32,560 >>   Num examples = 4,952
[INFO|trainer.py:1971] 2024-06-24 22:24:32,560 >>   Num Epochs = 5
[INFO|trainer.py:1972] 2024-06-24 22:24:32,560 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-06-24 22:24:32,560 >>   Total train batch size (w. parallel, distributed & accumulation) = 12
[INFO|trainer.py:1976] 2024-06-24 22:24:32,560 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1977] 2024-06-24 22:24:32,560 >>   Total optimization steps = 2,065
[INFO|trainer.py:1978] 2024-06-24 22:24:32,563 >>   Number of trainable parameters = 2,457,600
  0%|          | 0/2065 [00:00<?, ?it/s]  0%|          | 1/2065 [00:14<8:22:11, 14.60s/it]  0%|          | 2/2065 [00:17<4:29:32,  7.84s/it]  0%|          | 3/2065 [00:20<3:11:25,  5.57s/it]  0%|          | 4/2065 [00:23<2:36:13,  4.55s/it]  0%|          | 5/2065 [00:26<2:16:37,  3.98s/it]  0%|          | 6/2065 [00:29<2:04:10,  3.62s/it]  0%|          | 7/2065 [00:32<1:58:00,  3.44s/it]  0%|          | 8/2065 [00:35<1:55:53,  3.38s/it]  0%|          | 9/2065 [00:38<1:51:06,  3.24s/it]  0%|          | 10/2065 [00:41<1:48:06,  3.16s/it]                                                   {'loss': 2.5928, 'grad_norm': 0.794397234916687, 'learning_rate': 1e-05, 'epoch': 0.02}
  0%|          | 10/2065 [00:41<1:48:06,  3.16s/it]  1%|          | 11/2065 [00:44<1:46:14,  3.10s/it]  1%|          | 12/2065 [00:47<1:44:09,  3.04s/it]  1%|          | 13/2065 [00:50<1:43:40,  3.03s/it]  1%|          | 14/2065 [00:53<1:43:06,  3.02s/it]  1%|          | 15/2065 [00:56<1:46:17,  3.11s/it]  1%|          | 16/2065 [01:00<1:46:34,  3.12s/it]  1%|          | 17/2065 [01:03<1:45:39,  3.10s/it]  1%|          | 18/2065 [01:06<1:44:31,  3.06s/it]  1%|          | 19/2065 [01:08<1:42:53,  3.02s/it]  1%|          | 20/2065 [01:11<1:41:17,  2.97s/it]                                                   {'loss': 2.5987, 'grad_norm': 0.3025112450122833, 'learning_rate': 2e-05, 'epoch': 0.05}
  1%|          | 20/2065 [01:11<1:41:17,  2.97s/it]  1%|          | 21/2065 [01:14<1:41:42,  2.99s/it]  1%|          | 22/2065 [01:17<1:42:45,  3.02s/it]  1%|          | 23/2065 [01:21<1:43:14,  3.03s/it]  1%|          | 24/2065 [01:24<1:45:18,  3.10s/it]  1%|          | 25/2065 [01:27<1:44:46,  3.08s/it]  1%|▏         | 26/2065 [01:30<1:44:18,  3.07s/it]  1%|▏         | 27/2065 [01:33<1:44:20,  3.07s/it]  1%|▏         | 28/2065 [01:36<1:44:25,  3.08s/it]  1%|▏         | 29/2065 [01:39<1:44:28,  3.08s/it]  1%|▏         | 30/2065 [01:42<1:45:14,  3.10s/it]                                                   {'loss': 2.6021, 'grad_norm': 0.31718194484710693, 'learning_rate': 1.9998820020169668e-05, 'epoch': 0.07}
  1%|▏         | 30/2065 [01:42<1:45:14,  3.10s/it]  2%|▏         | 31/2065 [01:45<1:44:47,  3.09s/it]  2%|▏         | 32/2065 [01:49<1:46:30,  3.14s/it]  2%|▏         | 33/2065 [01:52<1:45:57,  3.13s/it]  2%|▏         | 34/2065 [01:55<1:45:23,  3.11s/it]  2%|▏         | 35/2065 [01:58<1:45:03,  3.11s/it]  2%|▏         | 36/2065 [02:01<1:44:55,  3.10s/it]  2%|▏         | 37/2065 [02:04<1:44:45,  3.10s/it]  2%|▏         | 38/2065 [02:07<1:45:12,  3.11s/it]  2%|▏         | 39/2065 [02:10<1:46:20,  3.15s/it]  2%|▏         | 40/2065 [02:14<1:46:32,  3.16s/it]                                                   {'loss': 2.5326, 'grad_norm': 0.3556591272354126, 'learning_rate': 1.999528035914915e-05, 'epoch': 0.1}
  2%|▏         | 40/2065 [02:14<1:46:32,  3.16s/it]  2%|▏         | 41/2065 [02:17<1:45:34,  3.13s/it]  2%|▏         | 42/2065 [02:20<1:45:43,  3.14s/it]  2%|▏         | 43/2065 [02:23<1:45:49,  3.14s/it]  2%|▏         | 44/2065 [02:26<1:45:32,  3.13s/it]  2%|▏         | 45/2065 [02:29<1:46:06,  3.15s/it]  2%|▏         | 46/2065 [02:32<1:45:45,  3.14s/it]  2%|▏         | 47/2065 [02:36<1:47:25,  3.19s/it]  2%|▏         | 48/2065 [02:39<1:46:31,  3.17s/it]  2%|▏         | 49/2065 [02:42<1:46:25,  3.17s/it]  2%|▏         | 50/2065 [02:45<1:47:57,  3.21s/it]                                                   {'loss': 2.5472, 'grad_norm': 0.48073089122772217, 'learning_rate': 1.9989381852284165e-05, 'epoch': 0.12}
  2%|▏         | 50/2065 [02:45<1:47:57,  3.21s/it]  2%|▏         | 51/2065 [02:48<1:47:50,  3.21s/it]  3%|▎         | 52/2065 [02:52<1:46:33,  3.18s/it]  3%|▎         | 53/2065 [02:55<1:46:20,  3.17s/it]  3%|▎         | 54/2065 [02:58<1:46:22,  3.17s/it]  3%|▎         | 55/2065 [03:01<1:46:15,  3.17s/it]  3%|▎         | 56/2065 [03:04<1:45:43,  3.16s/it]  3%|▎         | 57/2065 [03:07<1:44:59,  3.14s/it]  3%|▎         | 58/2065 [03:11<1:45:29,  3.15s/it]  3%|▎         | 59/2065 [03:14<1:45:08,  3.14s/it]  3%|▎         | 60/2065 [03:17<1:45:01,  3.14s/it]                                                   {'loss': 2.5426, 'grad_norm': 0.3548201024532318, 'learning_rate': 1.9981125891598545e-05, 'epoch': 0.15}
  3%|▎         | 60/2065 [03:17<1:45:01,  3.14s/it]  3%|▎         | 61/2065 [03:20<1:45:40,  3.16s/it]  3%|▎         | 62/2065 [03:23<1:44:58,  3.14s/it]  3%|▎         | 63/2065 [03:26<1:45:46,  3.17s/it]  3%|▎         | 64/2065 [03:29<1:45:46,  3.17s/it]  3%|▎         | 65/2065 [03:33<1:45:18,  3.16s/it]  3%|▎         | 66/2065 [03:36<1:45:18,  3.16s/it]  3%|▎         | 67/2065 [03:39<1:46:01,  3.18s/it]  3%|▎         | 68/2065 [03:42<1:45:46,  3.18s/it]  3%|▎         | 69/2065 [03:45<1:46:15,  3.19s/it]  3%|▎         | 70/2065 [03:49<1:45:38,  3.18s/it]                                                   {'loss': 2.5227, 'grad_norm': 0.3582150340080261, 'learning_rate': 1.9970514425465706e-05, 'epoch': 0.17}
  3%|▎         | 70/2065 [03:49<1:45:38,  3.18s/it]  3%|▎         | 71/2065 [03:52<1:47:00,  3.22s/it]  3%|▎         | 72/2065 [03:55<1:46:36,  3.21s/it]  4%|▎         | 73/2065 [03:58<1:45:31,  3.18s/it]  4%|▎         | 74/2065 [04:01<1:45:50,  3.19s/it]  4%|▎         | 75/2065 [04:05<1:45:19,  3.18s/it]  4%|▎         | 76/2065 [04:08<1:45:01,  3.17s/it]  4%|▎         | 77/2065 [04:11<1:44:24,  3.15s/it]  4%|▍         | 78/2065 [04:14<1:45:33,  3.19s/it]  4%|▍         | 79/2065 [04:17<1:45:53,  3.20s/it]  4%|▍         | 80/2065 [04:20<1:45:14,  3.18s/it]                                                   {'loss': 2.5344, 'grad_norm': 0.5028988718986511, 'learning_rate': 1.9957549958148844e-05, 'epoch': 0.19}
  4%|▍         | 80/2065 [04:20<1:45:14,  3.18s/it]  4%|▍         | 81/2065 [04:24<1:44:53,  3.17s/it]  4%|▍         | 82/2065 [04:27<1:45:03,  3.18s/it]  4%|▍         | 83/2065 [04:30<1:45:41,  3.20s/it]  4%|▍         | 84/2065 [04:33<1:45:32,  3.20s/it]  4%|▍         | 85/2065 [04:36<1:45:07,  3.19s/it]  4%|▍         | 86/2065 [04:40<1:45:23,  3.20s/it]  4%|▍         | 87/2065 [04:43<1:44:34,  3.17s/it]  4%|▍         | 88/2065 [04:46<1:44:02,  3.16s/it]  4%|▍         | 89/2065 [04:49<1:44:41,  3.18s/it]  4%|▍         | 90/2065 [04:52<1:44:08,  3.16s/it]                                                   {'loss': 2.5289, 'grad_norm': 0.6083694100379944, 'learning_rate': 1.9942235549209955e-05, 'epoch': 0.22}
  4%|▍         | 90/2065 [04:52<1:44:08,  3.16s/it]  4%|▍         | 91/2065 [04:55<1:44:13,  3.17s/it]  4%|▍         | 92/2065 [04:59<1:44:10,  3.17s/it]  5%|▍         | 93/2065 [05:02<1:44:09,  3.17s/it]  5%|▍         | 94/2065 [05:05<1:43:52,  3.16s/it]  5%|▍         | 95/2065 [05:08<1:43:04,  3.14s/it]  5%|▍         | 96/2065 [05:11<1:42:55,  3.14s/it]  5%|▍         | 97/2065 [05:14<1:43:22,  3.15s/it]  5%|▍         | 98/2065 [05:17<1:43:02,  3.14s/it]  5%|▍         | 99/2065 [05:21<1:43:17,  3.15s/it]  5%|▍         | 100/2065 [05:24<1:44:29,  3.19s/it]                                                    {'loss': 2.5503, 'grad_norm': inf, 'learning_rate': 1.9926446356718197e-05, 'epoch': 0.24}
  5%|▍         | 100/2065 [05:24<1:44:29,  3.19s/it][INFO|trainer.py:3512] 2024-06-24 22:29:56,935 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 22:29:56,936 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 22:29:56,936 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.46it/s][A
 27%|██▋       | 3/11 [00:03<00:08,  1.08s/it][A
 36%|███▋      | 4/11 [00:04<00:09,  1.36s/it][A
 45%|████▌     | 5/11 [00:06<00:08,  1.44s/it][A
 55%|█████▍    | 6/11 [00:08<00:07,  1.49s/it][A
 64%|██████▎   | 7/11 [00:09<00:06,  1.54s/it][A
 73%|███████▎  | 8/11 [00:11<00:04,  1.60s/it][A
 82%|████████▏ | 9/11 [00:12<00:03,  1.56s/it][A
 91%|█████████ | 10/11 [00:14<00:01,  1.58s/it][A
100%|██████████| 11/11 [00:16<00:00,  1.58s/it][A                                                    
                                               [A{'eval_loss': 2.4673523902893066, 'eval_runtime': 18.1042, 'eval_samples_per_second': 14.417, 'eval_steps_per_second': 0.608, 'epoch': 0.24}
  5%|▍         | 100/2065 [05:42<1:44:29,  3.19s/it]
100%|██████████| 11/11 [00:16<00:00,  1.58s/it][A
                                               [A  5%|▍         | 101/2065 [05:45<4:41:15,  8.59s/it]  5%|▍         | 102/2065 [05:48<3:46:51,  6.93s/it]  5%|▍         | 103/2065 [05:51<3:09:15,  5.79s/it]  5%|▌         | 104/2065 [05:54<2:43:30,  5.00s/it]  5%|▌         | 105/2065 [05:57<2:24:22,  4.42s/it]  5%|▌         | 106/2065 [06:01<2:12:14,  4.05s/it]  5%|▌         | 107/2065 [06:04<2:03:12,  3.78s/it]  5%|▌         | 108/2065 [06:07<1:56:23,  3.57s/it]  5%|▌         | 109/2065 [06:10<1:52:31,  3.45s/it]  5%|▌         | 110/2065 [06:13<1:49:06,  3.35s/it]                                                    {'loss': 2.4897, 'grad_norm': 0.7004223465919495, 'learning_rate': 1.990667747332213e-05, 'epoch': 0.27}
  5%|▌         | 110/2065 [06:13<1:49:06,  3.35s/it]  5%|▌         | 111/2065 [06:16<1:47:53,  3.31s/it]  5%|▌         | 112/2065 [06:20<1:47:26,  3.30s/it]  5%|▌         | 113/2065 [06:23<1:46:38,  3.28s/it]  6%|▌         | 114/2065 [06:26<1:45:17,  3.24s/it]  6%|▌         | 115/2065 [06:29<1:44:22,  3.21s/it]  6%|▌         | 116/2065 [06:32<1:43:28,  3.19s/it]  6%|▌         | 117/2065 [06:35<1:43:27,  3.19s/it]  6%|▌         | 118/2065 [06:39<1:42:43,  3.17s/it]  6%|▌         | 119/2065 [06:42<1:41:56,  3.14s/it]  6%|▌         | 120/2065 [06:45<1:41:59,  3.15s/it]                                                    {'loss': 2.4711, 'grad_norm': 0.9919630289077759, 'learning_rate': 1.9886886398684333e-05, 'epoch': 0.29}
  6%|▌         | 120/2065 [06:45<1:41:59,  3.15s/it]  6%|▌         | 121/2065 [06:48<1:42:22,  3.16s/it]  6%|▌         | 122/2065 [06:51<1:41:39,  3.14s/it]  6%|▌         | 123/2065 [06:54<1:41:17,  3.13s/it]  6%|▌         | 124/2065 [06:57<1:41:19,  3.13s/it]  6%|▌         | 125/2065 [07:00<1:41:05,  3.13s/it]  6%|▌         | 126/2065 [07:04<1:41:02,  3.13s/it]  6%|▌         | 127/2065 [07:07<1:41:36,  3.15s/it]  6%|▌         | 128/2065 [07:10<1:41:30,  3.14s/it]  6%|▌         | 129/2065 [07:13<1:41:31,  3.15s/it]  6%|▋         | 130/2065 [07:16<1:42:07,  3.17s/it]                                                    {'loss': 2.4463, 'grad_norm': 0.9406483769416809, 'learning_rate': 1.9862679881976605e-05, 'epoch': 0.31}
  6%|▋         | 130/2065 [07:16<1:42:07,  3.17s/it]  6%|▋         | 131/2065 [07:19<1:41:24,  3.15s/it]  6%|▋         | 132/2065 [07:23<1:41:23,  3.15s/it]  6%|▋         | 133/2065 [07:26<1:41:43,  3.16s/it]  6%|▋         | 134/2065 [07:29<1:41:37,  3.16s/it]  7%|▋         | 135/2065 [07:32<1:41:37,  3.16s/it]  7%|▋         | 136/2065 [07:35<1:40:51,  3.14s/it]  7%|▋         | 137/2065 [07:38<1:41:43,  3.17s/it]  7%|▋         | 138/2065 [07:42<1:44:27,  3.25s/it]  7%|▋         | 139/2065 [07:45<1:43:30,  3.22s/it]  7%|▋         | 140/2065 [07:48<1:42:28,  3.19s/it]                                                    {'loss': 2.4111, 'grad_norm': 0.23478330671787262, 'learning_rate': 1.9836145812602126e-05, 'epoch': 0.34}
  7%|▋         | 140/2065 [07:48<1:42:28,  3.19s/it]  7%|▋         | 141/2065 [07:51<1:41:40,  3.17s/it]  7%|▋         | 142/2065 [07:54<1:41:17,  3.16s/it]  7%|▋         | 143/2065 [07:58<1:42:40,  3.21s/it]  7%|▋         | 144/2065 [08:01<1:42:45,  3.21s/it]  7%|▋         | 145/2065 [08:04<1:41:38,  3.18s/it]  7%|▋         | 146/2065 [08:07<1:41:15,  3.17s/it]  7%|▋         | 147/2065 [08:10<1:41:24,  3.17s/it]  7%|▋         | 148/2065 [08:13<1:41:10,  3.17s/it]  7%|▋         | 149/2065 [08:17<1:42:49,  3.22s/it]  7%|▋         | 150/2065 [08:20<1:42:23,  3.21s/it]                                                    {'loss': 2.4523, 'grad_norm': 0.21896426379680634, 'learning_rate': 1.980729045249423e-05, 'epoch': 0.36}
  7%|▋         | 150/2065 [08:20<1:42:23,  3.21s/it][INFO|trainer.py:3203] 2024-06-24 22:32:53,234 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-150
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63caa5cc50>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 750fd987-8b3c-4d72-a62c-ae9e9e7e03ef)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-24 22:33:03,373 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-24 22:33:03,375 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-150/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
  7%|▋         | 151/2065 [08:33<3:19:12,  6.24s/it]  7%|▋         | 152/2065 [08:36<2:47:05,  5.24s/it]  7%|▋         | 153/2065 [08:39<2:24:41,  4.54s/it]  7%|▋         | 154/2065 [08:42<2:09:38,  4.07s/it]  8%|▊         | 155/2065 [08:45<1:59:58,  3.77s/it]  8%|▊         | 156/2065 [08:48<1:53:14,  3.56s/it]  8%|▊         | 157/2065 [08:51<1:48:03,  3.40s/it]  8%|▊         | 158/2065 [08:54<1:44:19,  3.28s/it]  8%|▊         | 159/2065 [08:57<1:42:50,  3.24s/it]  8%|▊         | 160/2065 [09:01<1:41:56,  3.21s/it]                                                    {'loss': 2.3898, 'grad_norm': 0.20973782241344452, 'learning_rate': 1.9776120611401507e-05, 'epoch': 0.39}
  8%|▊         | 160/2065 [09:01<1:41:56,  3.21s/it]  8%|▊         | 161/2065 [09:04<1:40:54,  3.18s/it]  8%|▊         | 162/2065 [09:07<1:39:58,  3.15s/it]  8%|▊         | 163/2065 [09:10<1:38:54,  3.12s/it]  8%|▊         | 164/2065 [09:13<1:38:32,  3.11s/it]  8%|▊         | 165/2065 [09:16<1:38:27,  3.11s/it]  8%|▊         | 166/2065 [09:19<1:38:06,  3.10s/it]  8%|▊         | 167/2065 [09:22<1:39:05,  3.13s/it]  8%|▊         | 168/2065 [09:25<1:39:12,  3.14s/it]  8%|▊         | 169/2065 [09:28<1:38:28,  3.12s/it]  8%|▊         | 170/2065 [09:32<1:38:26,  3.12s/it]                                                    {'loss': 2.3624, 'grad_norm': 0.24392104148864746, 'learning_rate': 1.9742643645280705e-05, 'epoch': 0.41}
  8%|▊         | 170/2065 [09:32<1:38:26,  3.12s/it]  8%|▊         | 171/2065 [09:35<1:38:31,  3.12s/it]  8%|▊         | 172/2065 [09:38<1:38:08,  3.11s/it]  8%|▊         | 173/2065 [09:41<1:38:00,  3.11s/it]  8%|▊         | 174/2065 [09:44<1:38:41,  3.13s/it]  8%|▊         | 175/2065 [09:47<1:39:08,  3.15s/it]  9%|▊         | 176/2065 [09:50<1:38:21,  3.12s/it]  9%|▊         | 177/2065 [09:53<1:38:19,  3.12s/it]  9%|▊         | 178/2065 [09:57<1:40:17,  3.19s/it]  9%|▊         | 179/2065 [10:00<1:39:55,  3.18s/it]  9%|▊         | 180/2065 [10:03<1:40:26,  3.20s/it]                                                    {'loss': 2.396, 'grad_norm': 0.2077142298221588, 'learning_rate': 1.9706867454560793e-05, 'epoch': 0.44}
  9%|▊         | 180/2065 [10:03<1:40:26,  3.20s/it]  9%|▉         | 181/2065 [10:06<1:39:38,  3.17s/it]  9%|▉         | 182/2065 [10:09<1:39:31,  3.17s/it]  9%|▉         | 183/2065 [10:13<1:39:11,  3.16s/it]  9%|▉         | 184/2065 [10:16<1:39:31,  3.17s/it]  9%|▉         | 185/2065 [10:19<1:38:14,  3.14s/it]  9%|▉         | 186/2065 [10:22<1:37:46,  3.12s/it]  9%|▉         | 187/2065 [10:25<1:37:23,  3.11s/it]  9%|▉         | 188/2065 [10:28<1:37:04,  3.10s/it]  9%|▉         | 189/2065 [10:31<1:37:15,  3.11s/it]  9%|▉         | 190/2065 [10:34<1:37:06,  3.11s/it]                                                    {'loss': 2.3631, 'grad_norm': 0.25115710496902466, 'learning_rate': 1.9668800482278467e-05, 'epoch': 0.46}
  9%|▉         | 190/2065 [10:34<1:37:06,  3.11s/it]  9%|▉         | 191/2065 [10:38<1:37:21,  3.12s/it]  9%|▉         | 192/2065 [10:41<1:37:10,  3.11s/it]  9%|▉         | 193/2065 [10:44<1:36:57,  3.11s/it]  9%|▉         | 194/2065 [10:47<1:37:06,  3.11s/it]  9%|▉         | 195/2065 [10:50<1:37:36,  3.13s/it]  9%|▉         | 196/2065 [10:53<1:37:01,  3.11s/it] 10%|▉         | 197/2065 [10:56<1:36:48,  3.11s/it] 10%|▉         | 198/2065 [10:59<1:36:42,  3.11s/it] 10%|▉         | 199/2065 [11:02<1:36:59,  3.12s/it] 10%|▉         | 200/2065 [11:06<1:37:03,  3.12s/it]                                                    {'loss': 2.3926, 'grad_norm': 0.2080565243959427, 'learning_rate': 1.9628451712085616e-05, 'epoch': 0.48}
 10%|▉         | 200/2065 [11:06<1:37:03,  3.12s/it][INFO|trainer.py:3512] 2024-06-24 22:35:38,682 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 22:35:38,682 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 22:35:38,682 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.33it/s][A
 27%|██▋       | 3/11 [00:03<00:08,  1.09s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.25s/it][A
 45%|████▌     | 5/11 [00:06<00:08,  1.35s/it][A
 55%|█████▍    | 6/11 [00:07<00:07,  1.41s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.44s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.47s/it][A
 82%|████████▏ | 9/11 [00:12<00:02,  1.49s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.50s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.50s/it][A                                                    
                                               [A{'eval_loss': 2.366811513900757, 'eval_runtime': 16.7917, 'eval_samples_per_second': 15.543, 'eval_steps_per_second': 0.655, 'epoch': 0.48}
 10%|▉         | 200/2065 [11:22<1:37:03,  3.12s/it]
100%|██████████| 11/11 [00:15<00:00,  1.50s/it][A
                                               [A 10%|▉         | 201/2065 [11:26<4:14:29,  8.19s/it] 10%|▉         | 202/2065 [11:29<3:26:46,  6.66s/it] 10%|▉         | 203/2065 [11:32<2:56:49,  5.70s/it] 10%|▉         | 204/2065 [11:35<2:32:10,  4.91s/it] 10%|▉         | 205/2065 [11:38<2:15:12,  4.36s/it] 10%|▉         | 206/2065 [11:41<2:03:55,  4.00s/it] 10%|█         | 207/2065 [11:45<1:56:48,  3.77s/it] 10%|█         | 208/2065 [11:48<1:50:34,  3.57s/it] 10%|█         | 209/2065 [11:51<1:46:06,  3.43s/it] 10%|█         | 210/2065 [11:54<1:43:40,  3.35s/it]                                                    {'loss': 2.4126, 'grad_norm': 0.22407957911491394, 'learning_rate': 1.9585830666129248e-05, 'epoch': 0.51}
 10%|█         | 210/2065 [11:54<1:43:40,  3.35s/it] 10%|█         | 211/2065 [11:57<1:40:51,  3.26s/it] 10%|█         | 212/2065 [12:00<1:39:08,  3.21s/it] 10%|█         | 213/2065 [12:03<1:38:33,  3.19s/it] 10%|█         | 214/2065 [12:06<1:37:33,  3.16s/it] 10%|█         | 215/2065 [12:10<1:36:54,  3.14s/it] 10%|█         | 216/2065 [12:13<1:37:24,  3.16s/it] 11%|█         | 217/2065 [12:16<1:36:39,  3.14s/it] 11%|█         | 218/2065 [12:19<1:36:44,  3.14s/it] 11%|█         | 219/2065 [12:22<1:36:11,  3.13s/it] 11%|█         | 220/2065 [12:25<1:36:01,  3.12s/it]                                                    {'loss': 2.363, 'grad_norm': 0.23707793653011322, 'learning_rate': 1.9540947402804275e-05, 'epoch': 0.53}
 11%|█         | 220/2065 [12:25<1:36:01,  3.12s/it] 11%|█         | 221/2065 [12:28<1:35:37,  3.11s/it] 11%|█         | 222/2065 [12:31<1:35:34,  3.11s/it] 11%|█         | 223/2065 [12:35<1:35:40,  3.12s/it] 11%|█         | 224/2065 [12:38<1:35:10,  3.10s/it] 11%|█         | 225/2065 [12:41<1:35:16,  3.11s/it] 11%|█         | 226/2065 [12:44<1:35:13,  3.11s/it] 11%|█         | 227/2065 [12:47<1:34:54,  3.10s/it] 11%|█         | 228/2065 [12:50<1:34:29,  3.09s/it] 11%|█         | 229/2065 [12:53<1:34:42,  3.10s/it] 11%|█         | 230/2065 [12:56<1:35:42,  3.13s/it]                                                    {'loss': 2.4048, 'grad_norm': 0.2080175131559372, 'learning_rate': 1.9493812514379787e-05, 'epoch': 0.56}
 11%|█         | 230/2065 [12:56<1:35:42,  3.13s/it] 11%|█         | 231/2065 [12:59<1:35:20,  3.12s/it] 11%|█         | 232/2065 [13:02<1:35:05,  3.11s/it] 11%|█▏        | 233/2065 [13:06<1:34:54,  3.11s/it] 11%|█▏        | 234/2065 [13:09<1:35:03,  3.12s/it] 11%|█▏        | 235/2065 [13:12<1:35:11,  3.12s/it] 11%|█▏        | 236/2065 [13:15<1:34:42,  3.11s/it] 11%|█▏        | 237/2065 [13:18<1:34:19,  3.10s/it] 12%|█▏        | 238/2065 [13:21<1:34:22,  3.10s/it] 12%|█▏        | 239/2065 [13:24<1:33:57,  3.09s/it] 12%|█▏        | 240/2065 [13:27<1:33:42,  3.08s/it]                                                    {'loss': 2.3689, 'grad_norm': 0.22014129161834717, 'learning_rate': 1.9444437124499312e-05, 'epoch': 0.58}
 12%|█▏        | 240/2065 [13:27<1:33:42,  3.08s/it] 12%|█▏        | 241/2065 [13:30<1:34:37,  3.11s/it] 12%|█▏        | 242/2065 [13:33<1:34:28,  3.11s/it] 12%|█▏        | 243/2065 [13:37<1:34:32,  3.11s/it] 12%|█▏        | 244/2065 [13:40<1:34:48,  3.12s/it] 12%|█▏        | 245/2065 [13:43<1:34:48,  3.13s/it] 12%|█▏        | 246/2065 [13:46<1:34:23,  3.11s/it] 12%|█▏        | 247/2065 [13:49<1:34:12,  3.11s/it] 12%|█▏        | 248/2065 [13:52<1:34:03,  3.11s/it] 12%|█▏        | 249/2065 [13:55<1:33:38,  3.09s/it] 12%|█▏        | 250/2065 [13:58<1:33:32,  3.09s/it]                                                    {'loss': 2.3585, 'grad_norm': 0.23052671551704407, 'learning_rate': 1.9392832885555686e-05, 'epoch': 0.61}
 12%|█▏        | 250/2065 [13:58<1:33:32,  3.09s/it] 12%|█▏        | 251/2065 [14:02<1:34:22,  3.12s/it] 12%|█▏        | 252/2065 [14:05<1:33:56,  3.11s/it] 12%|█▏        | 253/2065 [14:08<1:34:14,  3.12s/it] 12%|█▏        | 254/2065 [14:11<1:33:41,  3.10s/it] 12%|█▏        | 255/2065 [14:14<1:33:35,  3.10s/it] 12%|█▏        | 256/2065 [14:17<1:33:15,  3.09s/it] 12%|█▏        | 257/2065 [14:20<1:33:36,  3.11s/it] 12%|█▏        | 258/2065 [14:23<1:34:05,  3.12s/it] 13%|█▎        | 259/2065 [14:26<1:33:46,  3.12s/it] 13%|█▎        | 260/2065 [14:29<1:33:22,  3.10s/it]                                                    {'loss': 2.3456, 'grad_norm': 0.21354986727237701, 'learning_rate': 1.933901197594113e-05, 'epoch': 0.63}
 13%|█▎        | 260/2065 [14:29<1:33:22,  3.10s/it] 13%|█▎        | 261/2065 [14:33<1:33:05,  3.10s/it] 13%|█▎        | 262/2065 [14:36<1:32:44,  3.09s/it] 13%|█▎        | 263/2065 [14:39<1:33:01,  3.10s/it] 13%|█▎        | 264/2065 [14:42<1:32:49,  3.09s/it] 13%|█▎        | 265/2065 [14:45<1:33:05,  3.10s/it] 13%|█▎        | 266/2065 [14:48<1:32:40,  3.09s/it] 13%|█▎        | 267/2065 [14:51<1:32:58,  3.10s/it] 13%|█▎        | 268/2065 [14:54<1:32:44,  3.10s/it] 13%|█▎        | 269/2065 [14:57<1:32:37,  3.09s/it] 13%|█▎        | 270/2065 [15:00<1:32:17,  3.09s/it]                                                    {'loss': 2.3556, 'grad_norm': 0.23079843819141388, 'learning_rate': 1.9282987097173205e-05, 'epoch': 0.65}
 13%|█▎        | 270/2065 [15:00<1:32:17,  3.09s/it] 13%|█▎        | 271/2065 [15:03<1:32:15,  3.09s/it] 13%|█▎        | 272/2065 [15:06<1:31:59,  3.08s/it] 13%|█▎        | 273/2065 [15:10<1:32:05,  3.08s/it] 13%|█▎        | 274/2065 [15:13<1:33:22,  3.13s/it] 13%|█▎        | 275/2065 [15:16<1:32:54,  3.11s/it] 13%|█▎        | 276/2065 [15:19<1:32:20,  3.10s/it] 13%|█▎        | 277/2065 [15:22<1:32:47,  3.11s/it] 13%|█▎        | 278/2065 [15:25<1:32:25,  3.10s/it] 14%|█▎        | 279/2065 [15:28<1:32:18,  3.10s/it] 14%|█▎        | 280/2065 [15:31<1:32:34,  3.11s/it]                                                    {'loss': 2.3832, 'grad_norm': 0.21863287687301636, 'learning_rate': 1.9224771470897298e-05, 'epoch': 0.68}
 14%|█▎        | 280/2065 [15:31<1:32:34,  3.11s/it] 14%|█▎        | 281/2065 [15:35<1:32:18,  3.10s/it] 14%|█▎        | 282/2065 [15:38<1:31:56,  3.09s/it] 14%|█▎        | 283/2065 [15:41<1:31:46,  3.09s/it] 14%|█▍        | 284/2065 [15:44<1:31:45,  3.09s/it] 14%|█▍        | 285/2065 [15:47<1:31:46,  3.09s/it] 14%|█▍        | 286/2065 [15:50<1:32:19,  3.11s/it] 14%|█▍        | 287/2065 [15:53<1:31:54,  3.10s/it] 14%|█▍        | 288/2065 [15:56<1:31:28,  3.09s/it] 14%|█▍        | 289/2065 [15:59<1:31:13,  3.08s/it] 14%|█▍        | 290/2065 [16:02<1:31:23,  3.09s/it]                                                    {'loss': 2.347, 'grad_norm': 0.20953014492988586, 'learning_rate': 1.9164378835766374e-05, 'epoch': 0.7}
 14%|█▍        | 290/2065 [16:02<1:31:23,  3.09s/it] 14%|█▍        | 291/2065 [16:05<1:31:12,  3.09s/it] 14%|█▍        | 292/2065 [16:08<1:31:10,  3.09s/it] 14%|█▍        | 293/2065 [16:12<1:30:51,  3.08s/it] 14%|█▍        | 294/2065 [16:15<1:31:30,  3.10s/it] 14%|█▍        | 295/2065 [16:18<1:31:08,  3.09s/it] 14%|█▍        | 296/2065 [16:21<1:31:02,  3.09s/it] 14%|█▍        | 297/2065 [16:24<1:30:59,  3.09s/it] 14%|█▍        | 298/2065 [16:27<1:30:49,  3.08s/it] 14%|█▍        | 299/2065 [16:30<1:31:57,  3.12s/it] 15%|█▍        | 300/2065 [16:33<1:31:46,  3.12s/it]                                                    {'loss': 2.3451, 'grad_norm': 0.2153158038854599, 'learning_rate': 1.9101823444198704e-05, 'epoch': 0.73}
 15%|█▍        | 300/2065 [16:33<1:31:46,  3.12s/it][INFO|trainer.py:3512] 2024-06-24 22:41:06,444 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 22:41:06,445 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 22:41:06,445 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.33it/s][A
 27%|██▋       | 3/11 [00:03<00:08,  1.08s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.25s/it][A
 45%|████▌     | 5/11 [00:06<00:08,  1.35s/it][A
 55%|█████▍    | 6/11 [00:07<00:07,  1.41s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.45s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.47s/it][A
 82%|████████▏ | 9/11 [00:12<00:02,  1.49s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.50s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.51s/it][A                                                    
                                               [A{'eval_loss': 2.3374476432800293, 'eval_runtime': 16.7927, 'eval_samples_per_second': 15.542, 'eval_steps_per_second': 0.655, 'epoch': 0.73}
 15%|█▍        | 300/2065 [16:50<1:31:46,  3.12s/it]
100%|██████████| 11/11 [00:15<00:00,  1.51s/it][A
                                               [A[INFO|trainer.py:3203] 2024-06-24 22:41:23,241 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-300
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63caabe1d0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: f3690437-a2ca-406b-8050-b1c2b72360bf)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-24 22:41:33,313 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-24 22:41:33,315 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-300/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 15%|█▍        | 301/2065 [17:03<5:26:20, 11.10s/it] 15%|█▍        | 302/2065 [17:06<4:13:33,  8.63s/it] 15%|█▍        | 303/2065 [17:09<3:23:08,  6.92s/it] 15%|█▍        | 304/2065 [17:12<2:48:53,  5.75s/it] 15%|█▍        | 305/2065 [17:15<2:24:10,  4.91s/it] 15%|█▍        | 306/2065 [17:18<2:07:10,  4.34s/it] 15%|█▍        | 307/2065 [17:21<1:55:42,  3.95s/it] 15%|█▍        | 308/2065 [17:24<1:47:21,  3.67s/it] 15%|█▍        | 309/2065 [17:27<1:41:45,  3.48s/it] 15%|█▌        | 310/2065 [17:30<1:38:39,  3.37s/it]                                                    {'loss': 2.3307, 'grad_norm': 0.21952423453330994, 'learning_rate': 1.903712005901435e-05, 'epoch': 0.75}
 15%|█▌        | 310/2065 [17:30<1:38:39,  3.37s/it] 15%|█▌        | 311/2065 [17:33<1:35:47,  3.28s/it] 15%|█▌        | 312/2065 [17:36<1:33:33,  3.20s/it] 15%|█▌        | 313/2065 [17:39<1:31:58,  3.15s/it] 15%|█▌        | 314/2065 [17:42<1:31:04,  3.12s/it] 15%|█▌        | 315/2065 [17:45<1:30:22,  3.10s/it] 15%|█▌        | 316/2065 [17:48<1:29:40,  3.08s/it] 15%|█▌        | 317/2065 [17:51<1:29:17,  3.07s/it] 15%|█▌        | 318/2065 [17:54<1:29:26,  3.07s/it] 15%|█▌        | 319/2065 [17:58<1:30:00,  3.09s/it] 15%|█▌        | 320/2065 [18:01<1:30:41,  3.12s/it]                                                    {'loss': 2.3879, 'grad_norm': 0.21768632531166077, 'learning_rate': 1.8970283949951208e-05, 'epoch': 0.77}
 15%|█▌        | 320/2065 [18:01<1:30:41,  3.12s/it] 16%|█▌        | 321/2065 [18:04<1:29:50,  3.09s/it] 16%|█▌        | 322/2065 [18:07<1:29:20,  3.08s/it] 16%|█▌        | 323/2065 [18:10<1:29:41,  3.09s/it] 16%|█▌        | 324/2065 [18:13<1:29:15,  3.08s/it] 16%|█▌        | 325/2065 [18:16<1:29:01,  3.07s/it] 16%|█▌        | 326/2065 [18:19<1:28:51,  3.07s/it] 16%|█▌        | 327/2065 [18:22<1:28:44,  3.06s/it] 16%|█▌        | 328/2065 [18:25<1:28:31,  3.06s/it] 16%|█▌        | 329/2065 [18:28<1:29:01,  3.08s/it] 16%|█▌        | 330/2065 [18:31<1:28:35,  3.06s/it]                                                    {'loss': 2.3146, 'grad_norm': 0.222487673163414, 'learning_rate': 1.8901330890061408e-05, 'epoch': 0.8}
 16%|█▌        | 330/2065 [18:31<1:28:35,  3.06s/it] 16%|█▌        | 331/2065 [18:34<1:28:24,  3.06s/it] 16%|█▌        | 332/2065 [18:37<1:28:10,  3.05s/it] 16%|█▌        | 333/2065 [18:40<1:28:25,  3.06s/it] 16%|█▌        | 334/2065 [18:44<1:28:20,  3.06s/it] 16%|█▌        | 335/2065 [18:47<1:28:32,  3.07s/it] 16%|█▋        | 336/2065 [18:50<1:28:12,  3.06s/it] 16%|█▋        | 337/2065 [18:53<1:28:13,  3.06s/it] 16%|█▋        | 338/2065 [18:56<1:28:14,  3.07s/it] 16%|█▋        | 339/2065 [18:59<1:28:03,  3.06s/it] 16%|█▋        | 340/2065 [19:02<1:27:57,  3.06s/it]                                                    {'loss': 2.3202, 'grad_norm': 0.24080292880535126, 'learning_rate': 1.883027715198893e-05, 'epoch': 0.82}
 16%|█▋        | 340/2065 [19:02<1:27:57,  3.06s/it] 17%|█▋        | 341/2065 [19:05<1:28:15,  3.07s/it] 17%|█▋        | 342/2065 [19:08<1:28:20,  3.08s/it] 17%|█▋        | 343/2065 [19:11<1:27:57,  3.06s/it] 17%|█▋        | 344/2065 [19:14<1:27:55,  3.07s/it] 17%|█▋        | 345/2065 [19:17<1:27:37,  3.06s/it] 17%|█▋        | 346/2065 [19:20<1:27:46,  3.06s/it] 17%|█▋        | 347/2065 [19:23<1:28:00,  3.07s/it] 17%|█▋        | 348/2065 [19:27<1:28:33,  3.09s/it] 17%|█▋        | 349/2065 [19:30<1:28:19,  3.09s/it] 17%|█▋        | 350/2065 [19:33<1:28:03,  3.08s/it]                                                    {'loss': 2.3367, 'grad_norm': 0.22492854297161102, 'learning_rate': 1.8757139504129335e-05, 'epoch': 0.85}
 17%|█▋        | 350/2065 [19:33<1:28:03,  3.08s/it] 17%|█▋        | 351/2065 [19:36<1:27:52,  3.08s/it] 17%|█▋        | 352/2065 [19:39<1:27:52,  3.08s/it] 17%|█▋        | 353/2065 [19:42<1:27:49,  3.08s/it] 17%|█▋        | 354/2065 [19:45<1:28:14,  3.09s/it] 17%|█▋        | 355/2065 [19:48<1:27:49,  3.08s/it] 17%|█▋        | 356/2065 [19:51<1:27:43,  3.08s/it] 17%|█▋        | 357/2065 [19:54<1:27:18,  3.07s/it] 17%|█▋        | 358/2065 [19:58<1:29:13,  3.14s/it] 17%|█▋        | 359/2065 [20:01<1:28:18,  3.11s/it] 17%|█▋        | 360/2065 [20:04<1:27:43,  3.09s/it]                                                    {'loss': 2.2879, 'grad_norm': 0.2243751734495163, 'learning_rate': 1.868193520667248e-05, 'epoch': 0.87}
 17%|█▋        | 360/2065 [20:04<1:27:43,  3.09s/it] 17%|█▋        | 361/2065 [20:07<1:27:39,  3.09s/it] 18%|█▊        | 362/2065 [20:10<1:27:09,  3.07s/it] 18%|█▊        | 363/2065 [20:13<1:26:49,  3.06s/it] 18%|█▊        | 364/2065 [20:16<1:26:34,  3.05s/it] 18%|█▊        | 365/2065 [20:19<1:26:23,  3.05s/it] 18%|█▊        | 366/2065 [20:22<1:26:14,  3.05s/it] 18%|█▊        | 367/2065 [20:25<1:26:14,  3.05s/it] 18%|█▊        | 368/2065 [20:28<1:26:16,  3.05s/it] 18%|█▊        | 369/2065 [20:31<1:26:14,  3.05s/it] 18%|█▊        | 370/2065 [20:34<1:25:57,  3.04s/it]                                                    {'loss': 2.323, 'grad_norm': 0.24357952177524567, 'learning_rate': 1.8604682007529197e-05, 'epoch': 0.9}
 18%|█▊        | 370/2065 [20:34<1:25:57,  3.04s/it] 18%|█▊        | 371/2065 [20:37<1:25:54,  3.04s/it] 18%|█▊        | 372/2065 [20:40<1:25:57,  3.05s/it] 18%|█▊        | 373/2065 [20:43<1:25:59,  3.05s/it] 18%|█▊        | 374/2065 [20:46<1:25:56,  3.05s/it] 18%|█▊        | 375/2065 [20:49<1:26:05,  3.06s/it] 18%|█▊        | 376/2065 [20:52<1:25:53,  3.05s/it] 18%|█▊        | 377/2065 [20:55<1:25:44,  3.05s/it] 18%|█▊        | 378/2065 [20:58<1:25:48,  3.05s/it] 18%|█▊        | 379/2065 [21:02<1:25:57,  3.06s/it] 18%|█▊        | 380/2065 [21:05<1:25:47,  3.05s/it]                                                    {'loss': 2.308, 'grad_norm': 0.24686986207962036, 'learning_rate': 1.8525398138142852e-05, 'epoch': 0.92}
 18%|█▊        | 380/2065 [21:05<1:25:47,  3.05s/it] 18%|█▊        | 381/2065 [21:08<1:25:35,  3.05s/it] 18%|█▊        | 382/2065 [21:11<1:25:26,  3.05s/it] 19%|█▊        | 383/2065 [21:14<1:25:31,  3.05s/it] 19%|█▊        | 384/2065 [21:17<1:25:23,  3.05s/it] 19%|█▊        | 385/2065 [21:20<1:25:32,  3.05s/it] 19%|█▊        | 386/2065 [21:23<1:25:21,  3.05s/it] 19%|█▊        | 387/2065 [21:26<1:25:22,  3.05s/it] 19%|█▉        | 388/2065 [21:29<1:25:27,  3.06s/it] 19%|█▉        | 389/2065 [21:32<1:25:15,  3.05s/it] 19%|█▉        | 390/2065 [21:35<1:25:07,  3.05s/it]                                                    {'loss': 2.349, 'grad_norm': 0.24371053278446198, 'learning_rate': 1.8444102309186796e-05, 'epoch': 0.94}
 19%|█▉        | 390/2065 [21:35<1:25:07,  3.05s/it] 19%|█▉        | 391/2065 [21:38<1:25:30,  3.06s/it] 19%|█▉        | 392/2065 [21:41<1:25:19,  3.06s/it] 19%|█▉        | 393/2065 [21:44<1:25:07,  3.05s/it] 19%|█▉        | 394/2065 [21:47<1:25:01,  3.05s/it] 19%|█▉        | 395/2065 [21:50<1:24:56,  3.05s/it] 19%|█▉        | 396/2065 [21:53<1:24:45,  3.05s/it] 19%|█▉        | 397/2065 [21:56<1:24:38,  3.04s/it] 19%|█▉        | 398/2065 [22:00<1:24:28,  3.04s/it] 19%|█▉        | 399/2065 [22:03<1:24:24,  3.04s/it] 19%|█▉        | 400/2065 [22:06<1:24:27,  3.04s/it]                                                    {'loss': 2.278, 'grad_norm': 0.22387278079986572, 'learning_rate': 1.8360813706148716e-05, 'epoch': 0.97}
 19%|█▉        | 400/2065 [22:06<1:24:27,  3.04s/it][INFO|trainer.py:3512] 2024-06-24 22:46:38,709 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 22:46:38,709 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 22:46:38,710 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.32it/s][A
 27%|██▋       | 3/11 [00:03<00:08,  1.08s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.25s/it][A
 45%|████▌     | 5/11 [00:06<00:08,  1.44s/it][A
 55%|█████▍    | 6/11 [00:07<00:07,  1.46s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.48s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.50s/it][A
 82%|████████▏ | 9/11 [00:12<00:03,  1.50s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.51s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.51s/it][A                                                    
                                               [A{'eval_loss': 2.3172826766967773, 'eval_runtime': 17.0039, 'eval_samples_per_second': 15.349, 'eval_steps_per_second': 0.647, 'epoch': 0.97}
 19%|█▉        | 400/2065 [22:23<1:24:27,  3.04s/it]
100%|██████████| 11/11 [00:16<00:00,  1.51s/it][A
                                               [A 19%|█▉        | 401/2065 [22:26<3:46:07,  8.15s/it] 19%|█▉        | 402/2065 [22:29<3:03:26,  6.62s/it] 20%|█▉        | 403/2065 [22:32<2:33:29,  5.54s/it] 20%|█▉        | 404/2065 [22:35<2:12:33,  4.79s/it] 20%|█▉        | 405/2065 [22:38<1:57:49,  4.26s/it] 20%|█▉        | 406/2065 [22:41<1:47:33,  3.89s/it] 20%|█▉        | 407/2065 [22:44<1:40:24,  3.63s/it] 20%|█▉        | 408/2065 [22:47<1:35:28,  3.46s/it] 20%|█▉        | 409/2065 [22:50<1:31:55,  3.33s/it] 20%|█▉        | 410/2065 [22:53<1:29:24,  3.24s/it]                                                    {'loss': 2.3346, 'grad_norm': 0.26465481519699097, 'learning_rate': 1.8275551984802952e-05, 'epoch': 0.99}
 20%|█▉        | 410/2065 [22:53<1:29:24,  3.24s/it] 20%|█▉        | 411/2065 [22:56<1:27:33,  3.18s/it] 20%|█▉        | 412/2065 [22:59<1:26:20,  3.13s/it] 20%|██        | 413/2065 [23:02<1:25:27,  3.10s/it] 20%|██        | 414/2065 [23:05<1:24:53,  3.08s/it] 20%|██        | 415/2065 [23:08<1:24:26,  3.07s/it] 20%|██        | 416/2065 [23:11<1:24:05,  3.06s/it] 20%|██        | 417/2065 [23:14<1:23:53,  3.05s/it] 20%|██        | 418/2065 [23:17<1:23:41,  3.05s/it] 20%|██        | 419/2065 [23:20<1:23:33,  3.05s/it] 20%|██        | 420/2065 [23:23<1:23:27,  3.04s/it]                                                    {'loss': 2.3566, 'grad_norm': 0.2456989288330078, 'learning_rate': 1.81883372665718e-05, 'epoch': 1.02}
 20%|██        | 420/2065 [23:23<1:23:27,  3.04s/it] 20%|██        | 421/2065 [23:26<1:23:24,  3.04s/it] 20%|██        | 422/2065 [23:29<1:23:17,  3.04s/it] 20%|██        | 423/2065 [23:32<1:23:10,  3.04s/it] 21%|██        | 424/2065 [23:35<1:23:08,  3.04s/it] 21%|██        | 425/2065 [23:39<1:23:06,  3.04s/it] 21%|██        | 426/2065 [23:42<1:23:04,  3.04s/it] 21%|██        | 427/2065 [23:45<1:23:02,  3.04s/it] 21%|██        | 428/2065 [23:48<1:22:57,  3.04s/it] 21%|██        | 429/2065 [23:51<1:22:52,  3.04s/it] 21%|██        | 430/2065 [23:54<1:22:51,  3.04s/it]                                                    {'loss': 2.3096, 'grad_norm': 0.258510559797287, 'learning_rate': 1.8099190133776942e-05, 'epoch': 1.04}
 21%|██        | 430/2065 [23:54<1:22:51,  3.04s/it] 21%|██        | 431/2065 [23:57<1:22:50,  3.04s/it] 21%|██        | 432/2065 [24:00<1:22:48,  3.04s/it] 21%|██        | 433/2065 [24:03<1:22:45,  3.04s/it] 21%|██        | 434/2065 [24:06<1:22:42,  3.04s/it] 21%|██        | 435/2065 [24:09<1:22:38,  3.04s/it] 21%|██        | 436/2065 [24:12<1:22:35,  3.04s/it] 21%|██        | 437/2065 [24:15<1:22:33,  3.04s/it] 21%|██        | 438/2065 [24:18<1:22:30,  3.04s/it] 21%|██▏       | 439/2065 [24:21<1:22:27,  3.04s/it] 21%|██▏       | 440/2065 [24:24<1:22:23,  3.04s/it]                                                    {'loss': 2.3641, 'grad_norm': 0.260731041431427, 'learning_rate': 1.800813162478211e-05, 'epoch': 1.07}
 21%|██▏       | 440/2065 [24:24<1:22:23,  3.04s/it] 21%|██▏       | 441/2065 [24:27<1:22:21,  3.04s/it] 21%|██▏       | 442/2065 [24:30<1:22:19,  3.04s/it] 21%|██▏       | 443/2065 [24:33<1:22:19,  3.05s/it] 22%|██▏       | 444/2065 [24:36<1:22:10,  3.04s/it] 22%|██▏       | 445/2065 [24:39<1:22:07,  3.04s/it] 22%|██▏       | 446/2065 [24:42<1:22:04,  3.04s/it] 22%|██▏       | 447/2065 [24:45<1:22:01,  3.04s/it] 22%|██▏       | 448/2065 [24:48<1:21:58,  3.04s/it] 22%|██▏       | 449/2065 [24:52<1:21:55,  3.04s/it] 22%|██▏       | 450/2065 [24:55<1:21:45,  3.04s/it]                                                    {'loss': 2.3156, 'grad_norm': 0.25313615798950195, 'learning_rate': 1.7915183229028094e-05, 'epoch': 1.09}
 22%|██▏       | 450/2065 [24:55<1:21:45,  3.04s/it][INFO|trainer.py:3203] 2024-06-24 22:49:27,681 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-450
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63ca591a50>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 0196880f-a939-4845-9f53-639b0b1a2a5f)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-24 22:49:37,746 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-450/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-24 22:49:37,749 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-450/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 22%|██▏       | 451/2065 [25:07<2:41:29,  6.00s/it] 22%|██▏       | 452/2065 [25:10<2:15:39,  5.05s/it] 22%|██▏       | 453/2065 [25:13<1:57:59,  4.39s/it] 22%|██▏       | 454/2065 [25:16<1:46:01,  3.95s/it] 22%|██▏       | 455/2065 [25:19<1:37:55,  3.65s/it] 22%|██▏       | 456/2065 [25:22<1:32:22,  3.44s/it] 22%|██▏       | 457/2065 [25:25<1:28:36,  3.31s/it] 22%|██▏       | 458/2065 [25:28<1:26:01,  3.21s/it] 22%|██▏       | 459/2065 [25:31<1:24:16,  3.15s/it] 22%|██▏       | 460/2065 [25:34<1:23:01,  3.10s/it]                                                    {'loss': 2.3841, 'grad_norm': 0.2183941900730133, 'learning_rate': 1.7820366881961354e-05, 'epoch': 1.11}
 22%|██▏       | 460/2065 [25:34<1:23:01,  3.10s/it] 22%|██▏       | 461/2065 [25:37<1:22:08,  3.07s/it] 22%|██▏       | 462/2065 [25:40<1:21:34,  3.05s/it] 22%|██▏       | 463/2065 [25:43<1:21:09,  3.04s/it] 22%|██▏       | 464/2065 [25:46<1:20:53,  3.03s/it] 23%|██▎       | 465/2065 [25:49<1:20:39,  3.02s/it] 23%|██▎       | 466/2065 [25:52<1:20:32,  3.02s/it] 23%|██▎       | 467/2065 [25:55<1:20:26,  3.02s/it] 23%|██▎       | 468/2065 [25:58<1:20:19,  3.02s/it] 23%|██▎       | 469/2065 [26:01<1:20:17,  3.02s/it] 23%|██▎       | 470/2065 [26:04<1:20:16,  3.02s/it]                                                    {'loss': 2.351, 'grad_norm': 0.24584253132343292, 'learning_rate': 1.7723704959857308e-05, 'epoch': 1.14}
 23%|██▎       | 470/2065 [26:04<1:20:16,  3.02s/it] 23%|██▎       | 471/2065 [26:07<1:20:16,  3.02s/it] 23%|██▎       | 472/2065 [26:10<1:20:09,  3.02s/it] 23%|██▎       | 473/2065 [26:13<1:20:09,  3.02s/it] 23%|██▎       | 474/2065 [26:16<1:20:08,  3.02s/it] 23%|██▎       | 475/2065 [26:19<1:20:07,  3.02s/it] 23%|██▎       | 476/2065 [26:22<1:20:03,  3.02s/it] 23%|██▎       | 477/2065 [26:25<1:20:05,  3.03s/it] 23%|██▎       | 478/2065 [26:28<1:20:01,  3.03s/it] 23%|██▎       | 479/2065 [26:31<1:19:58,  3.03s/it] 23%|██▎       | 480/2065 [26:34<1:19:55,  3.03s/it]                                                    {'loss': 2.2972, 'grad_norm': 0.28486499190330505, 'learning_rate': 1.7625220274539648e-05, 'epoch': 1.16}
 23%|██▎       | 480/2065 [26:34<1:19:55,  3.03s/it] 23%|██▎       | 481/2065 [26:37<1:19:50,  3.02s/it] 23%|██▎       | 482/2065 [26:40<1:19:47,  3.02s/it] 23%|██▎       | 483/2065 [26:43<1:19:48,  3.03s/it] 23%|██▎       | 484/2065 [26:46<1:19:48,  3.03s/it] 23%|██▎       | 485/2065 [26:49<1:19:42,  3.03s/it] 24%|██▎       | 486/2065 [26:53<1:19:42,  3.03s/it] 24%|██▎       | 487/2065 [26:56<1:19:38,  3.03s/it] 24%|██▎       | 488/2065 [26:59<1:19:38,  3.03s/it] 24%|██▎       | 489/2065 [27:02<1:19:35,  3.03s/it] 24%|██▎       | 490/2065 [27:05<1:19:26,  3.03s/it]                                                    {'loss': 2.3046, 'grad_norm': 0.2710567116737366, 'learning_rate': 1.7524936067996824e-05, 'epoch': 1.19}
 24%|██▎       | 490/2065 [27:05<1:19:26,  3.03s/it] 24%|██▍       | 491/2065 [27:08<1:19:27,  3.03s/it] 24%|██▍       | 492/2065 [27:11<1:19:23,  3.03s/it] 24%|██▍       | 493/2065 [27:14<1:19:20,  3.03s/it] 24%|██▍       | 494/2065 [27:17<1:19:18,  3.03s/it] 24%|██▍       | 495/2065 [27:20<1:19:09,  3.02s/it] 24%|██▍       | 496/2065 [27:23<1:19:06,  3.02s/it] 24%|██▍       | 497/2065 [27:26<1:19:04,  3.03s/it] 24%|██▍       | 498/2065 [27:29<1:19:02,  3.03s/it] 24%|██▍       | 499/2065 [27:32<1:18:58,  3.03s/it] 24%|██▍       | 500/2065 [27:35<1:18:57,  3.03s/it]                                                    {'loss': 2.2667, 'grad_norm': 0.27191799879074097, 'learning_rate': 1.7422876006897047e-05, 'epoch': 1.21}
 24%|██▍       | 500/2065 [27:35<1:18:57,  3.03s/it][INFO|trainer.py:3512] 2024-06-24 22:52:08,010 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 22:52:08,010 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 22:52:08,010 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.33it/s][A
 27%|██▋       | 3/11 [00:03<00:08,  1.07s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.24s/it][A
 45%|████▌     | 5/11 [00:06<00:08,  1.34s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.40s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.44s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.46s/it][A
 82%|████████▏ | 9/11 [00:12<00:02,  1.48s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.49s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A                                                    
                                               [A{'eval_loss': 2.30092191696167, 'eval_runtime': 16.6728, 'eval_samples_per_second': 15.654, 'eval_steps_per_second': 0.66, 'epoch': 1.21}
 24%|██▍       | 500/2065 [27:52<1:18:57,  3.03s/it]
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A
                                               [A 24%|██▍       | 501/2065 [27:55<3:29:17,  8.03s/it] 24%|██▍       | 502/2065 [27:58<2:50:09,  6.53s/it] 24%|██▍       | 503/2065 [28:01<2:22:35,  5.48s/it] 24%|██▍       | 504/2065 [28:04<2:03:19,  4.74s/it] 24%|██▍       | 505/2065 [28:07<1:49:49,  4.22s/it] 25%|██▍       | 506/2065 [28:10<1:40:22,  3.86s/it] 25%|██▍       | 507/2065 [28:13<1:33:46,  3.61s/it] 25%|██▍       | 508/2065 [28:16<1:29:10,  3.44s/it] 25%|██▍       | 509/2065 [28:19<1:25:53,  3.31s/it] 25%|██▍       | 510/2065 [28:22<1:23:45,  3.23s/it]                                                    {'loss': 2.2844, 'grad_norm': 0.2897297143936157, 'learning_rate': 1.7319064177003028e-05, 'epoch': 1.23}
 25%|██▍       | 510/2065 [28:22<1:23:45,  3.23s/it] 25%|██▍       | 511/2065 [28:25<1:22:07,  3.17s/it] 25%|██▍       | 512/2065 [28:28<1:20:57,  3.13s/it] 25%|██▍       | 513/2065 [28:31<1:20:07,  3.10s/it] 25%|██▍       | 514/2065 [28:34<1:19:31,  3.08s/it] 25%|██▍       | 515/2065 [28:37<1:18:59,  3.06s/it] 25%|██▍       | 516/2065 [28:40<1:18:41,  3.05s/it] 25%|██▌       | 517/2065 [28:43<1:18:29,  3.04s/it] 25%|██▌       | 518/2065 [28:46<1:18:22,  3.04s/it] 25%|██▌       | 519/2065 [28:49<1:18:12,  3.04s/it] 25%|██▌       | 520/2065 [28:52<1:18:06,  3.03s/it]                                                    {'loss': 2.2817, 'grad_norm': 0.2398514300584793, 'learning_rate': 1.7213525077487857e-05, 'epoch': 1.26}
 25%|██▌       | 520/2065 [28:52<1:18:06,  3.03s/it] 25%|██▌       | 521/2065 [28:55<1:18:02,  3.03s/it] 25%|██▌       | 522/2065 [28:58<1:17:58,  3.03s/it] 25%|██▌       | 523/2065 [29:01<1:17:54,  3.03s/it] 25%|██▌       | 524/2065 [29:04<1:17:54,  3.03s/it] 25%|██▌       | 525/2065 [29:07<1:17:50,  3.03s/it] 25%|██▌       | 526/2065 [29:10<1:17:46,  3.03s/it] 26%|██▌       | 527/2065 [29:13<1:17:42,  3.03s/it] 26%|██▌       | 528/2065 [29:16<1:17:39,  3.03s/it] 26%|██▌       | 529/2065 [29:19<1:17:40,  3.03s/it] 26%|██▌       | 530/2065 [29:22<1:17:34,  3.03s/it]                                                    {'loss': 2.3309, 'grad_norm': 0.2580547034740448, 'learning_rate': 1.7106283615153273e-05, 'epoch': 1.28}
 26%|██▌       | 530/2065 [29:22<1:17:34,  3.03s/it] 26%|██▌       | 531/2065 [29:25<1:17:28,  3.03s/it] 26%|██▌       | 532/2065 [29:28<1:17:23,  3.03s/it] 26%|██▌       | 533/2065 [29:31<1:17:20,  3.03s/it] 26%|██▌       | 534/2065 [29:35<1:17:11,  3.03s/it] 26%|██▌       | 535/2065 [29:38<1:17:10,  3.03s/it] 26%|██▌       | 536/2065 [29:41<1:17:07,  3.03s/it] 26%|██▌       | 537/2065 [29:44<1:17:04,  3.03s/it] 26%|██▌       | 538/2065 [29:47<1:16:59,  3.03s/it] 26%|██▌       | 539/2065 [29:50<1:16:57,  3.03s/it] 26%|██▌       | 540/2065 [29:53<1:16:55,  3.03s/it]                                                    {'loss': 2.3281, 'grad_norm': 0.2564562261104584, 'learning_rate': 1.6997365098551794e-05, 'epoch': 1.31}
 26%|██▌       | 540/2065 [29:53<1:16:55,  3.03s/it] 26%|██▌       | 541/2065 [29:56<1:16:53,  3.03s/it] 26%|██▌       | 542/2065 [29:59<1:16:50,  3.03s/it] 26%|██▋       | 543/2065 [30:02<1:16:46,  3.03s/it] 26%|██▋       | 544/2065 [30:05<1:16:42,  3.03s/it] 26%|██▋       | 545/2065 [30:08<1:16:40,  3.03s/it] 26%|██▋       | 546/2065 [30:11<1:16:39,  3.03s/it] 26%|██▋       | 547/2065 [30:14<1:16:35,  3.03s/it] 27%|██▋       | 548/2065 [30:17<1:16:32,  3.03s/it] 27%|██▋       | 549/2065 [30:20<1:16:30,  3.03s/it] 27%|██▋       | 550/2065 [30:23<1:16:27,  3.03s/it]                                                    {'loss': 2.3221, 'grad_norm': 0.29018059372901917, 'learning_rate': 1.6886795232013958e-05, 'epoch': 1.33}
 27%|██▋       | 550/2065 [30:23<1:16:27,  3.03s/it] 27%|██▋       | 551/2065 [30:26<1:16:25,  3.03s/it] 27%|██▋       | 552/2065 [30:29<1:16:19,  3.03s/it] 27%|██▋       | 553/2065 [30:32<1:16:17,  3.03s/it] 27%|██▋       | 554/2065 [30:35<1:16:15,  3.03s/it] 27%|██▋       | 555/2065 [30:38<1:16:14,  3.03s/it] 27%|██▋       | 556/2065 [30:41<1:16:10,  3.03s/it] 27%|██▋       | 557/2065 [30:44<1:16:06,  3.03s/it] 27%|██▋       | 558/2065 [30:47<1:16:02,  3.03s/it] 27%|██▋       | 559/2065 [30:50<1:15:58,  3.03s/it] 27%|██▋       | 560/2065 [30:53<1:15:56,  3.03s/it]                                                    {'loss': 2.3104, 'grad_norm': 0.26231586933135986, 'learning_rate': 1.6774600109582242e-05, 'epoch': 1.36}
 27%|██▋       | 560/2065 [30:53<1:15:56,  3.03s/it] 27%|██▋       | 561/2065 [30:56<1:15:49,  3.02s/it] 27%|██▋       | 562/2065 [30:59<1:15:40,  3.02s/it] 27%|██▋       | 563/2065 [31:02<1:15:39,  3.02s/it] 27%|██▋       | 564/2065 [31:05<1:15:40,  3.02s/it] 27%|██▋       | 565/2065 [31:08<1:15:37,  3.03s/it] 27%|██▋       | 566/2065 [31:11<1:15:35,  3.03s/it] 27%|██▋       | 567/2065 [31:14<1:15:34,  3.03s/it] 28%|██▊       | 568/2065 [31:17<1:15:32,  3.03s/it] 28%|██▊       | 569/2065 [31:20<1:15:29,  3.03s/it] 28%|██▊       | 570/2065 [31:23<1:15:24,  3.03s/it]                                                    {'loss': 2.3157, 'grad_norm': 0.2663114368915558, 'learning_rate': 1.666080620885295e-05, 'epoch': 1.38}
 28%|██▊       | 570/2065 [31:23<1:15:24,  3.03s/it] 28%|██▊       | 571/2065 [31:27<1:15:25,  3.03s/it] 28%|██▊       | 572/2065 [31:30<1:15:22,  3.03s/it] 28%|██▊       | 573/2065 [31:33<1:15:20,  3.03s/it] 28%|██▊       | 574/2065 [31:36<1:15:16,  3.03s/it] 28%|██▊       | 575/2065 [31:39<1:15:07,  3.03s/it] 28%|██▊       | 576/2065 [31:42<1:15:05,  3.03s/it] 28%|██▊       | 577/2065 [31:45<1:15:03,  3.03s/it] 28%|██▊       | 578/2065 [31:48<1:15:01,  3.03s/it] 28%|██▊       | 579/2065 [31:51<1:14:58,  3.03s/it] 28%|██▊       | 580/2065 [31:54<1:14:55,  3.03s/it]                                                    {'loss': 2.3184, 'grad_norm': 0.2688835561275482, 'learning_rate': 1.6545440384727615e-05, 'epoch': 1.4}
 28%|██▊       | 580/2065 [31:54<1:14:55,  3.03s/it] 28%|██▊       | 581/2065 [31:57<1:15:07,  3.04s/it] 28%|██▊       | 582/2065 [32:00<1:14:59,  3.03s/it] 28%|██▊       | 583/2065 [32:03<1:14:54,  3.03s/it] 28%|██▊       | 584/2065 [32:06<1:14:48,  3.03s/it] 28%|██▊       | 585/2065 [32:09<1:14:39,  3.03s/it] 28%|██▊       | 586/2065 [32:12<1:14:36,  3.03s/it] 28%|██▊       | 587/2065 [32:15<1:14:35,  3.03s/it] 28%|██▊       | 588/2065 [32:18<1:14:32,  3.03s/it] 29%|██▊       | 589/2065 [32:21<1:14:28,  3.03s/it] 29%|██▊       | 590/2065 [32:24<1:14:27,  3.03s/it]                                                    {'loss': 2.3188, 'grad_norm': 0.2435438632965088, 'learning_rate': 1.6428529863075355e-05, 'epoch': 1.43}
 29%|██▊       | 590/2065 [32:24<1:14:27,  3.03s/it] 29%|██▊       | 591/2065 [32:27<1:14:26,  3.03s/it] 29%|██▊       | 592/2065 [32:30<1:14:23,  3.03s/it] 29%|██▊       | 593/2065 [32:33<1:14:19,  3.03s/it] 29%|██▉       | 594/2065 [32:36<1:14:15,  3.03s/it] 29%|██▉       | 595/2065 [32:39<1:14:13,  3.03s/it] 29%|██▉       | 596/2065 [32:42<1:14:11,  3.03s/it] 29%|██▉       | 597/2065 [32:45<1:14:07,  3.03s/it] 29%|██▉       | 598/2065 [32:48<1:14:04,  3.03s/it] 29%|██▉       | 599/2065 [32:51<1:14:00,  3.03s/it] 29%|██▉       | 600/2065 [32:54<1:13:59,  3.03s/it]                                                    {'loss': 2.332, 'grad_norm': 0.24872535467147827, 'learning_rate': 1.631010223430767e-05, 'epoch': 1.45}
 29%|██▉       | 600/2065 [32:54<1:13:59,  3.03s/it][INFO|trainer.py:3512] 2024-06-24 22:57:27,478 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 22:57:27,478 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 22:57:27,478 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.33it/s][A
 27%|██▋       | 3/11 [00:03<00:08,  1.07s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.24s/it][A
 45%|████▌     | 5/11 [00:06<00:08,  1.34s/it][A
 55%|█████▍    | 6/11 [00:07<00:07,  1.41s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.44s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.47s/it][A
 82%|████████▏ | 9/11 [00:12<00:02,  1.48s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.49s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.50s/it][A                                                    
                                               [A{'eval_loss': 2.290971040725708, 'eval_runtime': 16.7173, 'eval_samples_per_second': 15.613, 'eval_steps_per_second': 0.658, 'epoch': 1.45}
 29%|██▉       | 600/2065 [33:11<1:13:59,  3.03s/it]
100%|██████████| 11/11 [00:15<00:00,  1.50s/it][A
                                               [A[INFO|trainer.py:3203] 2024-06-24 22:57:44,199 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-600
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63cb50e1d0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 4957c098-4663-41d5-89c3-c761f02c295b)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-24 22:57:54,264 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-24 22:57:54,266 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-600/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 29%|██▉       | 601/2065 [33:24<4:28:43, 11.01s/it] 29%|██▉       | 602/2065 [33:27<3:28:20,  8.54s/it] 29%|██▉       | 603/2065 [33:30<2:46:33,  6.84s/it] 29%|██▉       | 604/2065 [33:33<2:17:44,  5.66s/it] 29%|██▉       | 605/2065 [33:35<1:57:47,  4.84s/it] 29%|██▉       | 606/2065 [33:38<1:43:57,  4.28s/it] 29%|██▉       | 607/2065 [33:41<1:34:18,  3.88s/it] 29%|██▉       | 608/2065 [33:44<1:27:36,  3.61s/it] 29%|██▉       | 609/2065 [33:47<1:22:57,  3.42s/it] 30%|██▉       | 610/2065 [33:50<1:19:45,  3.29s/it]                                                    {'loss': 2.3086, 'grad_norm': 0.26954105496406555, 'learning_rate': 1.6190185446867222e-05, 'epoch': 1.48}
 30%|██▉       | 610/2065 [33:50<1:19:45,  3.29s/it] 30%|██▉       | 611/2065 [33:53<1:17:32,  3.20s/it] 30%|██▉       | 612/2065 [33:56<1:15:57,  3.14s/it] 30%|██▉       | 613/2065 [33:59<1:14:51,  3.09s/it] 30%|██▉       | 614/2065 [34:02<1:14:07,  3.07s/it] 30%|██▉       | 615/2065 [34:05<1:13:36,  3.05s/it] 30%|██▉       | 616/2065 [34:08<1:13:12,  3.03s/it] 30%|██▉       | 617/2065 [34:11<1:12:58,  3.02s/it] 30%|██▉       | 618/2065 [34:14<1:12:48,  3.02s/it] 30%|██▉       | 619/2065 [34:17<1:12:34,  3.01s/it] 30%|███       | 620/2065 [34:20<1:12:30,  3.01s/it]                                                    {'loss': 2.3078, 'grad_norm': 0.28524065017700195, 'learning_rate': 1.606880780063211e-05, 'epoch': 1.5}
 30%|███       | 620/2065 [34:20<1:12:30,  3.01s/it] 30%|███       | 621/2065 [34:23<1:12:28,  3.01s/it] 30%|███       | 622/2065 [34:26<1:12:19,  3.01s/it] 30%|███       | 623/2065 [34:29<1:12:16,  3.01s/it] 30%|███       | 624/2065 [34:32<1:12:14,  3.01s/it] 30%|███       | 625/2065 [34:35<1:12:08,  3.01s/it] 30%|███       | 626/2065 [34:38<1:12:09,  3.01s/it] 30%|███       | 627/2065 [34:41<1:12:07,  3.01s/it] 30%|███       | 628/2065 [34:44<1:12:03,  3.01s/it] 30%|███       | 629/2065 [34:47<1:12:00,  3.01s/it] 31%|███       | 630/2065 [34:50<1:11:59,  3.01s/it]                                                    {'loss': 2.2829, 'grad_norm': 0.25682052969932556, 'learning_rate': 1.5945997940237215e-05, 'epoch': 1.53}
 31%|███       | 630/2065 [34:50<1:11:59,  3.01s/it] 31%|███       | 631/2065 [34:53<1:11:57,  3.01s/it] 31%|███       | 632/2065 [34:56<1:11:54,  3.01s/it] 31%|███       | 633/2065 [34:59<1:11:51,  3.01s/it] 31%|███       | 634/2065 [35:02<1:11:50,  3.01s/it] 31%|███       | 635/2065 [35:05<1:11:44,  3.01s/it] 31%|███       | 636/2065 [35:08<1:11:43,  3.01s/it] 31%|███       | 637/2065 [35:11<1:11:41,  3.01s/it] 31%|███       | 638/2065 [35:14<1:11:37,  3.01s/it] 31%|███       | 639/2065 [35:18<1:11:34,  3.01s/it] 31%|███       | 640/2065 [35:21<1:11:32,  3.01s/it]                                                    {'loss': 2.3224, 'grad_norm': 0.2617805302143097, 'learning_rate': 1.582178484831418e-05, 'epoch': 1.55}
 31%|███       | 640/2065 [35:21<1:11:32,  3.01s/it] 31%|███       | 641/2065 [35:24<1:11:32,  3.01s/it] 31%|███       | 642/2065 [35:27<1:11:25,  3.01s/it] 31%|███       | 643/2065 [35:30<1:11:23,  3.01s/it] 31%|███       | 644/2065 [35:33<1:11:21,  3.01s/it] 31%|███       | 645/2065 [35:36<1:11:16,  3.01s/it] 31%|███▏      | 646/2065 [35:39<1:11:16,  3.01s/it] 31%|███▏      | 647/2065 [35:42<1:11:14,  3.01s/it] 31%|███▏      | 648/2065 [35:45<1:11:11,  3.01s/it] 31%|███▏      | 649/2065 [35:48<1:11:17,  3.02s/it] 31%|███▏      | 650/2065 [35:51<1:11:14,  3.02s/it]                                                    {'loss': 2.2828, 'grad_norm': 0.27219682931900024, 'learning_rate': 1.5696197838651643e-05, 'epoch': 1.57}
 31%|███▏      | 650/2065 [35:51<1:11:14,  3.02s/it] 32%|███▏      | 651/2065 [35:54<1:11:12,  3.02s/it] 32%|███▏      | 652/2065 [35:57<1:11:09,  3.02s/it] 32%|███▏      | 653/2065 [36:00<1:11:03,  3.02s/it] 32%|███▏      | 654/2065 [36:03<1:11:01,  3.02s/it] 32%|███▏      | 655/2065 [36:06<1:10:58,  3.02s/it] 32%|███▏      | 656/2065 [36:09<1:10:57,  3.02s/it] 32%|███▏      | 657/2065 [36:12<1:10:50,  3.02s/it] 32%|███▏      | 658/2065 [36:15<1:10:47,  3.02s/it] 32%|███▏      | 659/2065 [36:18<1:10:46,  3.02s/it] 32%|███▏      | 660/2065 [36:21<1:10:44,  3.02s/it]                                                    {'loss': 2.289, 'grad_norm': 0.290255606174469, 'learning_rate': 1.5569266549277258e-05, 'epoch': 1.6}
 32%|███▏      | 660/2065 [36:21<1:10:44,  3.02s/it] 32%|███▏      | 661/2065 [36:24<1:10:40,  3.02s/it] 32%|███▏      | 662/2065 [36:27<1:10:38,  3.02s/it] 32%|███▏      | 663/2065 [36:30<1:10:38,  3.02s/it] 32%|███▏      | 664/2065 [36:33<1:10:36,  3.02s/it] 32%|███▏      | 665/2065 [36:36<1:10:32,  3.02s/it] 32%|███▏      | 666/2065 [36:39<1:10:30,  3.02s/it] 32%|███▏      | 667/2065 [36:42<1:10:27,  3.02s/it] 32%|███▏      | 668/2065 [36:45<1:10:26,  3.03s/it] 32%|███▏      | 669/2065 [36:48<1:10:23,  3.03s/it] 32%|███▏      | 670/2065 [36:51<1:10:22,  3.03s/it]                                                    {'loss': 2.2823, 'grad_norm': 0.2835750877857208, 'learning_rate': 1.54410209354633e-05, 'epoch': 1.62}
 32%|███▏      | 670/2065 [36:51<1:10:22,  3.03s/it] 32%|███▏      | 671/2065 [36:54<1:10:20,  3.03s/it] 33%|███▎      | 672/2065 [36:57<1:10:17,  3.03s/it] 33%|███▎      | 673/2065 [37:00<1:10:13,  3.03s/it] 33%|███▎      | 674/2065 [37:03<1:10:06,  3.02s/it] 33%|███▎      | 675/2065 [37:06<1:10:06,  3.03s/it] 33%|███▎      | 676/2065 [37:09<1:10:03,  3.03s/it] 33%|███▎      | 677/2065 [37:12<1:10:00,  3.03s/it] 33%|███▎      | 678/2065 [37:15<1:09:57,  3.03s/it] 33%|███▎      | 679/2065 [37:18<1:09:49,  3.02s/it] 33%|███▎      | 680/2065 [37:21<1:09:47,  3.02s/it]                                                    {'loss': 2.311, 'grad_norm': 0.2506592869758606, 'learning_rate': 1.5311491262657286e-05, 'epoch': 1.65}
 33%|███▎      | 680/2065 [37:21<1:09:47,  3.02s/it] 33%|███▎      | 681/2065 [37:24<1:09:46,  3.02s/it] 33%|███▎      | 682/2065 [37:27<1:09:43,  3.03s/it] 33%|███▎      | 683/2065 [37:30<1:09:38,  3.02s/it] 33%|███▎      | 684/2065 [37:33<1:09:37,  3.02s/it] 33%|███▎      | 685/2065 [37:37<1:09:35,  3.03s/it] 33%|███▎      | 686/2065 [37:40<1:09:33,  3.03s/it] 33%|███▎      | 687/2065 [37:43<1:09:27,  3.02s/it] 33%|███▎      | 688/2065 [37:46<1:09:25,  3.02s/it] 33%|███▎      | 689/2065 [37:49<1:09:22,  3.03s/it] 33%|███▎      | 690/2065 [37:52<1:09:19,  3.03s/it]                                                    {'loss': 2.2692, 'grad_norm': 0.2945500612258911, 'learning_rate': 1.5180708099339487e-05, 'epoch': 1.67}
 33%|███▎      | 690/2065 [37:52<1:09:19,  3.03s/it] 33%|███▎      | 691/2065 [37:55<1:09:17,  3.03s/it] 34%|███▎      | 692/2065 [37:58<1:09:12,  3.02s/it] 34%|███▎      | 693/2065 [38:01<1:09:08,  3.02s/it] 34%|███▎      | 694/2065 [38:04<1:09:05,  3.02s/it] 34%|███▎      | 695/2065 [38:07<1:09:03,  3.02s/it] 34%|███▎      | 696/2065 [38:10<1:08:54,  3.02s/it] 34%|███▍      | 697/2065 [38:13<1:08:54,  3.02s/it] 34%|███▍      | 698/2065 [38:16<1:08:52,  3.02s/it] 34%|███▍      | 699/2065 [38:19<1:08:51,  3.02s/it] 34%|███▍      | 700/2065 [38:22<1:08:46,  3.02s/it]                                                    {'loss': 2.3308, 'grad_norm': 0.27228662371635437, 'learning_rate': 1.5048702309808878e-05, 'epoch': 1.69}
 34%|███▍      | 700/2065 [38:22<1:08:46,  3.02s/it][INFO|trainer.py:3512] 2024-06-24 23:02:54,991 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 23:02:54,991 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 23:02:54,991 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.33it/s][A
 27%|██▋       | 3/11 [00:03<00:08,  1.07s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.24s/it][A
 45%|████▌     | 5/11 [00:06<00:08,  1.34s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.40s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.44s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.46s/it][A
 82%|████████▏ | 9/11 [00:12<00:02,  1.48s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.49s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A                                                    
                                               [A{'eval_loss': 2.2845420837402344, 'eval_runtime': 16.639, 'eval_samples_per_second': 15.686, 'eval_steps_per_second': 0.661, 'epoch': 1.69}
 34%|███▍      | 700/2065 [38:39<1:08:46,  3.02s/it]
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A
                                               [A 34%|███▍      | 701/2065 [38:42<3:02:14,  8.02s/it] 34%|███▍      | 702/2065 [38:45<2:27:59,  6.51s/it] 34%|███▍      | 703/2065 [38:48<2:04:01,  5.46s/it] 34%|███▍      | 704/2065 [38:51<1:47:16,  4.73s/it] 34%|███▍      | 705/2065 [38:54<1:35:26,  4.21s/it] 34%|███▍      | 706/2065 [38:57<1:27:15,  3.85s/it] 34%|███▍      | 707/2065 [39:00<1:21:31,  3.60s/it] 34%|███▍      | 708/2065 [39:03<1:17:27,  3.42s/it] 34%|███▍      | 709/2065 [39:06<1:14:37,  3.30s/it] 34%|███▍      | 710/2065 [39:09<1:12:37,  3.22s/it]                                                    {'loss': 2.2877, 'grad_norm': 0.28319355845451355, 'learning_rate': 1.491550504689928e-05, 'epoch': 1.72}
 34%|███▍      | 710/2065 [39:09<1:12:37,  3.22s/it] 34%|███▍      | 711/2065 [39:12<1:11:14,  3.16s/it] 34%|███▍      | 712/2065 [39:15<1:10:17,  3.12s/it] 35%|███▍      | 713/2065 [39:18<1:09:33,  3.09s/it] 35%|███▍      | 714/2065 [39:21<1:09:02,  3.07s/it] 35%|███▍      | 715/2065 [39:24<1:08:39,  3.05s/it] 35%|███▍      | 716/2065 [39:27<1:08:30,  3.05s/it] 35%|███▍      | 717/2065 [39:30<1:08:14,  3.04s/it] 35%|███▍      | 718/2065 [39:33<1:08:04,  3.03s/it] 35%|███▍      | 719/2065 [39:36<1:07:55,  3.03s/it] 35%|███▍      | 720/2065 [39:39<1:07:47,  3.02s/it]                                                    {'loss': 2.298, 'grad_norm': 0.27825358510017395, 'learning_rate': 1.4781147744627435e-05, 'epoch': 1.74}
 35%|███▍      | 720/2065 [39:39<1:07:47,  3.02s/it] 35%|███▍      | 721/2065 [39:42<1:07:42,  3.02s/it] 35%|███▍      | 722/2065 [39:45<1:07:38,  3.02s/it] 35%|███▌      | 723/2065 [39:48<1:07:34,  3.02s/it] 35%|███▌      | 724/2065 [39:51<1:07:28,  3.02s/it] 35%|███▌      | 725/2065 [39:54<1:07:24,  3.02s/it] 35%|███▌      | 726/2065 [39:57<1:07:22,  3.02s/it] 35%|███▌      | 727/2065 [40:00<1:07:18,  3.02s/it] 35%|███▌      | 728/2065 [40:03<1:07:14,  3.02s/it] 35%|███▌      | 729/2065 [40:06<1:07:11,  3.02s/it] 35%|███▌      | 730/2065 [40:09<1:07:08,  3.02s/it]                                                    {'loss': 2.2845, 'grad_norm': 0.40735742449760437, 'learning_rate': 1.464566211077469e-05, 'epoch': 1.77}
 35%|███▌      | 730/2065 [40:09<1:07:08,  3.02s/it] 35%|███▌      | 731/2065 [40:12<1:07:02,  3.02s/it] 35%|███▌      | 732/2065 [40:15<1:06:59,  3.02s/it] 35%|███▌      | 733/2065 [40:18<1:06:57,  3.02s/it] 36%|███▌      | 734/2065 [40:21<1:06:54,  3.02s/it] 36%|███▌      | 735/2065 [40:24<1:07:00,  3.02s/it] 36%|███▌      | 736/2065 [40:27<1:06:57,  3.02s/it] 36%|███▌      | 737/2065 [40:30<1:06:52,  3.02s/it] 36%|███▌      | 738/2065 [40:33<1:06:47,  3.02s/it] 36%|███▌      | 739/2065 [40:36<1:06:43,  3.02s/it] 36%|███▌      | 740/2065 [40:39<1:06:38,  3.02s/it]                                                    {'loss': 2.3198, 'grad_norm': 0.2807469367980957, 'learning_rate': 1.4509080119404093e-05, 'epoch': 1.79}
 36%|███▌      | 740/2065 [40:39<1:06:38,  3.02s/it] 36%|███▌      | 741/2065 [40:42<1:06:36,  3.02s/it] 36%|███▌      | 742/2065 [40:45<1:06:32,  3.02s/it] 36%|███▌      | 743/2065 [40:48<1:06:24,  3.01s/it] 36%|███▌      | 744/2065 [40:51<1:06:22,  3.01s/it] 36%|███▌      | 745/2065 [40:54<1:06:20,  3.02s/it] 36%|███▌      | 746/2065 [40:57<1:06:16,  3.01s/it] 36%|███▌      | 747/2065 [41:00<1:06:14,  3.02s/it] 36%|███▌      | 748/2065 [41:03<1:06:12,  3.02s/it] 36%|███▋      | 749/2065 [41:06<1:06:10,  3.02s/it] 36%|███▋      | 750/2065 [41:09<1:06:03,  3.01s/it]                                                    {'loss': 2.3518, 'grad_norm': 0.2776091992855072, 'learning_rate': 1.4371434003314646e-05, 'epoch': 1.82}
 36%|███▋      | 750/2065 [41:09<1:06:03,  3.01s/it][INFO|trainer.py:3203] 2024-06-24 23:05:42,484 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-750
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63caa40210>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 1c0eacce-d5e1-4aa6-858b-7ae6a016f6b7)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-24 23:05:52,545 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-750/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-24 23:05:52,547 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-750/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 36%|███▋      | 751/2065 [41:22<2:11:02,  5.98s/it] 36%|███▋      | 752/2065 [41:25<1:49:56,  5.02s/it] 36%|███▋      | 753/2065 [41:28<1:35:37,  4.37s/it] 37%|███▋      | 754/2065 [41:31<1:25:51,  3.93s/it] 37%|███▋      | 755/2065 [41:34<1:19:16,  3.63s/it] 37%|███▋      | 756/2065 [41:37<1:14:43,  3.43s/it] 37%|███▋      | 757/2065 [41:40<1:11:37,  3.29s/it] 37%|███▋      | 758/2065 [41:43<1:09:29,  3.19s/it] 37%|███▋      | 759/2065 [41:46<1:08:01,  3.13s/it] 37%|███▋      | 760/2065 [41:49<1:07:01,  3.08s/it]                                                    {'loss': 2.3284, 'grad_norm': 0.3095780611038208, 'learning_rate': 1.423275624643449e-05, 'epoch': 1.84}
 37%|███▋      | 760/2065 [41:49<1:07:01,  3.08s/it] 37%|███▋      | 761/2065 [41:52<1:06:19,  3.05s/it] 37%|███▋      | 762/2065 [41:55<1:05:52,  3.03s/it] 37%|███▋      | 763/2065 [41:58<1:05:33,  3.02s/it] 37%|███▋      | 764/2065 [42:01<1:05:18,  3.01s/it] 37%|███▋      | 765/2065 [42:04<1:05:08,  3.01s/it] 37%|███▋      | 766/2065 [42:07<1:05:00,  3.00s/it] 37%|███▋      | 767/2065 [42:10<1:04:55,  3.00s/it] 37%|███▋      | 768/2065 [42:13<1:04:53,  3.00s/it] 37%|███▋      | 769/2065 [42:16<1:04:46,  3.00s/it] 37%|███▋      | 770/2065 [42:19<1:04:45,  3.00s/it]                                                    {'loss': 2.2813, 'grad_norm': 0.271676242351532, 'learning_rate': 1.4093079576154831e-05, 'epoch': 1.86}
 37%|███▋      | 770/2065 [42:19<1:04:45,  3.00s/it] 37%|███▋      | 771/2065 [42:22<1:04:47,  3.00s/it] 37%|███▋      | 772/2065 [42:25<1:04:43,  3.00s/it] 37%|███▋      | 773/2065 [42:28<1:04:41,  3.00s/it] 37%|███▋      | 774/2065 [42:31<1:04:38,  3.00s/it] 38%|███▊      | 775/2065 [42:34<1:04:35,  3.00s/it] 38%|███▊      | 776/2065 [42:37<1:04:33,  3.01s/it] 38%|███▊      | 777/2065 [42:40<1:04:31,  3.01s/it] 38%|███▊      | 778/2065 [42:43<1:04:28,  3.01s/it] 38%|███▊      | 779/2065 [42:46<1:04:28,  3.01s/it] 38%|███▊      | 780/2065 [42:49<1:04:24,  3.01s/it]                                                    {'loss': 2.265, 'grad_norm': 0.30611729621887207, 'learning_rate': 1.395243695560641e-05, 'epoch': 1.89}
 38%|███▊      | 780/2065 [42:49<1:04:24,  3.01s/it] 38%|███▊      | 781/2065 [42:52<1:04:21,  3.01s/it] 38%|███▊      | 782/2065 [42:55<1:04:19,  3.01s/it] 38%|███▊      | 783/2065 [42:58<1:04:17,  3.01s/it] 38%|███▊      | 784/2065 [43:01<1:04:15,  3.01s/it] 38%|███▊      | 785/2065 [43:04<1:04:10,  3.01s/it] 38%|███▊      | 786/2065 [43:07<1:04:16,  3.02s/it] 38%|███▊      | 787/2065 [43:10<1:04:13,  3.01s/it] 38%|███▊      | 788/2065 [43:13<1:04:13,  3.02s/it] 38%|███▊      | 789/2065 [43:16<1:04:16,  3.02s/it] 38%|███▊      | 790/2065 [43:19<1:04:08,  3.02s/it]                                                    {'loss': 2.3026, 'grad_norm': 0.2619491219520569, 'learning_rate': 1.3810861575880335e-05, 'epoch': 1.91}
 38%|███▊      | 790/2065 [43:19<1:04:08,  3.02s/it] 38%|███▊      | 791/2065 [43:22<1:04:04,  3.02s/it] 38%|███▊      | 792/2065 [43:25<1:03:59,  3.02s/it] 38%|███▊      | 793/2065 [43:28<1:03:55,  3.02s/it] 38%|███▊      | 794/2065 [43:31<1:03:52,  3.02s/it] 38%|███▊      | 795/2065 [43:34<1:03:49,  3.02s/it] 39%|███▊      | 796/2065 [43:37<1:03:45,  3.01s/it] 39%|███▊      | 797/2065 [43:40<1:03:41,  3.01s/it] 39%|███▊      | 798/2065 [43:43<1:03:39,  3.01s/it] 39%|███▊      | 799/2065 [43:46<1:03:35,  3.01s/it] 39%|███▊      | 800/2065 [43:49<1:03:32,  3.01s/it]                                                    {'loss': 2.2276, 'grad_norm': 0.2889317274093628, 'learning_rate': 1.3668386848195115e-05, 'epoch': 1.94}
 39%|███▊      | 800/2065 [43:49<1:03:32,  3.01s/it][INFO|trainer.py:3512] 2024-06-24 23:08:21,998 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 23:08:21,998 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 23:08:21,998 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.34it/s][A
 27%|██▋       | 3/11 [00:03<00:08,  1.07s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.24s/it][A
 45%|████▌     | 5/11 [00:06<00:08,  1.34s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.39s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.43s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.46s/it][A
 82%|████████▏ | 9/11 [00:12<00:02,  1.47s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.48s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A                                                    
                                               [A{'eval_loss': 2.2791481018066406, 'eval_runtime': 16.6165, 'eval_samples_per_second': 15.707, 'eval_steps_per_second': 0.662, 'epoch': 1.94}
 39%|███▊      | 800/2065 [44:06<1:03:32,  3.01s/it]
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A
                                               [A 39%|███▉      | 801/2065 [44:09<2:48:30,  8.00s/it] 39%|███▉      | 802/2065 [44:12<2:16:48,  6.50s/it] 39%|███▉      | 803/2065 [44:15<1:54:37,  5.45s/it] 39%|███▉      | 804/2065 [44:18<1:39:08,  4.72s/it] 39%|███▉      | 805/2065 [44:21<1:28:17,  4.20s/it] 39%|███▉      | 806/2065 [44:24<1:20:39,  3.84s/it] 39%|███▉      | 807/2065 [44:27<1:15:20,  3.59s/it] 39%|███▉      | 808/2065 [44:30<1:11:37,  3.42s/it] 39%|███▉      | 809/2065 [44:33<1:08:59,  3.30s/it] 39%|███▉      | 810/2065 [44:36<1:07:08,  3.21s/it]                                                    {'loss': 2.3135, 'grad_norm': 0.2722153067588806, 'learning_rate': 1.3525046396011744e-05, 'epoch': 1.96}
 39%|███▉      | 810/2065 [44:36<1:07:08,  3.21s/it] 39%|███▉      | 811/2065 [44:39<1:05:50,  3.15s/it] 39%|███▉      | 812/2065 [44:42<1:04:55,  3.11s/it] 39%|███▉      | 813/2065 [44:45<1:04:11,  3.08s/it] 39%|███▉      | 814/2065 [44:48<1:03:43,  3.06s/it] 39%|███▉      | 815/2065 [44:51<1:03:24,  3.04s/it] 40%|███▉      | 816/2065 [44:54<1:03:07,  3.03s/it] 40%|███▉      | 817/2065 [44:57<1:02:56,  3.03s/it] 40%|███▉      | 818/2065 [45:00<1:02:47,  3.02s/it] 40%|███▉      | 819/2065 [45:03<1:02:39,  3.02s/it] 40%|███▉      | 820/2065 [45:06<1:02:34,  3.02s/it]                                                    {'loss': 2.2892, 'grad_norm': 0.3160844147205353, 'learning_rate': 1.3380874047098721e-05, 'epoch': 1.99}
 40%|███▉      | 820/2065 [45:06<1:02:34,  3.02s/it] 40%|███▉      | 821/2065 [45:09<1:02:31,  3.02s/it] 40%|███▉      | 822/2065 [45:12<1:02:25,  3.01s/it] 40%|███▉      | 823/2065 [45:15<1:02:26,  3.02s/it] 40%|███▉      | 824/2065 [45:18<1:02:23,  3.02s/it] 40%|███▉      | 825/2065 [45:21<1:02:19,  3.02s/it] 40%|████      | 826/2065 [45:24<1:02:14,  3.01s/it] 40%|████      | 827/2065 [45:27<1:02:11,  3.01s/it] 40%|████      | 828/2065 [45:30<1:02:09,  3.01s/it] 40%|████      | 829/2065 [45:33<1:02:05,  3.01s/it] 40%|████      | 830/2065 [45:36<1:02:02,  3.01s/it]                                                    {'loss': 2.2948, 'grad_norm': 0.3157997727394104, 'learning_rate': 1.3235903825548803e-05, 'epoch': 2.01}
 40%|████      | 830/2065 [45:36<1:02:02,  3.01s/it] 40%|████      | 831/2065 [45:39<1:01:59,  3.01s/it] 40%|████      | 832/2065 [45:42<1:01:57,  3.02s/it] 40%|████      | 833/2065 [45:45<1:01:53,  3.01s/it] 40%|████      | 834/2065 [45:48<1:01:50,  3.01s/it] 40%|████      | 835/2065 [45:51<1:01:47,  3.01s/it] 40%|████      | 836/2065 [45:54<1:01:45,  3.01s/it] 41%|████      | 837/2065 [45:57<1:01:37,  3.01s/it] 41%|████      | 838/2065 [46:00<1:01:35,  3.01s/it] 41%|████      | 839/2065 [46:03<1:01:33,  3.01s/it] 41%|████      | 840/2065 [46:06<1:01:31,  3.01s/it]                                                    {'loss': 2.2907, 'grad_norm': 0.28331273794174194, 'learning_rate': 1.3090169943749475e-05, 'epoch': 2.03}
 41%|████      | 840/2065 [46:06<1:01:31,  3.01s/it] 41%|████      | 841/2065 [46:09<1:01:29,  3.01s/it] 41%|████      | 842/2065 [46:12<1:01:27,  3.02s/it] 41%|████      | 843/2065 [46:15<1:01:24,  3.01s/it] 41%|████      | 844/2065 [46:18<1:01:16,  3.01s/it] 41%|████      | 845/2065 [46:21<1:01:13,  3.01s/it] 41%|████      | 846/2065 [46:24<1:01:12,  3.01s/it] 41%|████      | 847/2065 [46:27<1:01:10,  3.01s/it] 41%|████      | 848/2065 [46:30<1:01:07,  3.01s/it] 41%|████      | 849/2065 [46:33<1:01:04,  3.01s/it] 41%|████      | 850/2065 [46:36<1:01:01,  3.01s/it]                                                    {'loss': 2.3, 'grad_norm': 0.2727881073951721, 'learning_rate': 1.2943706794308964e-05, 'epoch': 2.06}
 41%|████      | 850/2065 [46:36<1:01:01,  3.01s/it] 41%|████      | 851/2065 [46:39<1:00:58,  3.01s/it] 41%|████▏     | 852/2065 [46:42<1:00:55,  3.01s/it] 41%|████▏     | 853/2065 [46:45<1:00:52,  3.01s/it] 41%|████▏     | 854/2065 [46:48<1:00:46,  3.01s/it] 41%|████▏     | 855/2065 [46:51<1:00:45,  3.01s/it] 41%|████▏     | 856/2065 [46:54<1:00:42,  3.01s/it] 42%|████▏     | 857/2065 [46:57<1:00:39,  3.01s/it] 42%|████▏     | 858/2065 [47:00<1:00:32,  3.01s/it] 42%|████▏     | 859/2065 [47:03<1:00:31,  3.01s/it] 42%|████▏     | 860/2065 [47:06<1:00:28,  3.01s/it]                                                    {'loss': 2.299, 'grad_norm': 0.28484633564949036, 'learning_rate': 1.2796548941939707e-05, 'epoch': 2.08}
 42%|████▏     | 860/2065 [47:06<1:00:28,  3.01s/it] 42%|████▏     | 861/2065 [47:09<1:00:25,  3.01s/it] 42%|████▏     | 862/2065 [47:12<1:00:22,  3.01s/it] 42%|████▏     | 863/2065 [47:15<1:00:20,  3.01s/it] 42%|████▏     | 864/2065 [47:18<1:00:18,  3.01s/it] 42%|████▏     | 865/2065 [47:21<1:00:15,  3.01s/it] 42%|████▏     | 866/2065 [47:24<1:00:12,  3.01s/it] 42%|████▏     | 867/2065 [47:27<1:00:13,  3.02s/it] 42%|████▏     | 868/2065 [47:30<1:00:09,  3.02s/it] 42%|████▏     | 869/2065 [47:33<1:00:05,  3.01s/it] 42%|████▏     | 870/2065 [47:36<1:00:03,  3.02s/it]                                                    {'loss': 2.2991, 'grad_norm': 0.3001718521118164, 'learning_rate': 1.2648731115301253e-05, 'epoch': 2.11}
 42%|████▏     | 870/2065 [47:36<1:00:03,  3.02s/it] 42%|████▏     | 871/2065 [47:39<1:00:01,  3.02s/it] 42%|████▏     | 872/2065 [47:42<59:57,  3.02s/it]   42%|████▏     | 873/2065 [47:45<59:53,  3.01s/it] 42%|████▏     | 874/2065 [47:48<59:51,  3.02s/it] 42%|████▏     | 875/2065 [47:51<59:47,  3.01s/it] 42%|████▏     | 876/2065 [47:54<59:44,  3.01s/it] 42%|████▏     | 877/2065 [47:57<59:41,  3.01s/it] 43%|████▎     | 878/2065 [48:00<59:39,  3.02s/it] 43%|████▎     | 879/2065 [48:03<59:33,  3.01s/it] 43%|████▎     | 880/2065 [48:06<59:30,  3.01s/it]                                                  {'loss': 2.2702, 'grad_norm': 0.28751882910728455, 'learning_rate': 1.2500288198804388e-05, 'epoch': 2.13}
 43%|████▎     | 880/2065 [48:06<59:30,  3.01s/it] 43%|████▎     | 881/2065 [48:10<59:29,  3.02s/it] 43%|████▎     | 882/2065 [48:13<59:26,  3.01s/it] 43%|████▎     | 883/2065 [48:16<59:24,  3.02s/it] 43%|████▎     | 884/2065 [48:19<59:21,  3.02s/it] 43%|████▎     | 885/2065 [48:22<59:18,  3.02s/it] 43%|████▎     | 886/2065 [48:25<59:14,  3.01s/it] 43%|████▎     | 887/2065 [48:28<59:11,  3.01s/it] 43%|████▎     | 888/2065 [48:31<59:08,  3.01s/it] 43%|████▎     | 889/2065 [48:34<59:04,  3.01s/it] 43%|████▎     | 890/2065 [48:37<58:58,  3.01s/it]                                                  {'loss': 2.2381, 'grad_norm': 0.31799495220184326, 'learning_rate': 1.2351255224378603e-05, 'epoch': 2.15}
 43%|████▎     | 890/2065 [48:37<58:58,  3.01s/it] 43%|████▎     | 891/2065 [48:40<58:56,  3.01s/it] 43%|████▎     | 892/2065 [48:43<58:55,  3.01s/it] 43%|████▎     | 893/2065 [48:46<58:50,  3.01s/it] 43%|████▎     | 894/2065 [48:49<58:48,  3.01s/it] 43%|████▎     | 895/2065 [48:52<58:46,  3.01s/it] 43%|████▎     | 896/2065 [48:55<58:43,  3.01s/it] 43%|████▎     | 897/2065 [48:58<58:45,  3.02s/it] 43%|████▎     | 898/2065 [49:01<58:37,  3.01s/it] 44%|████▎     | 899/2065 [49:04<58:35,  3.01s/it] 44%|████▎     | 900/2065 [49:07<58:32,  3.02s/it]                                                  {'loss': 2.307, 'grad_norm': 0.2864703834056854, 'learning_rate': 1.220166736320467e-05, 'epoch': 2.18}
 44%|████▎     | 900/2065 [49:07<58:32,  3.02s/it][INFO|trainer.py:3512] 2024-06-24 23:13:39,897 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 23:13:39,897 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 23:13:39,897 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.34it/s][A
 27%|██▋       | 3/11 [00:03<00:08,  1.07s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.24s/it][A
 45%|████▌     | 5/11 [00:06<00:08,  1.33s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.40s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.43s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.46s/it][A
 82%|████████▏ | 9/11 [00:12<00:02,  1.47s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.48s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A                                                  
                                               [A{'eval_loss': 2.2747623920440674, 'eval_runtime': 16.6232, 'eval_samples_per_second': 15.701, 'eval_steps_per_second': 0.662, 'epoch': 2.18}
 44%|████▎     | 900/2065 [49:23<58:32,  3.02s/it]
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A
                                               [A[INFO|trainer.py:3203] 2024-06-24 23:13:56,523 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-900
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63caabbb10>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: c22e285c-39ad-4d42-acde-e4f20de704d3)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-24 23:14:06,583 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-24 23:14:06,585 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-900/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 44%|████▎     | 901/2065 [49:36<3:32:51, 10.97s/it] 44%|████▎     | 902/2065 [49:39<2:44:57,  8.51s/it] 44%|████▎     | 903/2065 [49:42<2:11:55,  6.81s/it] 44%|████▍     | 904/2065 [49:45<1:48:55,  5.63s/it] 44%|████▍     | 905/2065 [49:48<1:33:05,  4.81s/it] 44%|████▍     | 906/2065 [49:51<1:22:05,  4.25s/it] 44%|████▍     | 907/2065 [49:54<1:14:28,  3.86s/it] 44%|████▍     | 908/2065 [49:57<1:09:11,  3.59s/it] 44%|████▍     | 909/2065 [50:00<1:05:30,  3.40s/it] 44%|████▍     | 910/2065 [50:02<1:02:58,  3.27s/it]                                                    {'loss': 2.2873, 'grad_norm': 0.27320706844329834, 'learning_rate': 1.2051559917414402e-05, 'epoch': 2.2}
 44%|████▍     | 910/2065 [50:02<1:02:58,  3.27s/it] 44%|████▍     | 911/2065 [50:05<1:01:12,  3.18s/it] 44%|████▍     | 912/2065 [50:08<59:59,  3.12s/it]   44%|████▍     | 913/2065 [50:11<59:04,  3.08s/it] 44%|████▍     | 914/2065 [50:14<58:30,  3.05s/it] 44%|████▍     | 915/2065 [50:17<58:02,  3.03s/it] 44%|████▍     | 916/2065 [50:20<57:46,  3.02s/it] 44%|████▍     | 917/2065 [50:23<57:34,  3.01s/it] 44%|████▍     | 918/2065 [50:26<57:24,  3.00s/it] 45%|████▍     | 919/2065 [50:29<57:19,  3.00s/it] 45%|████▍     | 920/2065 [50:32<57:11,  3.00s/it]                                                  {'loss': 2.3052, 'grad_norm': 0.3105117082595825, 'learning_rate': 1.190096831175948e-05, 'epoch': 2.23}
 45%|████▍     | 920/2065 [50:32<57:11,  3.00s/it] 45%|████▍     | 921/2065 [50:35<57:10,  3.00s/it] 45%|████▍     | 922/2065 [50:38<57:07,  3.00s/it] 45%|████▍     | 923/2065 [50:41<57:03,  3.00s/it] 45%|████▍     | 924/2065 [50:44<57:01,  3.00s/it] 45%|████▍     | 925/2065 [50:47<56:58,  3.00s/it] 45%|████▍     | 926/2065 [50:50<56:55,  3.00s/it] 45%|████▍     | 927/2065 [50:53<56:53,  3.00s/it] 45%|████▍     | 928/2065 [50:56<56:51,  3.00s/it] 45%|████▍     | 929/2065 [50:59<56:48,  3.00s/it] 45%|████▌     | 930/2065 [51:02<56:46,  3.00s/it]                                                  {'loss': 2.2556, 'grad_norm': 0.3349578082561493, 'learning_rate': 1.1749928085251361e-05, 'epoch': 2.25}
 45%|████▌     | 930/2065 [51:02<56:46,  3.00s/it] 45%|████▌     | 931/2065 [51:05<56:42,  3.00s/it] 45%|████▌     | 932/2065 [51:08<56:41,  3.00s/it] 45%|████▌     | 933/2065 [51:11<56:39,  3.00s/it] 45%|████▌     | 934/2065 [51:14<56:36,  3.00s/it] 45%|████▌     | 935/2065 [51:17<56:33,  3.00s/it] 45%|████▌     | 936/2065 [51:20<56:31,  3.00s/it] 45%|████▌     | 937/2065 [51:23<56:28,  3.00s/it] 45%|████▌     | 938/2065 [51:26<56:25,  3.00s/it] 45%|████▌     | 939/2065 [51:29<56:24,  3.01s/it] 46%|████▌     | 940/2065 [51:32<56:20,  3.00s/it]                                                  {'loss': 2.281, 'grad_norm': 0.28882840275764465, 'learning_rate': 1.1598474882774215e-05, 'epoch': 2.28}
 46%|████▌     | 940/2065 [51:32<56:20,  3.00s/it] 46%|████▌     | 941/2065 [51:35<56:18,  3.01s/it] 46%|████▌     | 942/2065 [51:38<56:15,  3.01s/it] 46%|████▌     | 943/2065 [51:41<56:13,  3.01s/it] 46%|████▌     | 944/2065 [51:44<56:09,  3.01s/it] 46%|████▌     | 945/2065 [51:47<56:07,  3.01s/it] 46%|████▌     | 946/2065 [51:50<56:04,  3.01s/it] 46%|████▌     | 947/2065 [51:53<56:00,  3.01s/it] 46%|████▌     | 948/2065 [51:56<55:58,  3.01s/it] 46%|████▌     | 949/2065 [51:59<55:56,  3.01s/it] 46%|████▌     | 950/2065 [52:02<55:49,  3.00s/it]                                                  {'loss': 2.2653, 'grad_norm': 0.3214191198348999, 'learning_rate': 1.1446644446672877e-05, 'epoch': 2.3}
 46%|████▌     | 950/2065 [52:03<55:49,  3.00s/it] 46%|████▌     | 951/2065 [52:06<56:22,  3.04s/it] 46%|████▌     | 952/2065 [52:09<56:10,  3.03s/it] 46%|████▌     | 953/2065 [52:12<56:00,  3.02s/it] 46%|████▌     | 954/2065 [52:15<55:53,  3.02s/it] 46%|████▌     | 955/2065 [52:18<55:47,  3.02s/it] 46%|████▋     | 956/2065 [52:21<55:41,  3.01s/it] 46%|████▋     | 957/2065 [52:24<55:36,  3.01s/it] 46%|████▋     | 958/2065 [52:27<55:32,  3.01s/it] 46%|████▋     | 959/2065 [52:30<55:29,  3.01s/it] 46%|████▋     | 960/2065 [52:33<55:26,  3.01s/it]                                                  {'loss': 2.2654, 'grad_norm': 0.3127475082874298, 'learning_rate': 1.129447260831779e-05, 'epoch': 2.32}
 46%|████▋     | 960/2065 [52:33<55:26,  3.01s/it] 47%|████▋     | 961/2065 [52:36<55:20,  3.01s/it] 47%|████▋     | 962/2065 [52:39<55:18,  3.01s/it] 47%|████▋     | 963/2065 [52:42<55:16,  3.01s/it] 47%|████▋     | 964/2065 [52:45<55:11,  3.01s/it] 47%|████▋     | 965/2065 [52:48<55:10,  3.01s/it] 47%|████▋     | 966/2065 [52:51<55:08,  3.01s/it] 47%|████▋     | 967/2065 [52:54<55:05,  3.01s/it] 47%|████▋     | 968/2065 [52:57<55:02,  3.01s/it] 47%|████▋     | 969/2065 [53:00<54:58,  3.01s/it] 47%|████▋     | 970/2065 [53:03<54:55,  3.01s/it]                                                  {'loss': 2.2672, 'grad_norm': 0.2911340296268463, 'learning_rate': 1.114199527964896e-05, 'epoch': 2.35}
 47%|████▋     | 970/2065 [53:03<54:55,  3.01s/it] 47%|████▋     | 971/2065 [53:06<54:53,  3.01s/it] 47%|████▋     | 972/2065 [53:09<54:50,  3.01s/it] 47%|████▋     | 973/2065 [53:12<54:47,  3.01s/it] 47%|████▋     | 974/2065 [53:15<54:40,  3.01s/it] 47%|████▋     | 975/2065 [53:18<54:36,  3.01s/it] 47%|████▋     | 976/2065 [53:21<54:36,  3.01s/it] 47%|████▋     | 977/2065 [53:24<54:33,  3.01s/it] 47%|████▋     | 978/2065 [53:27<54:30,  3.01s/it] 47%|████▋     | 979/2065 [53:30<54:29,  3.01s/it] 47%|████▋     | 980/2065 [53:33<54:26,  3.01s/it]                                                  {'loss': 2.2775, 'grad_norm': 0.26869910955429077, 'learning_rate': 1.0989248444700862e-05, 'epoch': 2.37}
 47%|████▋     | 980/2065 [53:33<54:26,  3.01s/it] 48%|████▊     | 981/2065 [53:36<54:23,  3.01s/it] 48%|████▊     | 982/2065 [53:39<54:20,  3.01s/it] 48%|████▊     | 983/2065 [53:42<54:18,  3.01s/it] 48%|████▊     | 984/2065 [53:45<54:11,  3.01s/it] 48%|████▊     | 985/2065 [53:48<54:08,  3.01s/it] 48%|████▊     | 986/2065 [53:51<54:06,  3.01s/it] 48%|████▊     | 987/2065 [53:54<54:01,  3.01s/it] 48%|████▊     | 988/2065 [53:57<53:59,  3.01s/it] 48%|████▊     | 989/2065 [54:00<53:58,  3.01s/it] 48%|████▊     | 990/2065 [54:03<53:55,  3.01s/it]                                                  {'loss': 2.2723, 'grad_norm': 0.35576125979423523, 'learning_rate': 1.0836268151110379e-05, 'epoch': 2.4}
 48%|████▊     | 990/2065 [54:03<53:55,  3.01s/it] 48%|████▊     | 991/2065 [54:06<53:52,  3.01s/it] 48%|████▊     | 992/2065 [54:09<53:49,  3.01s/it] 48%|████▊     | 993/2065 [54:12<53:46,  3.01s/it] 48%|████▊     | 994/2065 [54:15<53:40,  3.01s/it] 48%|████▊     | 995/2065 [54:18<53:38,  3.01s/it] 48%|████▊     | 996/2065 [54:21<53:36,  3.01s/it] 48%|████▊     | 997/2065 [54:24<53:30,  3.01s/it] 48%|████▊     | 998/2065 [54:27<53:27,  3.01s/it] 48%|████▊     | 999/2065 [54:30<53:25,  3.01s/it] 48%|████▊     | 1000/2065 [54:33<53:23,  3.01s/it]                                                   {'loss': 2.3223, 'grad_norm': 0.2940526604652405, 'learning_rate': 1.0683090501609685e-05, 'epoch': 2.42}
 48%|████▊     | 1000/2065 [54:33<53:23,  3.01s/it][INFO|trainer.py:3512] 2024-06-24 23:19:06,100 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 23:19:06,100 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 23:19:06,100 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.34it/s][A
 27%|██▋       | 3/11 [00:02<00:08,  1.06s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.23s/it][A
 45%|████▌     | 5/11 [00:06<00:07,  1.33s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.38s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.42s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.45s/it][A
 82%|████████▏ | 9/11 [00:12<00:02,  1.46s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.48s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.48s/it][A                                                   
                                               [A{'eval_loss': 2.2712247371673584, 'eval_runtime': 16.5296, 'eval_samples_per_second': 15.79, 'eval_steps_per_second': 0.665, 'epoch': 2.42}
 48%|████▊     | 1000/2065 [54:50<53:23,  3.01s/it]
100%|██████████| 11/11 [00:15<00:00,  1.48s/it][A
                                               [A 48%|████▊     | 1001/2065 [54:53<2:21:17,  7.97s/it] 49%|████▊     | 1002/2065 [54:56<1:54:42,  6.47s/it] 49%|████▊     | 1003/2065 [54:59<1:36:07,  5.43s/it] 49%|████▊     | 1004/2065 [55:02<1:23:09,  4.70s/it] 49%|████▊     | 1005/2065 [55:05<1:14:00,  4.19s/it] 49%|████▊     | 1006/2065 [55:08<1:07:39,  3.83s/it] 49%|████▉     | 1007/2065 [55:11<1:03:12,  3.58s/it] 49%|████▉     | 1008/2065 [55:14<1:00:03,  3.41s/it] 49%|████▉     | 1009/2065 [55:17<57:50,  3.29s/it]   49%|████▉     | 1010/2065 [55:20<56:16,  3.20s/it]                                                   {'loss': 2.2618, 'grad_norm': 0.31415796279907227, 'learning_rate': 1.052975164550615e-05, 'epoch': 2.45}
 49%|████▉     | 1010/2065 [55:20<56:16,  3.20s/it] 49%|████▉     | 1011/2065 [55:23<55:11,  3.14s/it] 49%|████▉     | 1012/2065 [55:26<54:24,  3.10s/it] 49%|████▉     | 1013/2065 [55:29<53:50,  3.07s/it] 49%|████▉     | 1014/2065 [55:32<53:25,  3.05s/it] 49%|████▉     | 1015/2065 [55:35<53:08,  3.04s/it] 49%|████▉     | 1016/2065 [55:38<52:55,  3.03s/it] 49%|████▉     | 1017/2065 [55:41<52:44,  3.02s/it] 49%|████▉     | 1018/2065 [55:44<52:37,  3.02s/it] 49%|████▉     | 1019/2065 [55:47<52:30,  3.01s/it] 49%|████▉     | 1020/2065 [55:50<52:25,  3.01s/it]                                                   {'loss': 2.305, 'grad_norm': 0.2821630835533142, 'learning_rate': 1.0376287770151258e-05, 'epoch': 2.47}
 49%|████▉     | 1020/2065 [55:50<52:25,  3.01s/it] 49%|████▉     | 1021/2065 [55:53<52:22,  3.01s/it] 49%|████▉     | 1022/2065 [55:56<52:17,  3.01s/it] 50%|████▉     | 1023/2065 [55:59<52:09,  3.00s/it] 50%|████▉     | 1024/2065 [56:02<52:06,  3.00s/it] 50%|████▉     | 1025/2065 [56:05<52:03,  3.00s/it] 50%|████▉     | 1026/2065 [56:08<52:00,  3.00s/it] 50%|████▉     | 1027/2065 [56:11<51:58,  3.00s/it] 50%|████▉     | 1028/2065 [56:14<51:54,  3.00s/it] 50%|████▉     | 1029/2065 [56:17<51:51,  3.00s/it] 50%|████▉     | 1030/2065 [56:20<51:49,  3.00s/it]                                                   {'loss': 2.3046, 'grad_norm': 0.3077257573604584, 'learning_rate': 1.022273509240053e-05, 'epoch': 2.49}
 50%|████▉     | 1030/2065 [56:20<51:49,  3.00s/it] 50%|████▉     | 1031/2065 [56:23<51:48,  3.01s/it] 50%|████▉     | 1032/2065 [56:26<51:41,  3.00s/it] 50%|█████     | 1033/2065 [56:29<51:38,  3.00s/it] 50%|█████     | 1034/2065 [56:32<51:35,  3.00s/it] 50%|█████     | 1035/2065 [56:35<51:33,  3.00s/it] 50%|█████     | 1036/2065 [56:38<51:31,  3.00s/it] 50%|█████     | 1037/2065 [56:41<51:28,  3.00s/it] 50%|█████     | 1038/2065 [56:44<51:25,  3.00s/it] 50%|█████     | 1039/2065 [56:47<51:23,  3.01s/it] 50%|█████     | 1040/2065 [56:50<51:20,  3.01s/it]                                                   {'loss': 2.2694, 'grad_norm': 0.4377029836177826, 'learning_rate': 1.0069129850066498e-05, 'epoch': 2.52}
 50%|█████     | 1040/2065 [56:50<51:20,  3.01s/it] 50%|█████     | 1041/2065 [56:53<51:17,  3.01s/it] 50%|█████     | 1042/2065 [56:56<51:15,  3.01s/it] 51%|█████     | 1043/2065 [56:59<51:12,  3.01s/it] 51%|█████     | 1044/2065 [57:02<51:09,  3.01s/it] 51%|█████     | 1045/2065 [57:05<51:07,  3.01s/it] 51%|█████     | 1046/2065 [57:08<51:03,  3.01s/it] 51%|█████     | 1047/2065 [57:11<50:59,  3.01s/it] 51%|█████     | 1048/2065 [57:14<50:57,  3.01s/it] 51%|█████     | 1049/2065 [57:17<50:54,  3.01s/it] 51%|█████     | 1050/2065 [57:20<50:49,  3.00s/it]                                                   {'loss': 2.3482, 'grad_norm': 0.2836238443851471, 'learning_rate': 9.91550829336671e-06, 'epoch': 2.54}
 51%|█████     | 1050/2065 [57:20<50:49,  3.00s/it][INFO|trainer.py:3203] 2024-06-24 23:21:52,795 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1050
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63ca3e34d0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: b2288d80-a2f8-4d19-9928-8b4ac908f52b)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-24 23:22:02,861 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1050/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-24 23:22:02,863 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1050/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 51%|█████     | 1051/2065 [57:33<1:41:02,  5.98s/it] 51%|█████     | 1052/2065 [57:35<1:24:42,  5.02s/it] 51%|█████     | 1053/2065 [57:38<1:13:40,  4.37s/it] 51%|█████     | 1054/2065 [57:41<1:06:02,  3.92s/it] 51%|█████     | 1055/2065 [57:44<1:00:54,  3.62s/it] 51%|█████     | 1056/2065 [57:47<57:24,  3.41s/it]   51%|█████     | 1057/2065 [57:50<54:59,  3.27s/it] 51%|█████     | 1058/2065 [57:53<53:20,  3.18s/it] 51%|█████▏    | 1059/2065 [57:56<52:11,  3.11s/it] 51%|█████▏    | 1060/2065 [57:59<51:22,  3.07s/it]                                                   {'loss': 2.3101, 'grad_norm': 0.2870328724384308, 'learning_rate': 9.761906676368861e-06, 'epoch': 2.57}
 51%|█████▏    | 1060/2065 [57:59<51:22,  3.07s/it] 51%|█████▏    | 1061/2065 [58:02<50:51,  3.04s/it] 51%|█████▏    | 1062/2065 [58:05<50:28,  3.02s/it] 51%|█████▏    | 1063/2065 [58:08<50:13,  3.01s/it] 52%|█████▏    | 1064/2065 [58:11<50:01,  3.00s/it] 52%|█████▏    | 1065/2065 [58:14<49:54,  2.99s/it] 52%|█████▏    | 1066/2065 [58:17<49:48,  2.99s/it] 52%|█████▏    | 1067/2065 [58:20<49:43,  2.99s/it] 52%|█████▏    | 1068/2065 [58:23<49:40,  2.99s/it] 52%|█████▏    | 1069/2065 [58:26<49:36,  2.99s/it] 52%|█████▏    | 1070/2065 [58:29<49:34,  2.99s/it]                                                   {'loss': 2.2759, 'grad_norm': 0.3016309142112732, 'learning_rate': 9.608361248434932e-06, 'epoch': 2.59}
 52%|█████▏    | 1070/2065 [58:29<49:34,  2.99s/it] 52%|█████▏    | 1071/2065 [58:32<49:41,  3.00s/it] 52%|█████▏    | 1072/2065 [58:35<49:34,  3.00s/it] 52%|█████▏    | 1073/2065 [58:38<49:31,  3.00s/it] 52%|█████▏    | 1074/2065 [58:41<49:37,  3.00s/it] 52%|█████▏    | 1075/2065 [58:44<49:31,  3.00s/it] 52%|█████▏    | 1076/2065 [58:47<49:28,  3.00s/it] 52%|█████▏    | 1077/2065 [58:50<49:25,  3.00s/it] 52%|█████▏    | 1078/2065 [58:53<49:21,  3.00s/it] 52%|█████▏    | 1079/2065 [58:56<49:18,  3.00s/it] 52%|█████▏    | 1080/2065 [58:59<49:15,  3.00s/it]                                                   {'loss': 2.309, 'grad_norm': 0.2848656475543976, 'learning_rate': 9.454908245666528e-06, 'epoch': 2.62}
 52%|█████▏    | 1080/2065 [58:59<49:15,  3.00s/it] 52%|█████▏    | 1081/2065 [59:02<49:14,  3.00s/it] 52%|█████▏    | 1082/2065 [59:05<49:12,  3.00s/it] 52%|█████▏    | 1083/2065 [59:08<49:09,  3.00s/it] 52%|█████▏    | 1084/2065 [59:11<49:04,  3.00s/it] 53%|█████▎    | 1085/2065 [59:14<49:02,  3.00s/it] 53%|█████▎    | 1086/2065 [59:17<48:58,  3.00s/it] 53%|█████▎    | 1087/2065 [59:20<50:01,  3.07s/it] 53%|█████▎    | 1088/2065 [59:23<50:12,  3.08s/it] 53%|█████▎    | 1089/2065 [59:26<49:44,  3.06s/it] 53%|█████▎    | 1090/2065 [59:29<49:25,  3.04s/it]                                                   {'loss': 2.2542, 'grad_norm': 0.31562379002571106, 'learning_rate': 9.301583882353282e-06, 'epoch': 2.64}
 53%|█████▎    | 1090/2065 [59:29<49:25,  3.04s/it] 53%|█████▎    | 1091/2065 [59:32<49:09,  3.03s/it] 53%|█████▎    | 1092/2065 [59:35<48:58,  3.02s/it] 53%|█████▎    | 1093/2065 [59:38<48:49,  3.01s/it] 53%|█████▎    | 1094/2065 [59:41<48:43,  3.01s/it] 53%|█████▎    | 1095/2065 [59:44<48:37,  3.01s/it] 53%|█████▎    | 1096/2065 [59:47<48:32,  3.01s/it] 53%|█████▎    | 1097/2065 [59:50<48:27,  3.00s/it] 53%|█████▎    | 1098/2065 [59:53<48:24,  3.00s/it] 53%|█████▎    | 1099/2065 [59:56<48:20,  3.00s/it] 53%|█████▎    | 1100/2065 [59:59<48:14,  3.00s/it]                                                   {'loss': 2.2792, 'grad_norm': 0.33976656198501587, 'learning_rate': 9.148424342426436e-06, 'epoch': 2.66}
 53%|█████▎    | 1100/2065 [59:59<48:14,  3.00s/it][INFO|trainer.py:3512] 2024-06-24 23:24:32,154 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 23:24:32,154 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 23:24:32,155 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.35it/s][A
 27%|██▋       | 3/11 [00:02<00:08,  1.06s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.23s/it][A
 45%|████▌     | 5/11 [00:05<00:07,  1.32s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.38s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.42s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.44s/it][A
 82%|████████▏ | 9/11 [00:11<00:02,  1.46s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.47s/it][A
100%|██████████| 11/11 [00:14<00:00,  1.48s/it][A                                                   
                                               [A{'eval_loss': 2.2686309814453125, 'eval_runtime': 16.4995, 'eval_samples_per_second': 15.819, 'eval_steps_per_second': 0.667, 'epoch': 2.66}
 53%|█████▎    | 1100/2065 [1:00:16<48:14,  3.00s/it]
100%|██████████| 11/11 [00:15<00:00,  1.48s/it][A
                                               [A 53%|█████▎    | 1101/2065 [1:00:19<2:08:28,  8.00s/it] 53%|█████▎    | 1102/2065 [1:00:22<1:44:19,  6.50s/it] 53%|█████▎    | 1103/2065 [1:00:25<1:27:21,  5.45s/it] 53%|█████▎    | 1104/2065 [1:00:28<1:15:28,  4.71s/it] 54%|█████▎    | 1105/2065 [1:00:31<1:07:10,  4.20s/it] 54%|█████▎    | 1106/2065 [1:00:34<1:01:17,  3.84s/it] 54%|█████▎    | 1107/2065 [1:00:37<57:13,  3.58s/it]   54%|█████▎    | 1108/2065 [1:00:40<54:21,  3.41s/it] 54%|█████▎    | 1109/2065 [1:00:43<52:17,  3.28s/it] 54%|█████▍    | 1110/2065 [1:00:46<50:52,  3.20s/it]                                                     {'loss': 2.3144, 'grad_norm': 0.3092087209224701, 'learning_rate': 8.995465770919581e-06, 'epoch': 2.69}
 54%|█████▍    | 1110/2065 [1:00:46<50:52,  3.20s/it] 54%|█████▍    | 1111/2065 [1:00:49<50:00,  3.14s/it] 54%|█████▍    | 1112/2065 [1:00:52<49:13,  3.10s/it] 54%|█████▍    | 1113/2065 [1:00:55<48:42,  3.07s/it] 54%|█████▍    | 1114/2065 [1:00:58<48:20,  3.05s/it] 54%|█████▍    | 1115/2065 [1:01:01<48:03,  3.03s/it] 54%|█████▍    | 1116/2065 [1:01:04<47:50,  3.03s/it] 54%|█████▍    | 1117/2065 [1:01:07<47:40,  3.02s/it] 54%|█████▍    | 1118/2065 [1:01:10<47:33,  3.01s/it] 54%|█████▍    | 1119/2065 [1:01:13<47:28,  3.01s/it] 54%|█████▍    | 1120/2065 [1:01:16<47:23,  3.01s/it]                                                     {'loss': 2.3003, 'grad_norm': 0.26659324765205383, 'learning_rate': 8.842744265438561e-06, 'epoch': 2.71}
 54%|█████▍    | 1120/2065 [1:01:16<47:23,  3.01s/it] 54%|█████▍    | 1121/2065 [1:01:19<47:18,  3.01s/it] 54%|█████▍    | 1122/2065 [1:01:22<47:28,  3.02s/it] 54%|█████▍    | 1123/2065 [1:01:25<47:22,  3.02s/it] 54%|█████▍    | 1124/2065 [1:01:28<47:15,  3.01s/it] 54%|█████▍    | 1125/2065 [1:01:31<47:11,  3.01s/it] 55%|█████▍    | 1126/2065 [1:01:34<47:07,  3.01s/it] 55%|█████▍    | 1127/2065 [1:01:37<47:03,  3.01s/it] 55%|█████▍    | 1128/2065 [1:01:40<46:58,  3.01s/it] 55%|█████▍    | 1129/2065 [1:01:43<47:28,  3.04s/it] 55%|█████▍    | 1130/2065 [1:01:46<47:14,  3.03s/it]                                                     {'loss': 2.2703, 'grad_norm': 0.29434770345687866, 'learning_rate': 8.690295867642606e-06, 'epoch': 2.74}
 55%|█████▍    | 1130/2065 [1:01:46<47:14,  3.03s/it] 55%|█████▍    | 1131/2065 [1:01:49<47:07,  3.03s/it] 55%|█████▍    | 1132/2065 [1:01:52<46:57,  3.02s/it] 55%|█████▍    | 1133/2065 [1:01:55<46:50,  3.02s/it] 55%|█████▍    | 1134/2065 [1:01:58<46:44,  3.01s/it] 55%|█████▍    | 1135/2065 [1:02:01<46:39,  3.01s/it] 55%|█████▌    | 1136/2065 [1:02:04<46:34,  3.01s/it] 55%|█████▌    | 1137/2065 [1:02:07<46:30,  3.01s/it] 55%|█████▌    | 1138/2065 [1:02:10<46:27,  3.01s/it] 55%|█████▌    | 1139/2065 [1:02:13<46:23,  3.01s/it] 55%|█████▌    | 1140/2065 [1:02:16<46:23,  3.01s/it]                                                     {'loss': 2.2723, 'grad_norm': 0.3055500388145447, 'learning_rate': 8.538156554738628e-06, 'epoch': 2.76}
 55%|█████▌    | 1140/2065 [1:02:16<46:23,  3.01s/it] 55%|█████▌    | 1141/2065 [1:02:19<46:20,  3.01s/it] 55%|█████▌    | 1142/2065 [1:02:22<46:15,  3.01s/it] 55%|█████▌    | 1143/2065 [1:02:25<46:12,  3.01s/it] 55%|█████▌    | 1144/2065 [1:02:28<46:09,  3.01s/it] 55%|█████▌    | 1145/2065 [1:02:31<46:04,  3.00s/it] 55%|█████▌    | 1146/2065 [1:02:34<46:01,  3.01s/it] 56%|█████▌    | 1147/2065 [1:02:37<45:58,  3.01s/it] 56%|█████▌    | 1148/2065 [1:02:40<45:55,  3.01s/it] 56%|█████▌    | 1149/2065 [1:02:43<45:52,  3.01s/it] 56%|█████▌    | 1150/2065 [1:02:46<45:49,  3.00s/it]                                                     {'loss': 2.2858, 'grad_norm': 0.31046605110168457, 'learning_rate': 8.386362230990754e-06, 'epoch': 2.78}
 56%|█████▌    | 1150/2065 [1:02:46<45:49,  3.00s/it] 56%|█████▌    | 1151/2065 [1:02:49<45:44,  3.00s/it] 56%|█████▌    | 1152/2065 [1:02:52<45:42,  3.00s/it] 56%|█████▌    | 1153/2065 [1:02:55<45:40,  3.00s/it] 56%|█████▌    | 1154/2065 [1:02:58<45:33,  3.00s/it] 56%|█████▌    | 1155/2065 [1:03:01<45:31,  3.00s/it] 56%|█████▌    | 1156/2065 [1:03:04<45:30,  3.00s/it] 56%|█████▌    | 1157/2065 [1:03:07<45:26,  3.00s/it] 56%|█████▌    | 1158/2065 [1:03:10<45:24,  3.00s/it] 56%|█████▌    | 1159/2065 [1:03:13<45:21,  3.00s/it] 56%|█████▌    | 1160/2065 [1:03:16<45:17,  3.00s/it]                                                     {'loss': 2.3357, 'grad_norm': 0.3135664165019989, 'learning_rate': 8.234948719247063e-06, 'epoch': 2.81}
 56%|█████▌    | 1160/2065 [1:03:16<45:17,  3.00s/it] 56%|█████▌    | 1161/2065 [1:03:19<45:14,  3.00s/it] 56%|█████▋    | 1162/2065 [1:03:22<45:12,  3.00s/it] 56%|█████▋    | 1163/2065 [1:03:25<45:06,  3.00s/it] 56%|█████▋    | 1164/2065 [1:03:28<45:04,  3.00s/it] 56%|█████▋    | 1165/2065 [1:03:31<45:01,  3.00s/it] 56%|█████▋    | 1166/2065 [1:03:34<45:03,  3.01s/it] 57%|█████▋    | 1167/2065 [1:03:37<45:00,  3.01s/it] 57%|█████▋    | 1168/2065 [1:03:40<44:57,  3.01s/it] 57%|█████▋    | 1169/2065 [1:03:43<44:49,  3.00s/it] 57%|█████▋    | 1170/2065 [1:03:46<44:47,  3.00s/it]                                                     {'loss': 2.2959, 'grad_norm': 0.325387179851532, 'learning_rate': 8.083951752485524e-06, 'epoch': 2.83}
 57%|█████▋    | 1170/2065 [1:03:46<44:47,  3.00s/it] 57%|█████▋    | 1171/2065 [1:03:49<44:45,  3.00s/it] 57%|█████▋    | 1172/2065 [1:03:52<44:43,  3.01s/it] 57%|█████▋    | 1173/2065 [1:03:55<44:40,  3.01s/it] 57%|█████▋    | 1174/2065 [1:03:58<44:36,  3.00s/it] 57%|█████▋    | 1175/2065 [1:04:01<44:30,  3.00s/it] 57%|█████▋    | 1176/2065 [1:04:04<44:28,  3.00s/it] 57%|█████▋    | 1177/2065 [1:04:07<44:26,  3.00s/it] 57%|█████▋    | 1178/2065 [1:04:10<44:21,  3.00s/it] 57%|█████▋    | 1179/2065 [1:04:13<44:19,  3.00s/it] 57%|█████▋    | 1180/2065 [1:04:16<44:16,  3.00s/it]                                                     {'loss': 2.2808, 'grad_norm': 0.28249868750572205, 'learning_rate': 7.93340696538119e-06, 'epoch': 2.86}
 57%|█████▋    | 1180/2065 [1:04:16<44:16,  3.00s/it] 57%|█████▋    | 1181/2065 [1:04:19<44:13,  3.00s/it] 57%|█████▋    | 1182/2065 [1:04:22<44:10,  3.00s/it] 57%|█████▋    | 1183/2065 [1:04:25<44:07,  3.00s/it] 57%|█████▋    | 1184/2065 [1:04:28<44:03,  3.00s/it] 57%|█████▋    | 1185/2065 [1:04:31<44:00,  3.00s/it] 57%|█████▋    | 1186/2065 [1:04:34<43:57,  3.00s/it] 57%|█████▋    | 1187/2065 [1:04:37<43:55,  3.00s/it] 58%|█████▊    | 1188/2065 [1:04:40<43:52,  3.00s/it] 58%|█████▊    | 1189/2065 [1:04:43<43:49,  3.00s/it] 58%|█████▊    | 1190/2065 [1:04:46<43:46,  3.00s/it]                                                     {'loss': 2.2904, 'grad_norm': 0.3053905963897705, 'learning_rate': 7.783349885896524e-06, 'epoch': 2.88}
 58%|█████▊    | 1190/2065 [1:04:46<43:46,  3.00s/it] 58%|█████▊    | 1191/2065 [1:04:49<43:45,  3.00s/it] 58%|█████▊    | 1192/2065 [1:04:52<43:42,  3.00s/it] 58%|█████▊    | 1193/2065 [1:04:55<43:38,  3.00s/it] 58%|█████▊    | 1194/2065 [1:04:58<43:35,  3.00s/it] 58%|█████▊    | 1195/2065 [1:05:01<43:31,  3.00s/it] 58%|█████▊    | 1196/2065 [1:05:04<43:27,  3.00s/it] 58%|█████▊    | 1197/2065 [1:05:07<43:24,  3.00s/it] 58%|█████▊    | 1198/2065 [1:05:10<44:04,  3.05s/it] 58%|█████▊    | 1199/2065 [1:05:14<47:06,  3.26s/it] 58%|█████▊    | 1200/2065 [1:05:17<45:49,  3.18s/it]                                                     {'loss': 2.277, 'grad_norm': 0.3028755486011505, 'learning_rate': 7.633815926896972e-06, 'epoch': 2.91}
 58%|█████▊    | 1200/2065 [1:05:17<45:49,  3.18s/it][INFO|trainer.py:3512] 2024-06-24 23:29:50,524 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 23:29:50,524 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 23:29:50,524 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.38it/s][A
 27%|██▋       | 3/11 [00:02<00:08,  1.04s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.21s/it][A
 45%|████▌     | 5/11 [00:05<00:07,  1.31s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.37s/it][A
 64%|██████▎   | 7/11 [00:08<00:05,  1.41s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.44s/it][A
 82%|████████▏ | 9/11 [00:11<00:02,  1.46s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.47s/it][A
100%|██████████| 11/11 [00:14<00:00,  1.48s/it][A                                                     
                                               [A{'eval_loss': 2.2667648792266846, 'eval_runtime': 16.7716, 'eval_samples_per_second': 15.562, 'eval_steps_per_second': 0.656, 'epoch': 2.91}
 58%|█████▊    | 1200/2065 [1:05:34<45:49,  3.18s/it]
100%|██████████| 11/11 [00:15<00:00,  1.48s/it][A
                                               [A[INFO|trainer.py:3203] 2024-06-24 23:30:07,300 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1200
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63cb512c90>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: bf29f9a1-26d9-4f46-83de-6bac0fc1dc99)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-24 23:30:18,333 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-24 23:30:18,336 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1200/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 58%|█████▊    | 1201/2065 [1:05:49<2:49:51, 11.80s/it] 58%|█████▊    | 1202/2065 [1:05:52<2:10:38,  9.08s/it] 58%|█████▊    | 1203/2065 [1:05:54<1:43:28,  7.20s/it] 58%|█████▊    | 1204/2065 [1:05:57<1:24:37,  5.90s/it] 58%|█████▊    | 1205/2065 [1:06:00<1:11:30,  4.99s/it] 58%|█████▊    | 1206/2065 [1:06:03<1:02:29,  4.37s/it] 58%|█████▊    | 1207/2065 [1:06:06<56:14,  3.93s/it]   58%|█████▊    | 1208/2065 [1:06:09<51:52,  3.63s/it] 59%|█████▊    | 1209/2065 [1:06:12<48:52,  3.43s/it] 59%|█████▊    | 1210/2065 [1:06:15<46:48,  3.29s/it]                                                     {'loss': 2.2896, 'grad_norm': 0.30208733677864075, 'learning_rate': 7.48484037779364e-06, 'epoch': 2.93}
 59%|█████▊    | 1210/2065 [1:06:15<46:48,  3.29s/it] 59%|█████▊    | 1211/2065 [1:06:18<45:20,  3.19s/it] 59%|█████▊    | 1212/2065 [1:06:21<44:21,  3.12s/it] 59%|█████▊    | 1213/2065 [1:06:24<43:37,  3.07s/it] 59%|█████▉    | 1214/2065 [1:06:27<43:09,  3.04s/it] 59%|█████▉    | 1215/2065 [1:06:30<42:48,  3.02s/it] 59%|█████▉    | 1216/2065 [1:06:33<42:33,  3.01s/it] 59%|█████▉    | 1217/2065 [1:06:36<42:21,  3.00s/it] 59%|█████▉    | 1218/2065 [1:06:39<42:14,  2.99s/it] 59%|█████▉    | 1219/2065 [1:06:42<42:08,  2.99s/it] 59%|█████▉    | 1220/2065 [1:06:45<42:03,  2.99s/it]                                                     {'loss': 2.3423, 'grad_norm': 0.31989964842796326, 'learning_rate': 7.336458396215164e-06, 'epoch': 2.95}
 59%|█████▉    | 1220/2065 [1:06:45<42:03,  2.99s/it] 59%|█████▉    | 1221/2065 [1:06:48<41:59,  2.99s/it] 59%|█████▉    | 1222/2065 [1:06:51<41:56,  2.99s/it] 59%|█████▉    | 1223/2065 [1:06:54<41:54,  2.99s/it] 59%|█████▉    | 1224/2065 [1:06:57<41:49,  2.98s/it] 59%|█████▉    | 1225/2065 [1:07:00<41:47,  2.99s/it] 59%|█████▉    | 1226/2065 [1:07:03<41:45,  2.99s/it] 59%|█████▉    | 1227/2065 [1:07:05<41:43,  2.99s/it] 59%|█████▉    | 1228/2065 [1:07:08<41:41,  2.99s/it] 60%|█████▉    | 1229/2065 [1:07:11<41:39,  2.99s/it] 60%|█████▉    | 1230/2065 [1:07:14<41:37,  2.99s/it]                                                     {'loss': 2.2796, 'grad_norm': 0.28472399711608887, 'learning_rate': 7.1887049997106305e-06, 'epoch': 2.98}
 60%|█████▉    | 1230/2065 [1:07:14<41:37,  2.99s/it] 60%|█████▉    | 1231/2065 [1:07:17<41:32,  2.99s/it] 60%|█████▉    | 1232/2065 [1:07:20<41:30,  2.99s/it] 60%|█████▉    | 1233/2065 [1:07:23<41:27,  2.99s/it] 60%|█████▉    | 1234/2065 [1:07:26<41:25,  2.99s/it] 60%|█████▉    | 1235/2065 [1:07:29<41:23,  2.99s/it] 60%|█████▉    | 1236/2065 [1:07:32<41:17,  2.99s/it] 60%|█████▉    | 1237/2065 [1:07:35<41:16,  2.99s/it] 60%|█████▉    | 1238/2065 [1:07:38<41:14,  2.99s/it] 60%|██████    | 1239/2065 [1:07:41<41:12,  2.99s/it] 60%|██████    | 1240/2065 [1:07:44<41:10,  2.99s/it]                                                     {'loss': 2.2701, 'grad_norm': 0.2982970178127289, 'learning_rate': 7.041615057485589e-06, 'epoch': 3.0}
 60%|██████    | 1240/2065 [1:07:44<41:10,  2.99s/it] 60%|██████    | 1241/2065 [1:07:47<41:07,  2.99s/it] 60%|██████    | 1242/2065 [1:07:50<41:04,  2.99s/it] 60%|██████    | 1243/2065 [1:07:53<41:02,  3.00s/it] 60%|██████    | 1244/2065 [1:07:56<41:06,  3.00s/it] 60%|██████    | 1245/2065 [1:07:59<41:02,  3.00s/it] 60%|██████    | 1246/2065 [1:08:02<40:58,  3.00s/it] 60%|██████    | 1247/2065 [1:08:05<40:51,  3.00s/it] 60%|██████    | 1248/2065 [1:08:08<40:47,  3.00s/it] 60%|██████    | 1249/2065 [1:08:11<40:45,  3.00s/it] 61%|██████    | 1250/2065 [1:08:14<40:42,  3.00s/it]                                                     {'loss': 2.2863, 'grad_norm': 0.29939451813697815, 'learning_rate': 6.895223282173058e-06, 'epoch': 3.03}
 61%|██████    | 1250/2065 [1:08:14<40:42,  3.00s/it] 61%|██████    | 1251/2065 [1:08:17<40:40,  3.00s/it] 61%|██████    | 1252/2065 [1:08:20<40:38,  3.00s/it] 61%|██████    | 1253/2065 [1:08:23<40:35,  3.00s/it] 61%|██████    | 1254/2065 [1:08:26<40:32,  3.00s/it] 61%|██████    | 1255/2065 [1:08:29<40:28,  3.00s/it] 61%|██████    | 1256/2065 [1:08:32<40:25,  3.00s/it] 61%|██████    | 1257/2065 [1:08:35<40:22,  3.00s/it] 61%|██████    | 1258/2065 [1:08:38<40:16,  2.99s/it] 61%|██████    | 1259/2065 [1:08:41<40:14,  3.00s/it] 61%|██████    | 1260/2065 [1:08:44<40:12,  3.00s/it]                                                     {'loss': 2.296, 'grad_norm': 0.31764599680900574, 'learning_rate': 6.749564221641466e-06, 'epoch': 3.05}
 61%|██████    | 1260/2065 [1:08:44<40:12,  3.00s/it] 61%|██████    | 1261/2065 [1:08:47<40:10,  3.00s/it] 61%|██████    | 1262/2065 [1:08:50<40:08,  3.00s/it] 61%|██████    | 1263/2065 [1:08:53<40:05,  3.00s/it] 61%|██████    | 1264/2065 [1:08:56<40:02,  3.00s/it] 61%|██████▏   | 1265/2065 [1:08:59<39:59,  3.00s/it] 61%|██████▏   | 1266/2065 [1:09:02<39:56,  3.00s/it] 61%|██████▏   | 1267/2065 [1:09:05<39:52,  3.00s/it] 61%|██████▏   | 1268/2065 [1:09:08<39:50,  3.00s/it] 61%|██████▏   | 1269/2065 [1:09:11<39:45,  3.00s/it] 62%|██████▏   | 1270/2065 [1:09:14<39:42,  3.00s/it]                                                     {'loss': 2.2793, 'grad_norm': 0.32264843583106995, 'learning_rate': 6.604672250841526e-06, 'epoch': 3.08}
 62%|██████▏   | 1270/2065 [1:09:14<39:42,  3.00s/it] 62%|██████▏   | 1271/2065 [1:09:17<39:41,  3.00s/it] 62%|██████▏   | 1272/2065 [1:09:20<39:39,  3.00s/it] 62%|██████▏   | 1273/2065 [1:09:23<39:37,  3.00s/it] 62%|██████▏   | 1274/2065 [1:09:26<39:33,  3.00s/it] 62%|██████▏   | 1275/2065 [1:09:29<39:30,  3.00s/it] 62%|██████▏   | 1276/2065 [1:09:32<39:27,  3.00s/it] 62%|██████▏   | 1277/2065 [1:09:35<39:24,  3.00s/it] 62%|██████▏   | 1278/2065 [1:09:38<39:19,  3.00s/it] 62%|██████▏   | 1279/2065 [1:09:41<39:17,  3.00s/it] 62%|██████▏   | 1280/2065 [1:09:44<39:14,  3.00s/it]                                                     {'loss': 2.3252, 'grad_norm': 0.29796168208122253, 'learning_rate': 6.4605815636938595e-06, 'epoch': 3.1}
 62%|██████▏   | 1280/2065 [1:09:44<39:14,  3.00s/it] 62%|██████▏   | 1281/2065 [1:09:47<39:11,  3.00s/it] 62%|██████▏   | 1282/2065 [1:09:50<39:09,  3.00s/it] 62%|██████▏   | 1283/2065 [1:09:53<39:05,  3.00s/it] 62%|██████▏   | 1284/2065 [1:09:56<39:02,  3.00s/it] 62%|██████▏   | 1285/2065 [1:09:59<39:00,  3.00s/it] 62%|██████▏   | 1286/2065 [1:10:02<38:56,  3.00s/it] 62%|██████▏   | 1287/2065 [1:10:05<38:54,  3.00s/it] 62%|██████▏   | 1288/2065 [1:10:08<38:52,  3.00s/it] 62%|██████▏   | 1289/2065 [1:10:11<38:48,  3.00s/it] 62%|██████▏   | 1290/2065 [1:10:14<38:46,  3.00s/it]                                                     {'loss': 2.2667, 'grad_norm': 0.3174636960029602, 'learning_rate': 6.317326165019384e-06, 'epoch': 3.12}
 62%|██████▏   | 1290/2065 [1:10:14<38:46,  3.00s/it] 63%|██████▎   | 1291/2065 [1:10:17<38:43,  3.00s/it] 63%|██████▎   | 1292/2065 [1:10:20<38:39,  3.00s/it] 63%|██████▎   | 1293/2065 [1:10:23<38:36,  3.00s/it] 63%|██████▎   | 1294/2065 [1:10:26<38:33,  3.00s/it] 63%|██████▎   | 1295/2065 [1:10:29<38:27,  3.00s/it] 63%|██████▎   | 1296/2065 [1:10:32<38:26,  3.00s/it] 63%|██████▎   | 1297/2065 [1:10:35<38:24,  3.00s/it] 63%|██████▎   | 1298/2065 [1:10:38<38:24,  3.00s/it] 63%|██████▎   | 1299/2065 [1:10:41<38:20,  3.00s/it] 63%|██████▎   | 1300/2065 [1:10:44<38:17,  3.00s/it]                                                     {'loss': 2.3424, 'grad_norm': 0.3110792338848114, 'learning_rate': 6.174939862514309e-06, 'epoch': 3.15}
 63%|██████▎   | 1300/2065 [1:10:44<38:17,  3.00s/it][INFO|trainer.py:3512] 2024-06-24 23:35:17,484 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 23:35:17,484 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 23:35:17,484 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.34it/s][A
 27%|██▋       | 3/11 [00:02<00:08,  1.06s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.22s/it][A
 45%|████▌     | 5/11 [00:05<00:07,  1.32s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.38s/it][A
 64%|██████▎   | 7/11 [00:08<00:05,  1.42s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.44s/it][A
 82%|████████▏ | 9/11 [00:11<00:02,  1.46s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.47s/it][A
100%|██████████| 11/11 [00:14<00:00,  1.48s/it][A                                                     
                                               [A{'eval_loss': 2.265254497528076, 'eval_runtime': 16.4681, 'eval_samples_per_second': 15.849, 'eval_steps_per_second': 0.668, 'epoch': 3.15}
 63%|██████▎   | 1300/2065 [1:11:01<38:17,  3.00s/it]
100%|██████████| 11/11 [00:15<00:00,  1.48s/it][A
                                               [A 63%|██████▎   | 1301/2065 [1:11:04<1:41:08,  7.94s/it] 63%|██████▎   | 1302/2065 [1:11:07<1:22:04,  6.45s/it] 63%|██████▎   | 1303/2065 [1:11:10<1:08:46,  5.42s/it] 63%|██████▎   | 1304/2065 [1:11:13<59:27,  4.69s/it]   63%|██████▎   | 1305/2065 [1:11:16<52:56,  4.18s/it] 63%|██████▎   | 1306/2065 [1:11:19<48:21,  3.82s/it] 63%|██████▎   | 1307/2065 [1:11:22<45:06,  3.57s/it] 63%|██████▎   | 1308/2065 [1:11:25<42:52,  3.40s/it] 63%|██████▎   | 1309/2065 [1:11:28<41:17,  3.28s/it] 63%|██████▎   | 1310/2065 [1:11:31<40:10,  3.19s/it]                                                     {'loss': 2.2866, 'grad_norm': 0.2961253523826599, 'learning_rate': 6.033456258771636e-06, 'epoch': 3.17}
 63%|██████▎   | 1310/2065 [1:11:31<40:10,  3.19s/it] 63%|██████▎   | 1311/2065 [1:11:34<39:24,  3.14s/it] 64%|██████▎   | 1312/2065 [1:11:37<38:49,  3.09s/it] 64%|██████▎   | 1313/2065 [1:11:40<38:25,  3.07s/it] 64%|██████▎   | 1314/2065 [1:11:43<38:07,  3.05s/it] 64%|██████▎   | 1315/2065 [1:11:46<37:54,  3.03s/it] 64%|██████▎   | 1316/2065 [1:11:49<37:45,  3.02s/it] 64%|██████▍   | 1317/2065 [1:11:52<37:37,  3.02s/it] 64%|██████▍   | 1318/2065 [1:11:55<37:29,  3.01s/it] 64%|██████▍   | 1319/2065 [1:11:58<37:24,  3.01s/it] 64%|██████▍   | 1320/2065 [1:12:01<37:19,  3.01s/it]                                                     {'loss': 2.2687, 'grad_norm': 0.3244730830192566, 'learning_rate': 5.892908743351121e-06, 'epoch': 3.2}
 64%|██████▍   | 1320/2065 [1:12:01<37:19,  3.01s/it] 64%|██████▍   | 1321/2065 [1:12:04<37:12,  3.00s/it] 64%|██████▍   | 1322/2065 [1:12:07<37:09,  3.00s/it] 64%|██████▍   | 1323/2065 [1:12:10<37:06,  3.00s/it] 64%|██████▍   | 1324/2065 [1:12:13<37:03,  3.00s/it] 64%|██████▍   | 1325/2065 [1:12:16<37:00,  3.00s/it] 64%|██████▍   | 1326/2065 [1:12:19<36:57,  3.00s/it] 64%|██████▍   | 1327/2065 [1:12:22<36:53,  3.00s/it] 64%|██████▍   | 1328/2065 [1:12:25<36:50,  3.00s/it] 64%|██████▍   | 1329/2065 [1:12:28<36:46,  3.00s/it] 64%|██████▍   | 1330/2065 [1:12:31<36:43,  3.00s/it]                                                     {'loss': 2.2471, 'grad_norm': 0.30354931950569153, 'learning_rate': 5.753330484899443e-06, 'epoch': 3.22}
 64%|██████▍   | 1330/2065 [1:12:31<36:43,  3.00s/it] 64%|██████▍   | 1331/2065 [1:12:34<36:51,  3.01s/it] 65%|██████▍   | 1332/2065 [1:12:37<36:45,  3.01s/it] 65%|██████▍   | 1333/2065 [1:12:40<36:40,  3.01s/it] 65%|██████▍   | 1334/2065 [1:12:43<36:36,  3.01s/it] 65%|██████▍   | 1335/2065 [1:12:46<36:32,  3.00s/it] 65%|██████▍   | 1336/2065 [1:12:49<36:28,  3.00s/it] 65%|██████▍   | 1337/2065 [1:12:52<36:25,  3.00s/it] 65%|██████▍   | 1338/2065 [1:12:55<36:21,  3.00s/it] 65%|██████▍   | 1339/2065 [1:12:58<36:18,  3.00s/it] 65%|██████▍   | 1340/2065 [1:13:01<36:15,  3.00s/it]                                                     {'loss': 2.2815, 'grad_norm': 0.3089196979999542, 'learning_rate': 5.614754423322546e-06, 'epoch': 3.24}
 65%|██████▍   | 1340/2065 [1:13:01<36:15,  3.00s/it] 65%|██████▍   | 1341/2065 [1:13:04<36:14,  3.00s/it] 65%|██████▍   | 1342/2065 [1:13:07<36:11,  3.00s/it] 65%|██████▌   | 1343/2065 [1:13:10<36:08,  3.00s/it] 65%|██████▌   | 1344/2065 [1:13:13<36:04,  3.00s/it] 65%|██████▌   | 1345/2065 [1:13:16<36:01,  3.00s/it] 65%|██████▌   | 1346/2065 [1:13:19<35:59,  3.00s/it] 65%|██████▌   | 1347/2065 [1:13:22<35:54,  3.00s/it] 65%|██████▌   | 1348/2065 [1:13:25<35:51,  3.00s/it] 65%|██████▌   | 1349/2065 [1:13:28<35:50,  3.00s/it] 65%|██████▌   | 1350/2065 [1:13:31<36:23,  3.05s/it]                                                     {'loss': 2.2665, 'grad_norm': 0.328138530254364, 'learning_rate': 5.477213262011961e-06, 'epoch': 3.27}
 65%|██████▌   | 1350/2065 [1:13:31<36:23,  3.05s/it][INFO|trainer.py:3203] 2024-06-24 23:38:04,108 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1350
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63cb592c90>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 882f0dce-c63f-4cec-a39a-d038e809137c)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-24 23:38:14,166 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1350/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-24 23:38:14,169 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1350/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 65%|██████▌   | 1351/2065 [1:13:45<1:16:29,  6.43s/it] 65%|██████▌   | 1352/2065 [1:13:48<1:03:16,  5.32s/it] 66%|██████▌   | 1353/2065 [1:13:51<54:13,  4.57s/it]   66%|██████▌   | 1354/2065 [1:13:54<48:03,  4.05s/it] 66%|██████▌   | 1355/2065 [1:13:57<43:49,  3.70s/it] 66%|██████▌   | 1356/2065 [1:14:00<40:57,  3.47s/it] 66%|██████▌   | 1357/2065 [1:14:02<38:59,  3.30s/it] 66%|██████▌   | 1358/2065 [1:14:05<37:39,  3.20s/it] 66%|██████▌   | 1359/2065 [1:14:08<36:45,  3.12s/it] 66%|██████▌   | 1360/2065 [1:14:11<36:06,  3.07s/it]                                                     {'loss': 2.2718, 'grad_norm': 0.3134653568267822, 'learning_rate': 5.340739460126916e-06, 'epoch': 3.29}
 66%|██████▌   | 1360/2065 [1:14:11<36:06,  3.07s/it] 66%|██████▌   | 1361/2065 [1:14:14<35:48,  3.05s/it] 66%|██████▌   | 1362/2065 [1:14:17<35:26,  3.02s/it] 66%|██████▌   | 1363/2065 [1:14:20<35:11,  3.01s/it] 66%|██████▌   | 1364/2065 [1:14:23<35:00,  3.00s/it] 66%|██████▌   | 1365/2065 [1:14:26<34:53,  2.99s/it] 66%|██████▌   | 1366/2065 [1:14:29<34:46,  2.99s/it] 66%|██████▌   | 1367/2065 [1:14:32<34:42,  2.98s/it] 66%|██████▌   | 1368/2065 [1:14:35<34:36,  2.98s/it] 66%|██████▋   | 1369/2065 [1:14:38<34:34,  2.98s/it] 66%|██████▋   | 1370/2065 [1:14:41<34:30,  2.98s/it]                                                     {'loss': 2.3039, 'grad_norm': 0.31915169954299927, 'learning_rate': 5.205365224934136e-06, 'epoch': 3.32}
 66%|██████▋   | 1370/2065 [1:14:41<34:30,  2.98s/it] 66%|██████▋   | 1371/2065 [1:14:44<34:29,  2.98s/it] 66%|██████▋   | 1372/2065 [1:14:47<34:27,  2.98s/it] 66%|██████▋   | 1373/2065 [1:14:50<34:24,  2.98s/it] 67%|██████▋   | 1374/2065 [1:14:53<34:22,  2.98s/it] 67%|██████▋   | 1375/2065 [1:14:56<34:17,  2.98s/it] 67%|██████▋   | 1376/2065 [1:14:59<34:15,  2.98s/it] 67%|██████▋   | 1377/2065 [1:15:02<34:14,  2.99s/it] 67%|██████▋   | 1378/2065 [1:15:05<34:11,  2.99s/it] 67%|██████▋   | 1379/2065 [1:15:08<34:08,  2.99s/it] 67%|██████▋   | 1380/2065 [1:15:11<34:06,  2.99s/it]                                                     {'loss': 2.2295, 'grad_norm': 0.3272944688796997, 'learning_rate': 5.0711225042070326e-06, 'epoch': 3.34}
 67%|██████▋   | 1380/2065 [1:15:11<34:06,  2.99s/it] 67%|██████▋   | 1381/2065 [1:15:14<34:04,  2.99s/it] 67%|██████▋   | 1382/2065 [1:15:17<34:01,  2.99s/it] 67%|██████▋   | 1383/2065 [1:15:20<33:59,  2.99s/it] 67%|██████▋   | 1384/2065 [1:15:23<33:56,  2.99s/it] 67%|██████▋   | 1385/2065 [1:15:26<33:53,  2.99s/it] 67%|██████▋   | 1386/2065 [1:15:29<33:51,  2.99s/it] 67%|██████▋   | 1387/2065 [1:15:32<33:46,  2.99s/it] 67%|██████▋   | 1388/2065 [1:15:35<33:44,  2.99s/it] 67%|██████▋   | 1389/2065 [1:15:38<33:41,  2.99s/it] 67%|██████▋   | 1390/2065 [1:15:41<33:38,  2.99s/it]                                                     {'loss': 2.314, 'grad_norm': 0.343194842338562, 'learning_rate': 4.938042978686173e-06, 'epoch': 3.37}
 67%|██████▋   | 1390/2065 [1:15:41<33:38,  2.99s/it] 67%|██████▋   | 1391/2065 [1:15:44<33:36,  2.99s/it] 67%|██████▋   | 1392/2065 [1:15:47<33:32,  2.99s/it] 67%|██████▋   | 1393/2065 [1:15:50<33:31,  2.99s/it] 68%|██████▊   | 1394/2065 [1:15:53<33:28,  2.99s/it] 68%|██████▊   | 1395/2065 [1:15:56<33:25,  2.99s/it] 68%|██████▊   | 1396/2065 [1:15:59<33:22,  2.99s/it] 68%|██████▊   | 1397/2065 [1:16:02<33:19,  2.99s/it] 68%|██████▊   | 1398/2065 [1:16:05<33:16,  2.99s/it] 68%|██████▊   | 1399/2065 [1:16:08<33:14,  2.99s/it] 68%|██████▊   | 1400/2065 [1:16:11<33:09,  2.99s/it]                                                     {'loss': 2.2927, 'grad_norm': 0.2934901714324951, 'learning_rate': 4.8061580546027464e-06, 'epoch': 3.39}
 68%|██████▊   | 1400/2065 [1:16:11<33:09,  2.99s/it][INFO|trainer.py:3512] 2024-06-24 23:40:43,856 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 23:40:43,856 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 23:40:43,856 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.35it/s][A
 27%|██▋       | 3/11 [00:02<00:08,  1.06s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.23s/it][A
 45%|████▌     | 5/11 [00:05<00:07,  1.32s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.38s/it][A
 64%|██████▎   | 7/11 [00:08<00:05,  1.42s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.44s/it][A
 82%|████████▏ | 9/11 [00:11<00:02,  1.46s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.47s/it][A
100%|██████████| 11/11 [00:14<00:00,  1.47s/it][A                                                     
                                               [A{'eval_loss': 2.2640044689178467, 'eval_runtime': 16.482, 'eval_samples_per_second': 15.835, 'eval_steps_per_second': 0.667, 'epoch': 3.39}
 68%|██████▊   | 1400/2065 [1:16:27<33:09,  2.99s/it]
100%|██████████| 11/11 [00:15<00:00,  1.47s/it][A
                                               [A 68%|██████▊   | 1401/2065 [1:16:30<1:27:51,  7.94s/it] 68%|██████▊   | 1402/2065 [1:16:33<1:11:17,  6.45s/it] 68%|██████▊   | 1403/2065 [1:16:36<59:42,  5.41s/it]   68%|██████▊   | 1404/2065 [1:16:39<51:36,  4.68s/it] 68%|██████▊   | 1405/2065 [1:16:42<45:55,  4.18s/it] 68%|██████▊   | 1406/2065 [1:16:45<41:55,  3.82s/it] 68%|██████▊   | 1407/2065 [1:16:48<39:08,  3.57s/it] 68%|██████▊   | 1408/2065 [1:16:51<37:10,  3.40s/it] 68%|██████▊   | 1409/2065 [1:16:54<35:46,  3.27s/it] 68%|██████▊   | 1410/2065 [1:16:57<34:48,  3.19s/it]                                                     {'loss': 2.2826, 'grad_norm': 0.32084572315216064, 'learning_rate': 4.67549885626682e-06, 'epoch': 3.41}
 68%|██████▊   | 1410/2065 [1:16:57<34:48,  3.19s/it] 68%|██████▊   | 1411/2065 [1:17:00<34:07,  3.13s/it] 68%|██████▊   | 1412/2065 [1:17:03<33:37,  3.09s/it] 68%|██████▊   | 1413/2065 [1:17:06<33:16,  3.06s/it] 68%|██████▊   | 1414/2065 [1:17:09<32:59,  3.04s/it] 69%|██████▊   | 1415/2065 [1:17:12<32:48,  3.03s/it] 69%|██████▊   | 1416/2065 [1:17:15<32:37,  3.02s/it] 69%|██████▊   | 1417/2065 [1:17:18<32:30,  3.01s/it] 69%|██████▊   | 1418/2065 [1:17:21<32:25,  3.01s/it] 69%|██████▊   | 1419/2065 [1:17:24<32:19,  3.00s/it] 69%|██████▉   | 1420/2065 [1:17:27<32:15,  3.00s/it]                                                     {'loss': 2.2732, 'grad_norm': 0.32210448384284973, 'learning_rate': 4.546096218722133e-06, 'epoch': 3.44}
 69%|██████▉   | 1420/2065 [1:17:27<32:15,  3.00s/it] 69%|██████▉   | 1421/2065 [1:17:30<32:11,  3.00s/it] 69%|██████▉   | 1422/2065 [1:17:33<32:08,  3.00s/it] 69%|██████▉   | 1423/2065 [1:17:36<32:05,  3.00s/it] 69%|██████▉   | 1424/2065 [1:17:39<32:01,  3.00s/it] 69%|██████▉   | 1425/2065 [1:17:42<31:58,  3.00s/it] 69%|██████▉   | 1426/2065 [1:17:45<31:55,  3.00s/it] 69%|██████▉   | 1427/2065 [1:17:48<31:52,  3.00s/it] 69%|██████▉   | 1428/2065 [1:17:51<31:49,  3.00s/it] 69%|██████▉   | 1429/2065 [1:17:54<31:46,  3.00s/it] 69%|██████▉   | 1430/2065 [1:17:57<31:42,  3.00s/it]                                                     {'loss': 2.2772, 'grad_norm': 0.3072754442691803, 'learning_rate': 4.417980680469143e-06, 'epoch': 3.46}
 69%|██████▉   | 1430/2065 [1:17:57<31:42,  3.00s/it] 69%|██████▉   | 1431/2065 [1:18:00<31:40,  3.00s/it] 69%|██████▉   | 1432/2065 [1:18:03<31:37,  3.00s/it] 69%|██████▉   | 1433/2065 [1:18:06<31:34,  3.00s/it] 69%|██████▉   | 1434/2065 [1:18:09<31:31,  3.00s/it] 69%|██████▉   | 1435/2065 [1:18:12<31:28,  3.00s/it] 70%|██████▉   | 1436/2065 [1:18:15<31:25,  3.00s/it] 70%|██████▉   | 1437/2065 [1:18:18<31:22,  3.00s/it] 70%|██████▉   | 1438/2065 [1:18:21<31:19,  3.00s/it] 70%|██████▉   | 1439/2065 [1:18:24<31:16,  3.00s/it] 70%|██████▉   | 1440/2065 [1:18:27<31:14,  3.00s/it]                                                     {'loss': 2.2706, 'grad_norm': 0.3244553506374359, 'learning_rate': 4.2911824762580664e-06, 'epoch': 3.49}
 70%|██████▉   | 1440/2065 [1:18:27<31:14,  3.00s/it] 70%|██████▉   | 1441/2065 [1:18:30<31:11,  3.00s/it] 70%|██████▉   | 1442/2065 [1:18:33<31:08,  3.00s/it] 70%|██████▉   | 1443/2065 [1:18:36<31:06,  3.00s/it] 70%|██████▉   | 1444/2065 [1:18:39<31:05,  3.00s/it] 70%|██████▉   | 1445/2065 [1:18:42<31:02,  3.00s/it] 70%|███████   | 1446/2065 [1:18:45<30:59,  3.00s/it] 70%|███████   | 1447/2065 [1:18:48<30:54,  3.00s/it] 70%|███████   | 1448/2065 [1:18:51<30:51,  3.00s/it] 70%|███████   | 1449/2065 [1:18:54<30:49,  3.00s/it] 70%|███████   | 1450/2065 [1:18:57<30:44,  3.00s/it]                                                     {'loss': 2.305, 'grad_norm': 0.34345999360084534, 'learning_rate': 4.1657315299536084e-06, 'epoch': 3.51}
 70%|███████   | 1450/2065 [1:18:57<30:44,  3.00s/it] 70%|███████   | 1451/2065 [1:19:00<30:42,  3.00s/it] 70%|███████   | 1452/2065 [1:19:03<30:38,  3.00s/it] 70%|███████   | 1453/2065 [1:19:06<30:35,  3.00s/it] 70%|███████   | 1454/2065 [1:19:09<30:32,  3.00s/it] 70%|███████   | 1455/2065 [1:19:12<30:27,  3.00s/it] 71%|███████   | 1456/2065 [1:19:15<30:25,  3.00s/it] 71%|███████   | 1457/2065 [1:19:18<30:22,  3.00s/it] 71%|███████   | 1458/2065 [1:19:21<30:18,  3.00s/it] 71%|███████   | 1459/2065 [1:19:24<30:15,  3.00s/it] 71%|███████   | 1460/2065 [1:19:27<30:12,  3.00s/it]                                                     {'loss': 2.273, 'grad_norm': 0.2956121861934662, 'learning_rate': 4.041657447473027e-06, 'epoch': 3.54}
 71%|███████   | 1460/2065 [1:19:27<30:12,  3.00s/it] 71%|███████   | 1461/2065 [1:19:30<30:09,  3.00s/it] 71%|███████   | 1462/2065 [1:19:33<30:06,  3.00s/it] 71%|███████   | 1463/2065 [1:19:36<30:03,  3.00s/it] 71%|███████   | 1464/2065 [1:19:39<30:00,  3.00s/it] 71%|███████   | 1465/2065 [1:19:42<29:57,  3.00s/it] 71%|███████   | 1466/2065 [1:19:45<30:03,  3.01s/it] 71%|███████   | 1467/2065 [1:19:48<29:55,  3.00s/it] 71%|███████   | 1468/2065 [1:19:51<29:51,  3.00s/it] 71%|███████   | 1469/2065 [1:19:54<29:47,  3.00s/it] 71%|███████   | 1470/2065 [1:19:57<29:43,  3.00s/it]                                                     {'loss': 2.2538, 'grad_norm': 0.32554924488067627, 'learning_rate': 3.918989509799285e-06, 'epoch': 3.56}
 71%|███████   | 1470/2065 [1:19:57<29:43,  3.00s/it] 71%|███████   | 1471/2065 [1:20:00<29:41,  3.00s/it] 71%|███████▏  | 1472/2065 [1:20:03<29:36,  3.00s/it] 71%|███████▏  | 1473/2065 [1:20:06<29:33,  3.00s/it] 71%|███████▏  | 1474/2065 [1:20:09<29:30,  3.00s/it] 71%|███████▏  | 1475/2065 [1:20:12<29:27,  3.00s/it] 71%|███████▏  | 1476/2065 [1:20:15<29:24,  3.00s/it] 72%|███████▏  | 1477/2065 [1:20:18<29:21,  3.00s/it] 72%|███████▏  | 1478/2065 [1:20:21<29:18,  3.00s/it] 72%|███████▏  | 1479/2065 [1:20:24<29:16,  3.00s/it] 72%|███████▏  | 1480/2065 [1:20:27<29:15,  3.00s/it]                                                     {'loss': 2.2724, 'grad_norm': 0.3289753496646881, 'learning_rate': 3.7977566660708387e-06, 'epoch': 3.58}
 72%|███████▏  | 1480/2065 [1:20:27<29:15,  3.00s/it] 72%|███████▏  | 1481/2065 [1:20:30<29:13,  3.00s/it] 72%|███████▏  | 1482/2065 [1:20:33<29:10,  3.00s/it] 72%|███████▏  | 1483/2065 [1:20:36<29:05,  3.00s/it] 72%|███████▏  | 1484/2065 [1:20:39<29:03,  3.00s/it] 72%|███████▏  | 1485/2065 [1:20:42<28:59,  3.00s/it] 72%|███████▏  | 1486/2065 [1:20:45<28:54,  3.00s/it] 72%|███████▏  | 1487/2065 [1:20:48<28:52,  3.00s/it] 72%|███████▏  | 1488/2065 [1:20:51<28:48,  3.00s/it] 72%|███████▏  | 1489/2065 [1:20:54<28:45,  3.00s/it] 72%|███████▏  | 1490/2065 [1:20:57<28:43,  3.00s/it]                                                     {'loss': 2.3068, 'grad_norm': 0.33023759722709656, 'learning_rate': 3.6779875267497644e-06, 'epoch': 3.61}
 72%|███████▏  | 1490/2065 [1:20:57<28:43,  3.00s/it] 72%|███████▏  | 1491/2065 [1:21:00<28:40,  3.00s/it] 72%|███████▏  | 1492/2065 [1:21:03<28:37,  3.00s/it] 72%|███████▏  | 1493/2065 [1:21:06<28:34,  3.00s/it] 72%|███████▏  | 1494/2065 [1:21:09<28:30,  2.99s/it] 72%|███████▏  | 1495/2065 [1:21:12<28:27,  3.00s/it] 72%|███████▏  | 1496/2065 [1:21:15<28:24,  3.00s/it] 72%|███████▏  | 1497/2065 [1:21:18<28:21,  3.00s/it] 73%|███████▎  | 1498/2065 [1:21:21<28:18,  3.00s/it] 73%|███████▎  | 1499/2065 [1:21:24<28:16,  3.00s/it] 73%|███████▎  | 1500/2065 [1:21:27<28:12,  3.00s/it]                                                     {'loss': 2.2664, 'grad_norm': 0.3344685137271881, 'learning_rate': 3.5597103568698023e-06, 'epoch': 3.63}
 73%|███████▎  | 1500/2065 [1:21:27<28:12,  3.00s/it][INFO|trainer.py:3512] 2024-06-24 23:46:00,014 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 23:46:00,014 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 23:46:00,014 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.35it/s][A
 27%|██▋       | 3/11 [00:02<00:08,  1.06s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.23s/it][A
 45%|████▌     | 5/11 [00:06<00:07,  1.33s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.39s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.43s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.45s/it][A
 82%|████████▏ | 9/11 [00:12<00:02,  1.46s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.47s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.48s/it][A                                                     
                                               [A{'eval_loss': 2.2630624771118164, 'eval_runtime': 16.5137, 'eval_samples_per_second': 15.805, 'eval_steps_per_second': 0.666, 'epoch': 3.63}
 73%|███████▎  | 1500/2065 [1:21:43<28:12,  3.00s/it]
100%|██████████| 11/11 [00:15<00:00,  1.48s/it][A
                                               [A[INFO|trainer.py:3203] 2024-06-24 23:46:16,621 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1500
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63caa68250>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 017aa86c-11bd-430e-bb54-770162dda7b4)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-24 23:46:26,680 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-24 23:46:26,682 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1500/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 73%|███████▎  | 1501/2065 [1:21:56<1:43:10, 10.98s/it] 73%|███████▎  | 1502/2065 [1:21:59<1:19:50,  8.51s/it] 73%|███████▎  | 1503/2065 [1:22:02<1:03:44,  6.81s/it] 73%|███████▎  | 1504/2065 [1:22:05<52:32,  5.62s/it]   73%|███████▎  | 1505/2065 [1:22:08<44:48,  4.80s/it] 73%|███████▎  | 1506/2065 [1:22:11<39:26,  4.23s/it] 73%|███████▎  | 1507/2065 [1:22:14<35:42,  3.84s/it] 73%|███████▎  | 1508/2065 [1:22:17<33:07,  3.57s/it] 73%|███████▎  | 1509/2065 [1:22:20<31:20,  3.38s/it] 73%|███████▎  | 1510/2065 [1:22:22<30:05,  3.25s/it]                                                     {'loss': 2.2956, 'grad_norm': 0.31506702303886414, 'learning_rate': 3.442953069365915e-06, 'epoch': 3.66}
 73%|███████▎  | 1510/2065 [1:22:22<30:05,  3.25s/it] 73%|███████▎  | 1511/2065 [1:22:25<29:14,  3.17s/it] 73%|███████▎  | 1512/2065 [1:22:28<28:48,  3.13s/it] 73%|███████▎  | 1513/2065 [1:22:31<28:21,  3.08s/it] 73%|███████▎  | 1514/2065 [1:22:34<28:00,  3.05s/it] 73%|███████▎  | 1515/2065 [1:22:37<27:44,  3.03s/it] 73%|███████▎  | 1516/2065 [1:22:40<27:33,  3.01s/it] 73%|███████▎  | 1517/2065 [1:22:43<27:24,  3.00s/it] 74%|███████▎  | 1518/2065 [1:22:46<27:17,  2.99s/it] 74%|███████▎  | 1519/2065 [1:22:49<27:12,  2.99s/it] 74%|███████▎  | 1520/2065 [1:22:52<27:07,  2.99s/it]                                                     {'loss': 2.2844, 'grad_norm': 0.2852482497692108, 'learning_rate': 3.3277432184869683e-06, 'epoch': 3.68}
 74%|███████▎  | 1520/2065 [1:22:52<27:07,  2.99s/it] 74%|███████▎  | 1521/2065 [1:22:55<27:04,  2.99s/it] 74%|███████▎  | 1522/2065 [1:22:58<27:00,  2.98s/it] 74%|███████▍  | 1523/2065 [1:23:01<26:56,  2.98s/it] 74%|███████▍  | 1524/2065 [1:23:04<26:53,  2.98s/it] 74%|███████▍  | 1525/2065 [1:23:07<26:50,  2.98s/it] 74%|███████▍  | 1526/2065 [1:23:10<26:47,  2.98s/it] 74%|███████▍  | 1527/2065 [1:23:13<26:44,  2.98s/it] 74%|███████▍  | 1528/2065 [1:23:16<26:42,  2.98s/it] 74%|███████▍  | 1529/2065 [1:23:19<26:38,  2.98s/it] 74%|███████▍  | 1530/2065 [1:23:22<26:36,  2.98s/it]                                                     {'loss': 2.2495, 'grad_norm': 0.3461707830429077, 'learning_rate': 3.2141079932930187e-06, 'epoch': 3.7}
 74%|███████▍  | 1530/2065 [1:23:22<26:36,  2.98s/it] 74%|███████▍  | 1531/2065 [1:23:25<26:33,  2.98s/it] 74%|███████▍  | 1532/2065 [1:23:28<26:30,  2.98s/it] 74%|███████▍  | 1533/2065 [1:23:31<26:28,  2.99s/it] 74%|███████▍  | 1534/2065 [1:23:34<26:25,  2.99s/it] 74%|███████▍  | 1535/2065 [1:23:37<26:22,  2.99s/it] 74%|███████▍  | 1536/2065 [1:23:40<26:30,  3.01s/it] 74%|███████▍  | 1537/2065 [1:23:43<26:24,  3.00s/it] 74%|███████▍  | 1538/2065 [1:23:46<26:25,  3.01s/it] 75%|███████▍  | 1539/2065 [1:23:49<26:19,  3.00s/it] 75%|███████▍  | 1540/2065 [1:23:52<26:14,  3.00s/it]                                                     {'loss': 2.2667, 'grad_norm': 0.33524224162101746, 'learning_rate': 3.102074211238816e-06, 'epoch': 3.73}
 75%|███████▍  | 1540/2065 [1:23:52<26:14,  3.00s/it] 75%|███████▍  | 1541/2065 [1:23:55<26:10,  3.00s/it] 75%|███████▍  | 1542/2065 [1:23:58<26:04,  2.99s/it] 75%|███████▍  | 1543/2065 [1:24:01<26:01,  2.99s/it] 75%|███████▍  | 1544/2065 [1:24:04<25:58,  2.99s/it] 75%|███████▍  | 1545/2065 [1:24:07<25:54,  2.99s/it] 75%|███████▍  | 1546/2065 [1:24:10<25:52,  2.99s/it] 75%|███████▍  | 1547/2065 [1:24:13<25:48,  2.99s/it] 75%|███████▍  | 1548/2065 [1:24:16<25:46,  2.99s/it] 75%|███████▌  | 1549/2065 [1:24:19<25:51,  3.01s/it] 75%|███████▌  | 1550/2065 [1:24:22<25:44,  3.00s/it]                                                     {'loss': 2.2899, 'grad_norm': 0.3346559405326843, 'learning_rate': 2.9916683118449908e-06, 'epoch': 3.75}
 75%|███████▌  | 1550/2065 [1:24:22<25:44,  3.00s/it] 75%|███████▌  | 1551/2065 [1:24:25<25:40,  3.00s/it] 75%|███████▌  | 1552/2065 [1:24:28<25:37,  3.00s/it] 75%|███████▌  | 1553/2065 [1:24:31<25:33,  2.99s/it] 75%|███████▌  | 1554/2065 [1:24:34<25:29,  2.99s/it] 75%|███████▌  | 1555/2065 [1:24:37<25:25,  2.99s/it] 75%|███████▌  | 1556/2065 [1:24:40<25:22,  2.99s/it] 75%|███████▌  | 1557/2065 [1:24:43<25:19,  2.99s/it] 75%|███████▌  | 1558/2065 [1:24:46<25:16,  2.99s/it] 75%|███████▌  | 1559/2065 [1:24:49<25:13,  2.99s/it] 76%|███████▌  | 1560/2065 [1:24:52<25:10,  2.99s/it]                                                     {'loss': 2.3154, 'grad_norm': 0.30110418796539307, 'learning_rate': 2.8829163504584233e-06, 'epoch': 3.78}
 76%|███████▌  | 1560/2065 [1:24:52<25:10,  2.99s/it] 76%|███████▌  | 1561/2065 [1:24:55<25:07,  2.99s/it] 76%|███████▌  | 1562/2065 [1:24:58<25:04,  2.99s/it] 76%|███████▌  | 1563/2065 [1:25:01<25:01,  2.99s/it] 76%|███████▌  | 1564/2065 [1:25:04<24:58,  2.99s/it] 76%|███████▌  | 1565/2065 [1:25:07<24:54,  2.99s/it] 76%|███████▌  | 1566/2065 [1:25:10<24:52,  2.99s/it] 76%|███████▌  | 1567/2065 [1:25:13<24:49,  2.99s/it] 76%|███████▌  | 1568/2065 [1:25:16<24:46,  2.99s/it] 76%|███████▌  | 1569/2065 [1:25:19<24:43,  2.99s/it] 76%|███████▌  | 1570/2065 [1:25:22<24:40,  2.99s/it]                                                     {'loss': 2.3162, 'grad_norm': 0.314996600151062, 'learning_rate': 2.7758439921033063e-06, 'epoch': 3.8}
 76%|███████▌  | 1570/2065 [1:25:22<24:40,  2.99s/it] 76%|███████▌  | 1571/2065 [1:25:25<24:37,  2.99s/it] 76%|███████▌  | 1572/2065 [1:25:28<24:35,  2.99s/it] 76%|███████▌  | 1573/2065 [1:25:31<24:31,  2.99s/it] 76%|███████▌  | 1574/2065 [1:25:34<24:29,  2.99s/it] 76%|███████▋  | 1575/2065 [1:25:37<24:25,  2.99s/it] 76%|███████▋  | 1576/2065 [1:25:40<24:23,  2.99s/it] 76%|███████▋  | 1577/2065 [1:25:43<24:20,  2.99s/it] 76%|███████▋  | 1578/2065 [1:25:46<24:16,  2.99s/it] 76%|███████▋  | 1579/2065 [1:25:49<24:13,  2.99s/it] 77%|███████▋  | 1580/2065 [1:25:52<24:14,  3.00s/it]                                                     {'loss': 2.2527, 'grad_norm': 0.2879062592983246, 'learning_rate': 2.670476505424289e-06, 'epoch': 3.83}
 77%|███████▋  | 1580/2065 [1:25:52<24:14,  3.00s/it] 77%|███████▋  | 1581/2065 [1:25:55<24:09,  3.00s/it] 77%|███████▋  | 1582/2065 [1:25:58<24:06,  2.99s/it] 77%|███████▋  | 1583/2065 [1:26:01<24:03,  2.99s/it] 77%|███████▋  | 1584/2065 [1:26:04<24:00,  2.99s/it] 77%|███████▋  | 1585/2065 [1:26:07<23:56,  2.99s/it] 77%|███████▋  | 1586/2065 [1:26:10<23:52,  2.99s/it] 77%|███████▋  | 1587/2065 [1:26:13<23:49,  2.99s/it] 77%|███████▋  | 1588/2065 [1:26:16<23:46,  2.99s/it] 77%|███████▋  | 1589/2065 [1:26:19<23:44,  2.99s/it] 77%|███████▋  | 1590/2065 [1:26:22<23:41,  2.99s/it]                                                     {'loss': 2.2935, 'grad_norm': 0.31117579340934753, 'learning_rate': 2.566838756723179e-06, 'epoch': 3.85}
 77%|███████▋  | 1590/2065 [1:26:22<23:41,  2.99s/it] 77%|███████▋  | 1591/2065 [1:26:25<23:40,  3.00s/it] 77%|███████▋  | 1592/2065 [1:26:28<23:42,  3.01s/it] 77%|███████▋  | 1593/2065 [1:26:31<23:39,  3.01s/it] 77%|███████▋  | 1594/2065 [1:26:34<23:37,  3.01s/it] 77%|███████▋  | 1595/2065 [1:26:37<23:38,  3.02s/it] 77%|███████▋  | 1596/2065 [1:26:40<23:32,  3.01s/it] 77%|███████▋  | 1597/2065 [1:26:43<23:29,  3.01s/it] 77%|███████▋  | 1598/2065 [1:26:46<23:26,  3.01s/it] 77%|███████▋  | 1599/2065 [1:26:49<23:26,  3.02s/it] 77%|███████▋  | 1600/2065 [1:26:52<23:28,  3.03s/it]                                                     {'loss': 2.24, 'grad_norm': 0.32445618510246277, 'learning_rate': 2.46495520409061e-06, 'epoch': 3.87}
 77%|███████▋  | 1600/2065 [1:26:52<23:28,  3.03s/it][INFO|trainer.py:3512] 2024-06-24 23:51:25,039 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 23:51:25,039 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 23:51:25,039 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.30it/s][A
 27%|██▋       | 3/11 [00:03<00:08,  1.10s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.24s/it][A
 45%|████▌     | 5/11 [00:06<00:08,  1.34s/it][A
 55%|█████▍    | 6/11 [00:07<00:07,  1.42s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.44s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.45s/it][A
 82%|████████▏ | 9/11 [00:12<00:02,  1.47s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.48s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A                                                     
                                               [A{'eval_loss': 2.2624075412750244, 'eval_runtime': 16.6796, 'eval_samples_per_second': 15.648, 'eval_steps_per_second': 0.659, 'epoch': 3.87}
 77%|███████▋  | 1600/2065 [1:27:09<23:28,  3.03s/it]
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A
                                               [A 78%|███████▊  | 1601/2065 [1:27:12<1:02:03,  8.02s/it] 78%|███████▊  | 1602/2065 [1:27:15<50:25,  6.53s/it]   78%|███████▊  | 1603/2065 [1:27:18<42:15,  5.49s/it] 78%|███████▊  | 1604/2065 [1:27:21<36:29,  4.75s/it] 78%|███████▊  | 1605/2065 [1:27:24<32:29,  4.24s/it] 78%|███████▊  | 1606/2065 [1:27:27<29:39,  3.88s/it] 78%|███████▊  | 1607/2065 [1:27:30<27:49,  3.64s/it] 78%|███████▊  | 1608/2065 [1:27:33<26:17,  3.45s/it] 78%|███████▊  | 1609/2065 [1:27:36<25:34,  3.36s/it] 78%|███████▊  | 1610/2065 [1:27:39<24:47,  3.27s/it]                                                     {'loss': 2.2979, 'grad_norm': 0.289948970079422, 'learning_rate': 2.3648498916340046e-06, 'epoch': 3.9}
 78%|███████▊  | 1610/2065 [1:27:39<24:47,  3.27s/it] 78%|███████▊  | 1611/2065 [1:27:42<24:16,  3.21s/it] 78%|███████▊  | 1612/2065 [1:27:45<23:54,  3.17s/it] 78%|███████▊  | 1613/2065 [1:27:48<23:31,  3.12s/it] 78%|███████▊  | 1614/2065 [1:27:51<23:15,  3.09s/it] 78%|███████▊  | 1615/2065 [1:27:54<23:07,  3.08s/it] 78%|███████▊  | 1616/2065 [1:27:58<23:13,  3.10s/it] 78%|███████▊  | 1617/2065 [1:28:01<23:09,  3.10s/it] 78%|███████▊  | 1618/2065 [1:28:04<23:03,  3.10s/it] 78%|███████▊  | 1619/2065 [1:28:07<22:54,  3.08s/it] 78%|███████▊  | 1620/2065 [1:28:10<22:44,  3.07s/it]                                                     {'loss': 2.2844, 'grad_norm': 0.3360491693019867, 'learning_rate': 2.266546443803287e-06, 'epoch': 3.92}
 78%|███████▊  | 1620/2065 [1:28:10<22:44,  3.07s/it] 78%|███████▊  | 1621/2065 [1:28:13<22:38,  3.06s/it] 79%|███████▊  | 1622/2065 [1:28:16<22:35,  3.06s/it] 79%|███████▊  | 1623/2065 [1:28:19<22:35,  3.07s/it] 79%|███████▊  | 1624/2065 [1:28:22<22:37,  3.08s/it] 79%|███████▊  | 1625/2065 [1:28:25<22:27,  3.06s/it] 79%|███████▊  | 1626/2065 [1:28:28<22:22,  3.06s/it] 79%|███████▉  | 1627/2065 [1:28:31<22:18,  3.06s/it] 79%|███████▉  | 1628/2065 [1:28:34<22:17,  3.06s/it] 79%|███████▉  | 1629/2065 [1:28:37<22:14,  3.06s/it] 79%|███████▉  | 1630/2065 [1:28:40<22:07,  3.05s/it]                                                     {'loss': 2.2455, 'grad_norm': 0.3111622631549835, 'learning_rate': 2.1700680598155945e-06, 'epoch': 3.95}
 79%|███████▉  | 1630/2065 [1:28:40<22:07,  3.05s/it] 79%|███████▉  | 1631/2065 [1:28:43<22:06,  3.06s/it] 79%|███████▉  | 1632/2065 [1:28:47<22:10,  3.07s/it] 79%|███████▉  | 1633/2065 [1:28:50<22:04,  3.07s/it] 79%|███████▉  | 1634/2065 [1:28:53<22:28,  3.13s/it] 79%|███████▉  | 1635/2065 [1:28:56<22:23,  3.12s/it] 79%|███████▉  | 1636/2065 [1:28:59<22:05,  3.09s/it] 79%|███████▉  | 1637/2065 [1:29:02<22:08,  3.10s/it] 79%|███████▉  | 1638/2065 [1:29:05<22:17,  3.13s/it] 79%|███████▉  | 1639/2065 [1:29:08<22:08,  3.12s/it] 79%|███████▉  | 1640/2065 [1:29:11<21:54,  3.09s/it]                                                     {'loss': 2.3062, 'grad_norm': 0.35849401354789734, 'learning_rate': 2.0754375081803623e-06, 'epoch': 3.97}
 79%|███████▉  | 1640/2065 [1:29:11<21:54,  3.09s/it] 79%|███████▉  | 1641/2065 [1:29:14<21:45,  3.08s/it] 80%|███████▉  | 1642/2065 [1:29:18<21:37,  3.07s/it] 80%|███████▉  | 1643/2065 [1:29:21<21:41,  3.08s/it] 80%|███████▉  | 1644/2065 [1:29:24<21:35,  3.08s/it] 80%|███████▉  | 1645/2065 [1:29:27<21:25,  3.06s/it] 80%|███████▉  | 1646/2065 [1:29:30<21:26,  3.07s/it] 80%|███████▉  | 1647/2065 [1:29:33<21:28,  3.08s/it] 80%|███████▉  | 1648/2065 [1:29:36<21:21,  3.07s/it] 80%|███████▉  | 1649/2065 [1:29:39<21:20,  3.08s/it] 80%|███████▉  | 1650/2065 [1:29:42<21:06,  3.05s/it]                                                     {'loss': 2.232, 'grad_norm': 0.31454646587371826, 'learning_rate': 1.982677121326044e-06, 'epoch': 4.0}
 80%|███████▉  | 1650/2065 [1:29:42<21:06,  3.05s/it][INFO|trainer.py:3203] 2024-06-24 23:54:15,180 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1650
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b62c5842c90>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 5c4d6341-ed5c-4502-8dac-14df1767761d)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-24 23:54:25,237 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1650/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-24 23:54:25,239 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1650/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 80%|███████▉  | 1651/2065 [1:29:55<41:34,  6.03s/it] 80%|████████  | 1652/2065 [1:29:58<34:45,  5.05s/it] 80%|████████  | 1653/2065 [1:30:01<30:27,  4.44s/it] 80%|████████  | 1654/2065 [1:30:04<27:13,  3.97s/it] 80%|████████  | 1655/2065 [1:30:07<25:00,  3.66s/it] 80%|████████  | 1656/2065 [1:30:10<23:27,  3.44s/it] 80%|████████  | 1657/2065 [1:30:13<22:23,  3.29s/it] 80%|████████  | 1658/2065 [1:30:16<21:47,  3.21s/it] 80%|████████  | 1659/2065 [1:30:19<21:29,  3.18s/it] 80%|████████  | 1660/2065 [1:30:22<21:04,  3.12s/it]                                                     {'loss': 2.2682, 'grad_norm': 0.3207022547721863, 'learning_rate': 1.8918087903297444e-06, 'epoch': 4.02}
 80%|████████  | 1660/2065 [1:30:22<21:04,  3.12s/it] 80%|████████  | 1661/2065 [1:30:25<20:53,  3.10s/it] 80%|████████  | 1662/2065 [1:30:28<20:43,  3.09s/it] 81%|████████  | 1663/2065 [1:30:31<20:28,  3.06s/it] 81%|████████  | 1664/2065 [1:30:34<20:16,  3.03s/it] 81%|████████  | 1665/2065 [1:30:37<20:17,  3.04s/it] 81%|████████  | 1666/2065 [1:30:40<20:14,  3.04s/it] 81%|████████  | 1667/2065 [1:30:43<20:04,  3.03s/it] 81%|████████  | 1668/2065 [1:30:46<20:02,  3.03s/it] 81%|████████  | 1669/2065 [1:30:49<19:57,  3.02s/it] 81%|████████  | 1670/2065 [1:30:52<19:55,  3.03s/it]                                                     {'loss': 2.2917, 'grad_norm': 0.34615322947502136, 'learning_rate': 1.8028539597510231e-06, 'epoch': 4.04}
 81%|████████  | 1670/2065 [1:30:52<19:55,  3.03s/it] 81%|████████  | 1671/2065 [1:30:55<19:51,  3.03s/it] 81%|████████  | 1672/2065 [1:30:58<19:46,  3.02s/it] 81%|████████  | 1673/2065 [1:31:01<19:46,  3.03s/it] 81%|████████  | 1674/2065 [1:31:04<19:43,  3.03s/it] 81%|████████  | 1675/2065 [1:31:07<19:36,  3.02s/it] 81%|████████  | 1676/2065 [1:31:10<19:44,  3.05s/it] 81%|████████  | 1677/2065 [1:31:13<19:47,  3.06s/it] 81%|████████▏ | 1678/2065 [1:31:16<19:48,  3.07s/it] 81%|████████▏ | 1679/2065 [1:31:19<19:36,  3.05s/it] 81%|████████▏ | 1680/2065 [1:31:22<19:29,  3.04s/it]                                                     {'loss': 2.2389, 'grad_norm': 0.328309565782547, 'learning_rate': 1.7158336225710592e-06, 'epoch': 4.07}
 81%|████████▏ | 1680/2065 [1:31:22<19:29,  3.04s/it] 81%|████████▏ | 1681/2065 [1:31:25<19:24,  3.03s/it] 81%|████████▏ | 1682/2065 [1:31:28<19:21,  3.03s/it] 82%|████████▏ | 1683/2065 [1:31:31<19:21,  3.04s/it] 82%|████████▏ | 1684/2065 [1:31:34<19:17,  3.04s/it] 82%|████████▏ | 1685/2065 [1:31:37<19:13,  3.04s/it] 82%|████████▏ | 1686/2065 [1:31:41<19:20,  3.06s/it] 82%|████████▏ | 1687/2065 [1:31:44<19:13,  3.05s/it] 82%|████████▏ | 1688/2065 [1:31:47<19:05,  3.04s/it] 82%|████████▏ | 1689/2065 [1:31:50<19:00,  3.03s/it] 82%|████████▏ | 1690/2065 [1:31:53<18:56,  3.03s/it]                                                     {'loss': 2.3244, 'grad_norm': 0.3070127069950104, 'learning_rate': 1.6307683152383935e-06, 'epoch': 4.09}
 82%|████████▏ | 1690/2065 [1:31:53<18:56,  3.03s/it] 82%|████████▏ | 1691/2065 [1:31:56<19:00,  3.05s/it] 82%|████████▏ | 1692/2065 [1:31:59<18:54,  3.04s/it] 82%|████████▏ | 1693/2065 [1:32:02<18:52,  3.04s/it] 82%|████████▏ | 1694/2065 [1:32:05<18:46,  3.04s/it] 82%|████████▏ | 1695/2065 [1:32:08<18:41,  3.03s/it] 82%|████████▏ | 1696/2065 [1:32:11<18:34,  3.02s/it] 82%|████████▏ | 1697/2065 [1:32:14<18:35,  3.03s/it] 82%|████████▏ | 1698/2065 [1:32:17<18:34,  3.04s/it] 82%|████████▏ | 1699/2065 [1:32:20<18:28,  3.03s/it] 82%|████████▏ | 1700/2065 [1:32:23<18:27,  3.03s/it]                                                     {'loss': 2.3078, 'grad_norm': 0.32781463861465454, 'learning_rate': 1.5476781128224116e-06, 'epoch': 4.12}
 82%|████████▏ | 1700/2065 [1:32:23<18:27,  3.03s/it][INFO|trainer.py:3512] 2024-06-24 23:56:56,117 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-24 23:56:56,117 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-24 23:56:56,117 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.31it/s][A
 27%|██▋       | 3/11 [00:02<00:08,  1.05s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.27s/it][A
 45%|████▌     | 5/11 [00:06<00:08,  1.34s/it][A
 55%|█████▍    | 6/11 [00:07<00:07,  1.43s/it][A
 64%|██████▎   | 7/11 [00:09<00:05,  1.46s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.53s/it][A
 82%|████████▏ | 9/11 [00:12<00:02,  1.48s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.49s/it][A
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A                                                     
                                               [A{'eval_loss': 2.2620112895965576, 'eval_runtime': 16.8961, 'eval_samples_per_second': 15.447, 'eval_steps_per_second': 0.651, 'epoch': 4.12}
 82%|████████▏ | 1700/2065 [1:32:40<18:27,  3.03s/it]
100%|██████████| 11/11 [00:15<00:00,  1.49s/it][A
                                               [A 82%|████████▏ | 1701/2065 [1:32:43<49:10,  8.11s/it] 82%|████████▏ | 1702/2065 [1:32:46<40:01,  6.62s/it] 82%|████████▏ | 1703/2065 [1:32:49<33:24,  5.54s/it] 83%|████████▎ | 1704/2065 [1:32:52<28:46,  4.78s/it] 83%|████████▎ | 1705/2065 [1:32:55<25:32,  4.26s/it] 83%|████████▎ | 1706/2065 [1:32:58<23:13,  3.88s/it] 83%|████████▎ | 1707/2065 [1:33:01<21:42,  3.64s/it] 83%|████████▎ | 1708/2065 [1:33:04<20:33,  3.46s/it] 83%|████████▎ | 1709/2065 [1:33:07<19:43,  3.32s/it] 83%|████████▎ | 1710/2065 [1:33:10<19:09,  3.24s/it]                                                     {'loss': 2.2572, 'grad_norm': 0.3566797077655792, 'learning_rate': 1.4665826242756964e-06, 'epoch': 4.14}
 83%|████████▎ | 1710/2065 [1:33:10<19:09,  3.24s/it] 83%|████████▎ | 1711/2065 [1:33:13<18:43,  3.17s/it] 83%|████████▎ | 1712/2065 [1:33:16<18:28,  3.14s/it] 83%|████████▎ | 1713/2065 [1:33:19<18:10,  3.10s/it] 83%|████████▎ | 1714/2065 [1:33:22<17:56,  3.07s/it] 83%|████████▎ | 1715/2065 [1:33:25<17:46,  3.05s/it] 83%|████████▎ | 1716/2065 [1:33:28<17:40,  3.04s/it] 83%|████████▎ | 1717/2065 [1:33:31<17:34,  3.03s/it] 83%|████████▎ | 1718/2065 [1:33:34<17:31,  3.03s/it] 83%|████████▎ | 1719/2065 [1:33:37<17:29,  3.03s/it] 83%|████████▎ | 1720/2065 [1:33:41<17:23,  3.02s/it]                                                     {'loss': 2.2958, 'grad_norm': 0.3197076618671417, 'learning_rate': 1.387500987806415e-06, 'epoch': 4.16}
 83%|████████▎ | 1720/2065 [1:33:41<17:23,  3.02s/it] 83%|████████▎ | 1721/2065 [1:33:44<17:20,  3.03s/it] 83%|████████▎ | 1722/2065 [1:33:47<17:24,  3.05s/it] 83%|████████▎ | 1723/2065 [1:33:50<17:18,  3.04s/it] 83%|████████▎ | 1724/2065 [1:33:53<17:12,  3.03s/it] 84%|████████▎ | 1725/2065 [1:33:56<17:10,  3.03s/it] 84%|████████▎ | 1726/2065 [1:33:59<17:03,  3.02s/it] 84%|████████▎ | 1727/2065 [1:34:02<16:59,  3.02s/it] 84%|████████▎ | 1728/2065 [1:34:05<17:01,  3.03s/it] 84%|████████▎ | 1729/2065 [1:34:08<16:59,  3.03s/it] 84%|████████▍ | 1730/2065 [1:34:11<16:52,  3.02s/it]                                                     {'loss': 2.2652, 'grad_norm': 0.3064662218093872, 'learning_rate': 1.3104518663617649e-06, 'epoch': 4.19}
 84%|████████▍ | 1730/2065 [1:34:11<16:52,  3.02s/it] 84%|████████▍ | 1731/2065 [1:34:14<16:51,  3.03s/it] 84%|████████▍ | 1732/2065 [1:34:17<16:46,  3.02s/it] 84%|████████▍ | 1733/2065 [1:34:20<16:46,  3.03s/it] 84%|████████▍ | 1734/2065 [1:34:23<16:43,  3.03s/it] 84%|████████▍ | 1735/2065 [1:34:26<16:39,  3.03s/it] 84%|████████▍ | 1736/2065 [1:34:29<16:33,  3.02s/it] 84%|████████▍ | 1737/2065 [1:34:32<16:34,  3.03s/it] 84%|████████▍ | 1738/2065 [1:34:35<16:34,  3.04s/it] 84%|████████▍ | 1739/2065 [1:34:38<16:28,  3.03s/it] 84%|████████▍ | 1740/2065 [1:34:41<16:43,  3.09s/it]                                                     {'loss': 2.2496, 'grad_norm': 0.318523108959198, 'learning_rate': 1.2354534432235943e-06, 'epoch': 4.21}
 84%|████████▍ | 1740/2065 [1:34:41<16:43,  3.09s/it] 84%|████████▍ | 1741/2065 [1:34:44<16:36,  3.07s/it] 84%|████████▍ | 1742/2065 [1:34:47<16:30,  3.07s/it] 84%|████████▍ | 1743/2065 [1:34:50<16:24,  3.06s/it] 84%|████████▍ | 1744/2065 [1:34:53<16:16,  3.04s/it] 85%|████████▍ | 1745/2065 [1:34:56<16:10,  3.03s/it] 85%|████████▍ | 1746/2065 [1:34:59<16:05,  3.03s/it] 85%|████████▍ | 1747/2065 [1:35:02<16:03,  3.03s/it] 85%|████████▍ | 1748/2065 [1:35:05<15:58,  3.02s/it] 85%|████████▍ | 1749/2065 [1:35:09<15:58,  3.03s/it] 85%|████████▍ | 1750/2065 [1:35:12<15:55,  3.03s/it]                                                     {'loss': 2.2434, 'grad_norm': 0.34505751729011536, 'learning_rate': 1.1625234177172252e-06, 'epoch': 4.24}
 85%|████████▍ | 1750/2065 [1:35:12<15:55,  3.03s/it] 85%|████████▍ | 1751/2065 [1:35:15<15:52,  3.03s/it] 85%|████████▍ | 1752/2065 [1:35:18<15:51,  3.04s/it] 85%|████████▍ | 1753/2065 [1:35:21<15:47,  3.04s/it] 85%|████████▍ | 1754/2065 [1:35:24<15:43,  3.03s/it] 85%|████████▍ | 1755/2065 [1:35:27<15:39,  3.03s/it] 85%|████████▌ | 1756/2065 [1:35:30<15:34,  3.02s/it] 85%|████████▌ | 1757/2065 [1:35:33<15:30,  3.02s/it] 85%|████████▌ | 1758/2065 [1:35:36<15:26,  3.02s/it] 85%|████████▌ | 1759/2065 [1:35:39<15:25,  3.02s/it] 85%|████████▌ | 1760/2065 [1:35:42<15:37,  3.07s/it]                                                     {'loss': 2.2044, 'grad_norm': 0.31470179557800293, 'learning_rate': 1.0916790010344836e-06, 'epoch': 4.26}
 85%|████████▌ | 1760/2065 [1:35:42<15:37,  3.07s/it] 85%|████████▌ | 1761/2065 [1:35:47<18:19,  3.62s/it] 85%|████████▌ | 1762/2065 [1:35:50<17:22,  3.44s/it] 85%|████████▌ | 1763/2065 [1:35:53<16:39,  3.31s/it] 85%|████████▌ | 1764/2065 [1:35:56<16:11,  3.23s/it] 85%|████████▌ | 1765/2065 [1:35:59<15:49,  3.16s/it] 86%|████████▌ | 1766/2065 [1:36:02<15:36,  3.13s/it] 86%|████████▌ | 1767/2065 [1:36:05<15:23,  3.10s/it] 86%|████████▌ | 1768/2065 [1:36:08<15:12,  3.07s/it] 86%|████████▌ | 1769/2065 [1:36:11<15:02,  3.05s/it] 86%|████████▌ | 1770/2065 [1:36:14<14:55,  3.03s/it]                                                     {'loss': 2.3042, 'grad_norm': 0.30204957723617554, 'learning_rate': 1.0229369121719247e-06, 'epoch': 4.29}
 86%|████████▌ | 1770/2065 [1:36:14<14:55,  3.03s/it] 86%|████████▌ | 1771/2065 [1:36:17<14:51,  3.03s/it] 86%|████████▌ | 1772/2065 [1:36:20<14:49,  3.03s/it] 86%|████████▌ | 1773/2065 [1:36:23<14:46,  3.04s/it] 86%|████████▌ | 1774/2065 [1:36:26<14:42,  3.03s/it] 86%|████████▌ | 1775/2065 [1:36:29<14:35,  3.02s/it] 86%|████████▌ | 1776/2065 [1:36:32<14:33,  3.02s/it] 86%|████████▌ | 1777/2065 [1:36:35<14:27,  3.01s/it] 86%|████████▌ | 1778/2065 [1:36:38<14:22,  3.01s/it] 86%|████████▌ | 1779/2065 [1:36:41<14:22,  3.02s/it] 86%|████████▌ | 1780/2065 [1:36:44<14:23,  3.03s/it]                                                     {'loss': 2.2518, 'grad_norm': 0.3013640344142914, 'learning_rate': 9.56313373985218e-07, 'epoch': 4.31}
 86%|████████▌ | 1780/2065 [1:36:44<14:23,  3.03s/it] 86%|████████▌ | 1781/2065 [1:36:47<14:17,  3.02s/it] 86%|████████▋ | 1782/2065 [1:36:50<14:13,  3.02s/it] 86%|████████▋ | 1783/2065 [1:36:53<14:10,  3.02s/it] 86%|████████▋ | 1784/2065 [1:36:56<14:06,  3.01s/it] 86%|████████▋ | 1785/2065 [1:36:59<14:03,  3.01s/it] 86%|████████▋ | 1786/2065 [1:37:02<14:00,  3.01s/it] 87%|████████▋ | 1787/2065 [1:37:05<14:03,  3.03s/it] 87%|████████▋ | 1788/2065 [1:37:09<14:08,  3.06s/it] 87%|████████▋ | 1789/2065 [1:37:12<14:03,  3.06s/it] 87%|████████▋ | 1790/2065 [1:37:15<13:56,  3.04s/it]                                                     {'loss': 2.3138, 'grad_norm': 0.33742013573646545, 'learning_rate': 8.918241093606217e-07, 'epoch': 4.33}
 87%|████████▋ | 1790/2065 [1:37:15<13:56,  3.04s/it] 87%|████████▋ | 1791/2065 [1:37:18<13:51,  3.03s/it] 87%|████████▋ | 1792/2065 [1:37:21<13:46,  3.03s/it] 87%|████████▋ | 1793/2065 [1:37:24<13:43,  3.03s/it] 87%|████████▋ | 1794/2065 [1:37:27<13:39,  3.03s/it] 87%|████████▋ | 1795/2065 [1:37:30<13:37,  3.03s/it] 87%|████████▋ | 1796/2065 [1:37:33<13:34,  3.03s/it] 87%|████████▋ | 1797/2065 [1:37:36<13:30,  3.02s/it] 87%|████████▋ | 1798/2065 [1:37:39<13:25,  3.02s/it] 87%|████████▋ | 1799/2065 [1:37:42<13:21,  3.01s/it] 87%|████████▋ | 1800/2065 [1:37:45<13:18,  3.01s/it]                                                     {'loss': 2.3216, 'grad_norm': 0.29002076387405396, 'learning_rate': 8.294843375044425e-07, 'epoch': 4.36}
 87%|████████▋ | 1800/2065 [1:37:45<13:18,  3.01s/it][INFO|trainer.py:3512] 2024-06-25 00:02:17,897 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-25 00:02:17,897 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-25 00:02:17,897 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.35it/s][A
 27%|██▋       | 3/11 [00:02<00:08,  1.05s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.22s/it][A
 45%|████▌     | 5/11 [00:05<00:07,  1.32s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.38s/it][A
 64%|██████▎   | 7/11 [00:08<00:05,  1.42s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.45s/it][A
 82%|████████▏ | 9/11 [00:11<00:02,  1.46s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.48s/it][A
100%|██████████| 11/11 [00:14<00:00,  1.48s/it][A                                                     
                                               [A{'eval_loss': 2.2617571353912354, 'eval_runtime': 16.5103, 'eval_samples_per_second': 15.808, 'eval_steps_per_second': 0.666, 'epoch': 4.36}
 87%|████████▋ | 1800/2065 [1:38:01<13:18,  3.01s/it]
100%|██████████| 11/11 [00:15<00:00,  1.48s/it][A
                                               [A[INFO|trainer.py:3203] 2024-06-25 00:02:34,411 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1800
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63cb5c2c90>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 247d7326-c7fa-4cdd-87ea-1046534ed8f2)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-25 00:02:44,471 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-25 00:02:44,473 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1800/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 87%|████████▋ | 1801/2065 [1:38:14<48:09, 10.94s/it] 87%|████████▋ | 1802/2065 [1:38:17<37:14,  8.49s/it] 87%|████████▋ | 1803/2065 [1:38:20<29:43,  6.81s/it] 87%|████████▋ | 1804/2065 [1:38:23<24:28,  5.63s/it] 87%|████████▋ | 1805/2065 [1:38:26<20:49,  4.81s/it] 87%|████████▋ | 1806/2065 [1:38:29<18:20,  4.25s/it] 88%|████████▊ | 1807/2065 [1:38:32<16:34,  3.86s/it] 88%|████████▊ | 1808/2065 [1:38:34<15:21,  3.59s/it] 88%|████████▊ | 1809/2065 [1:38:38<14:39,  3.43s/it] 88%|████████▊ | 1810/2065 [1:38:41<14:00,  3.29s/it]                                                     {'loss': 2.2672, 'grad_norm': 0.31945356726646423, 'learning_rate': 7.693087703513613e-07, 'epoch': 4.38}
 88%|████████▊ | 1810/2065 [1:38:41<14:00,  3.29s/it] 88%|████████▊ | 1811/2065 [1:38:44<13:32,  3.20s/it] 88%|████████▊ | 1812/2065 [1:38:47<13:13,  3.14s/it] 88%|████████▊ | 1813/2065 [1:38:50<13:00,  3.10s/it] 88%|████████▊ | 1814/2065 [1:38:52<12:48,  3.06s/it] 88%|████████▊ | 1815/2065 [1:38:56<12:41,  3.05s/it] 88%|████████▊ | 1816/2065 [1:38:58<12:34,  3.03s/it] 88%|████████▊ | 1817/2065 [1:39:02<12:29,  3.02s/it] 88%|████████▊ | 1818/2065 [1:39:04<12:22,  3.01s/it] 88%|████████▊ | 1819/2065 [1:39:07<12:18,  3.00s/it] 88%|████████▊ | 1820/2065 [1:39:10<12:15,  3.00s/it]                                                     {'loss': 2.2962, 'grad_norm': 0.31038644909858704, 'learning_rate': 7.113116090924843e-07, 'epoch': 4.41}
 88%|████████▊ | 1820/2065 [1:39:10<12:15,  3.00s/it] 88%|████████▊ | 1821/2065 [1:39:14<12:20,  3.04s/it] 88%|████████▊ | 1822/2065 [1:39:17<12:14,  3.02s/it] 88%|████████▊ | 1823/2065 [1:39:20<12:09,  3.01s/it] 88%|████████▊ | 1824/2065 [1:39:23<12:05,  3.01s/it] 88%|████████▊ | 1825/2065 [1:39:26<12:04,  3.02s/it] 88%|████████▊ | 1826/2065 [1:39:29<11:59,  3.01s/it] 88%|████████▊ | 1827/2065 [1:39:32<11:55,  3.01s/it] 89%|████████▊ | 1828/2065 [1:39:35<11:54,  3.02s/it] 89%|████████▊ | 1829/2065 [1:39:38<11:51,  3.01s/it] 89%|████████▊ | 1830/2065 [1:39:41<11:45,  3.00s/it]                                                     {'loss': 2.2623, 'grad_norm': 0.3183594346046448, 'learning_rate': 6.555065408239103e-07, 'epoch': 4.43}
 89%|████████▊ | 1830/2065 [1:39:41<11:45,  3.00s/it] 89%|████████▊ | 1831/2065 [1:39:44<11:42,  3.00s/it] 89%|████████▊ | 1832/2065 [1:39:47<11:41,  3.01s/it] 89%|████████▉ | 1833/2065 [1:39:50<11:37,  3.01s/it] 89%|████████▉ | 1834/2065 [1:39:53<11:33,  3.00s/it] 89%|████████▉ | 1835/2065 [1:39:56<11:33,  3.02s/it] 89%|████████▉ | 1836/2065 [1:39:59<11:29,  3.01s/it] 89%|████████▉ | 1837/2065 [1:40:02<11:31,  3.03s/it] 89%|████████▉ | 1838/2065 [1:40:05<11:25,  3.02s/it] 89%|████████▉ | 1839/2065 [1:40:08<11:21,  3.01s/it] 89%|████████▉ | 1840/2065 [1:40:11<11:25,  3.05s/it]                                                     {'loss': 2.2796, 'grad_norm': 0.3429330289363861, 'learning_rate': 6.019067353166386e-07, 'epoch': 4.46}
 89%|████████▉ | 1840/2065 [1:40:11<11:25,  3.05s/it] 89%|████████▉ | 1841/2065 [1:40:14<11:21,  3.04s/it] 89%|████████▉ | 1842/2065 [1:40:17<11:15,  3.03s/it] 89%|████████▉ | 1843/2065 [1:40:20<11:09,  3.01s/it] 89%|████████▉ | 1844/2065 [1:40:23<11:05,  3.01s/it] 89%|████████▉ | 1845/2065 [1:40:26<11:02,  3.01s/it] 89%|████████▉ | 1846/2065 [1:40:29<10:59,  3.01s/it] 89%|████████▉ | 1847/2065 [1:40:32<10:54,  3.00s/it] 89%|████████▉ | 1848/2065 [1:40:35<10:52,  3.01s/it] 90%|████████▉ | 1849/2065 [1:40:38<10:47,  3.00s/it] 90%|████████▉ | 1850/2065 [1:40:41<10:45,  3.00s/it]                                                     {'loss': 2.3001, 'grad_norm': 0.328517884016037, 'learning_rate': 5.505248419085507e-07, 'epoch': 4.48}
 90%|████████▉ | 1850/2065 [1:40:41<10:45,  3.00s/it] 90%|████████▉ | 1851/2065 [1:40:44<10:43,  3.01s/it] 90%|████████▉ | 1852/2065 [1:40:47<10:39,  3.00s/it] 90%|████████▉ | 1853/2065 [1:40:50<10:36,  3.00s/it] 90%|████████▉ | 1854/2065 [1:40:53<10:33,  3.00s/it] 90%|████████▉ | 1855/2065 [1:40:56<10:30,  3.00s/it] 90%|████████▉ | 1856/2065 [1:40:59<10:26,  3.00s/it] 90%|████████▉ | 1857/2065 [1:41:02<10:22,  2.99s/it] 90%|████████▉ | 1858/2065 [1:41:05<10:21,  3.00s/it] 90%|█████████ | 1859/2065 [1:41:08<10:19,  3.01s/it] 90%|█████████ | 1860/2065 [1:41:11<10:16,  3.01s/it]                                                     {'loss': 2.2693, 'grad_norm': 0.3231825828552246, 'learning_rate': 5.013729865192174e-07, 'epoch': 4.5}
 90%|█████████ | 1860/2065 [1:41:11<10:16,  3.01s/it] 90%|█████████ | 1861/2065 [1:41:14<10:11,  3.00s/it] 90%|█████████ | 1862/2065 [1:41:17<10:08,  3.00s/it] 90%|█████████ | 1863/2065 [1:41:20<10:06,  3.00s/it] 90%|█████████ | 1864/2065 [1:41:23<10:03,  3.00s/it] 90%|█████████ | 1865/2065 [1:41:26<09:59,  3.00s/it] 90%|█████████ | 1866/2065 [1:41:29<09:56,  3.00s/it] 90%|█████████ | 1867/2065 [1:41:32<09:54,  3.00s/it] 90%|█████████ | 1868/2065 [1:41:35<09:52,  3.01s/it] 91%|█████████ | 1869/2065 [1:41:38<09:48,  3.00s/it] 91%|█████████ | 1870/2065 [1:41:41<09:45,  3.00s/it]                                                     {'loss': 2.3132, 'grad_norm': 0.30481457710266113, 'learning_rate': 4.5446276878823636e-07, 'epoch': 4.53}
 91%|█████████ | 1870/2065 [1:41:41<09:45,  3.00s/it] 91%|█████████ | 1871/2065 [1:41:44<09:42,  3.00s/it] 91%|█████████ | 1872/2065 [1:41:47<09:40,  3.01s/it] 91%|█████████ | 1873/2065 [1:41:50<09:37,  3.01s/it] 91%|█████████ | 1874/2065 [1:41:53<09:34,  3.01s/it] 91%|█████████ | 1875/2065 [1:41:56<09:30,  3.00s/it] 91%|█████████ | 1876/2065 [1:41:59<09:27,  3.00s/it] 91%|█████████ | 1877/2065 [1:42:02<09:24,  3.00s/it] 91%|█████████ | 1878/2065 [1:42:05<09:21,  3.00s/it] 91%|█████████ | 1879/2065 [1:42:08<09:17,  3.00s/it] 91%|█████████ | 1880/2065 [1:42:11<09:14,  3.00s/it]                                                     {'loss': 2.2916, 'grad_norm': 0.3268755376338959, 'learning_rate': 4.0980525933776105e-07, 'epoch': 4.55}
 91%|█████████ | 1880/2065 [1:42:11<09:14,  3.00s/it] 91%|█████████ | 1881/2065 [1:42:14<09:17,  3.03s/it] 91%|█████████ | 1882/2065 [1:42:17<09:11,  3.01s/it] 91%|█████████ | 1883/2065 [1:42:20<09:11,  3.03s/it] 91%|█████████ | 1884/2065 [1:42:23<09:05,  3.01s/it] 91%|█████████▏| 1885/2065 [1:42:26<09:02,  3.01s/it] 91%|█████████▏| 1886/2065 [1:42:29<08:59,  3.01s/it] 91%|█████████▏| 1887/2065 [1:42:32<08:54,  3.00s/it] 91%|█████████▏| 1888/2065 [1:42:35<08:53,  3.02s/it] 91%|█████████▏| 1889/2065 [1:42:38<08:49,  3.01s/it] 92%|█████████▏| 1890/2065 [1:42:41<08:45,  3.00s/it]                                                     {'loss': 2.266, 'grad_norm': 0.3257526755332947, 'learning_rate': 3.6741099715987604e-07, 'epoch': 4.58}
 92%|█████████▏| 1890/2065 [1:42:41<08:45,  3.00s/it] 92%|█████████▏| 1891/2065 [1:42:44<08:42,  3.00s/it] 92%|█████████▏| 1892/2065 [1:42:47<08:40,  3.01s/it] 92%|█████████▏| 1893/2065 [1:42:50<08:36,  3.01s/it] 92%|█████████▏| 1894/2065 [1:42:53<08:33,  3.00s/it] 92%|█████████▏| 1895/2065 [1:42:56<08:29,  3.00s/it] 92%|█████████▏| 1896/2065 [1:42:59<08:26,  3.00s/it] 92%|█████████▏| 1897/2065 [1:43:02<08:23,  3.00s/it] 92%|█████████▏| 1898/2065 [1:43:05<08:20,  3.00s/it] 92%|█████████▏| 1899/2065 [1:43:08<08:17,  3.00s/it] 92%|█████████▏| 1900/2065 [1:43:11<08:15,  3.00s/it]                                                     {'loss': 2.2632, 'grad_norm': 0.31387513875961304, 'learning_rate': 3.272899871294366e-07, 'epoch': 4.6}
 92%|█████████▏| 1900/2065 [1:43:11<08:15,  3.00s/it][INFO|trainer.py:3512] 2024-06-25 00:07:44,251 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-25 00:07:44,251 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-25 00:07:44,251 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.35it/s][A
 27%|██▋       | 3/11 [00:02<00:08,  1.05s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.21s/it][A
 45%|████▌     | 5/11 [00:05<00:07,  1.31s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.37s/it][A
 64%|██████▎   | 7/11 [00:08<00:05,  1.41s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.43s/it][A
 82%|████████▏ | 9/11 [00:11<00:02,  1.45s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.46s/it][A
100%|██████████| 11/11 [00:14<00:00,  1.47s/it][A                                                     
                                               [A{'eval_loss': 2.261643886566162, 'eval_runtime': 16.3701, 'eval_samples_per_second': 15.944, 'eval_steps_per_second': 0.672, 'epoch': 4.6}
 92%|█████████▏| 1900/2065 [1:43:28<08:15,  3.00s/it]
100%|██████████| 11/11 [00:15<00:00,  1.47s/it][A
                                               [A 92%|█████████▏| 1901/2065 [1:43:31<21:39,  7.93s/it] 92%|█████████▏| 1902/2065 [1:43:34<17:30,  6.44s/it] 92%|█████████▏| 1903/2065 [1:43:37<14:36,  5.41s/it] 92%|█████████▏| 1904/2065 [1:43:40<12:34,  4.69s/it] 92%|█████████▏| 1905/2065 [1:43:43<11:09,  4.18s/it] 92%|█████████▏| 1906/2065 [1:43:46<10:08,  3.82s/it] 92%|█████████▏| 1907/2065 [1:43:49<09:24,  3.57s/it] 92%|█████████▏| 1908/2065 [1:43:52<08:57,  3.42s/it] 92%|█████████▏| 1909/2065 [1:43:55<08:34,  3.30s/it] 92%|█████████▏| 1910/2065 [1:43:58<08:16,  3.21s/it]                                                     {'loss': 2.261, 'grad_norm': 0.33128872513771057, 'learning_rate': 2.894516976429673e-07, 'epoch': 4.62}
 92%|█████████▏| 1910/2065 [1:43:58<08:16,  3.21s/it] 93%|█████████▎| 1911/2065 [1:44:01<08:05,  3.15s/it] 93%|█████████▎| 1912/2065 [1:44:04<07:54,  3.10s/it] 93%|█████████▎| 1913/2065 [1:44:07<07:48,  3.08s/it] 93%|█████████▎| 1914/2065 [1:44:10<07:41,  3.06s/it] 93%|█████████▎| 1915/2065 [1:44:13<07:36,  3.04s/it] 93%|█████████▎| 1916/2065 [1:44:16<07:36,  3.06s/it] 93%|█████████▎| 1917/2065 [1:44:19<07:30,  3.04s/it] 93%|█████████▎| 1918/2065 [1:44:22<07:25,  3.03s/it] 93%|█████████▎| 1919/2065 [1:44:25<07:20,  3.02s/it] 93%|█████████▎| 1920/2065 [1:44:28<07:16,  3.01s/it]                                                     {'loss': 2.2736, 'grad_norm': 0.31021177768707275, 'learning_rate': 2.539050583841485e-07, 'epoch': 4.65}
 93%|█████████▎| 1920/2065 [1:44:28<07:16,  3.01s/it] 93%|█████████▎| 1921/2065 [1:44:31<07:14,  3.02s/it] 93%|█████████▎| 1922/2065 [1:44:34<07:11,  3.02s/it] 93%|█████████▎| 1923/2065 [1:44:37<07:07,  3.01s/it] 93%|█████████▎| 1924/2065 [1:44:40<07:04,  3.01s/it] 93%|█████████▎| 1925/2065 [1:44:43<07:00,  3.01s/it] 93%|█████████▎| 1926/2065 [1:44:46<06:58,  3.01s/it] 93%|█████████▎| 1927/2065 [1:44:49<06:54,  3.00s/it] 93%|█████████▎| 1928/2065 [1:44:52<06:52,  3.01s/it] 93%|█████████▎| 1929/2065 [1:44:55<06:49,  3.01s/it] 93%|█████████▎| 1930/2065 [1:44:58<06:46,  3.01s/it]                                                     {'loss': 2.2692, 'grad_norm': 0.3097880482673645, 'learning_rate': 2.2065845821645393e-07, 'epoch': 4.67}
 93%|█████████▎| 1930/2065 [1:44:58<06:46,  3.01s/it] 94%|█████████▎| 1931/2065 [1:45:01<06:42,  3.01s/it] 94%|█████████▎| 1932/2065 [1:45:04<06:39,  3.01s/it] 94%|█████████▎| 1933/2065 [1:45:07<06:37,  3.01s/it] 94%|█████████▎| 1934/2065 [1:45:10<06:33,  3.00s/it] 94%|█████████▎| 1935/2065 [1:45:13<06:30,  3.00s/it] 94%|█████████▍| 1936/2065 [1:45:16<06:26,  3.00s/it] 94%|█████████▍| 1937/2065 [1:45:19<06:27,  3.02s/it] 94%|█████████▍| 1938/2065 [1:45:22<06:24,  3.03s/it] 94%|█████████▍| 1939/2065 [1:45:25<06:21,  3.02s/it] 94%|█████████▍| 1940/2065 [1:45:28<06:17,  3.02s/it]                                                     {'loss': 2.285, 'grad_norm': 0.3247632086277008, 'learning_rate': 1.8971974320340925e-07, 'epoch': 4.7}
 94%|█████████▍| 1940/2065 [1:45:28<06:17,  3.02s/it] 94%|█████████▍| 1941/2065 [1:45:31<06:13,  3.01s/it] 94%|█████████▍| 1942/2065 [1:45:34<06:09,  3.00s/it] 94%|█████████▍| 1943/2065 [1:45:37<06:05,  3.00s/it] 94%|█████████▍| 1944/2065 [1:45:40<06:02,  3.00s/it] 94%|█████████▍| 1945/2065 [1:45:43<06:00,  3.01s/it] 94%|█████████▍| 1946/2065 [1:45:46<05:57,  3.00s/it] 94%|█████████▍| 1947/2065 [1:45:49<05:54,  3.01s/it] 94%|█████████▍| 1948/2065 [1:45:52<05:51,  3.00s/it] 94%|█████████▍| 1949/2065 [1:45:55<05:48,  3.00s/it] 94%|█████████▍| 1950/2065 [1:45:58<05:45,  3.01s/it]                                                     {'loss': 2.257, 'grad_norm': 0.31754711270332336, 'learning_rate': 1.6109621475695058e-07, 'epoch': 4.72}
 94%|█████████▍| 1950/2065 [1:45:58<05:45,  3.01s/it][INFO|trainer.py:3203] 2024-06-25 00:10:31,107 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1950
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63cb5c15d0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 86135d33-b476-42f9-b5e4-edeeee4ba6ba)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-25 00:10:41,170 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1950/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-25 00:10:41,172 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/checkpoint-1950/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
 94%|█████████▍| 1951/2065 [1:46:11<11:22,  5.98s/it] 95%|█████████▍| 1952/2065 [1:46:14<09:26,  5.01s/it] 95%|█████████▍| 1953/2065 [1:46:17<08:08,  4.36s/it] 95%|█████████▍| 1954/2065 [1:46:19<07:13,  3.91s/it] 95%|█████████▍| 1955/2065 [1:46:22<06:36,  3.60s/it] 95%|█████████▍| 1956/2065 [1:46:25<06:10,  3.40s/it] 95%|█████████▍| 1957/2065 [1:46:28<05:54,  3.28s/it] 95%|█████████▍| 1958/2065 [1:46:31<05:39,  3.17s/it] 95%|█████████▍| 1959/2065 [1:46:34<05:28,  3.10s/it] 95%|█████████▍| 1960/2065 [1:46:37<05:20,  3.06s/it]                                                     {'loss': 2.2034, 'grad_norm': 0.3176899254322052, 'learning_rate': 1.347946279143253e-07, 'epoch': 4.75}
 95%|█████████▍| 1960/2065 [1:46:37<05:20,  3.06s/it] 95%|█████████▍| 1961/2065 [1:46:40<05:14,  3.02s/it] 95%|█████████▌| 1962/2065 [1:46:43<05:08,  3.00s/it] 95%|█████████▌| 1963/2065 [1:46:46<05:04,  2.99s/it] 95%|█████████▌| 1964/2065 [1:46:49<05:01,  2.98s/it] 95%|█████████▌| 1965/2065 [1:46:52<04:57,  2.97s/it] 95%|█████████▌| 1966/2065 [1:46:55<04:54,  2.97s/it] 95%|█████████▌| 1967/2065 [1:46:58<04:50,  2.97s/it] 95%|█████████▌| 1968/2065 [1:47:01<04:48,  2.97s/it] 95%|█████████▌| 1969/2065 [1:47:04<04:45,  2.98s/it] 95%|█████████▌| 1970/2065 [1:47:07<04:42,  2.97s/it]                                                     {'loss': 2.3282, 'grad_norm': 0.31816455721855164, 'learning_rate': 1.1082118974393263e-07, 'epoch': 4.77}
 95%|█████████▌| 1970/2065 [1:47:07<04:42,  2.97s/it] 95%|█████████▌| 1971/2065 [1:47:10<04:39,  2.98s/it] 95%|█████████▌| 1972/2065 [1:47:13<04:37,  2.98s/it] 96%|█████████▌| 1973/2065 [1:47:16<04:34,  2.98s/it] 96%|█████████▌| 1974/2065 [1:47:19<04:30,  2.98s/it] 96%|█████████▌| 1975/2065 [1:47:22<04:27,  2.98s/it] 96%|█████████▌| 1976/2065 [1:47:24<04:25,  2.98s/it] 96%|█████████▌| 1977/2065 [1:47:27<04:22,  2.98s/it] 96%|█████████▌| 1978/2065 [1:47:30<04:19,  2.98s/it] 96%|█████████▌| 1979/2065 [1:47:33<04:16,  2.98s/it] 96%|█████████▌| 1980/2065 [1:47:36<04:14,  2.99s/it]                                                     {'loss': 2.3161, 'grad_norm': 0.3427845537662506, 'learning_rate': 8.918155788047e-08, 'epoch': 4.79}
 96%|█████████▌| 1980/2065 [1:47:36<04:14,  2.99s/it] 96%|█████████▌| 1981/2065 [1:47:39<04:10,  2.99s/it] 96%|█████████▌| 1982/2065 [1:47:42<04:08,  2.99s/it] 96%|█████████▌| 1983/2065 [1:47:45<04:05,  2.99s/it] 96%|█████████▌| 1984/2065 [1:47:48<04:02,  2.99s/it] 96%|█████████▌| 1985/2065 [1:47:51<03:59,  2.99s/it] 96%|█████████▌| 1986/2065 [1:47:54<03:55,  2.99s/it] 96%|█████████▌| 1987/2065 [1:47:57<03:53,  2.99s/it] 96%|█████████▋| 1988/2065 [1:48:00<03:49,  2.99s/it] 96%|█████████▋| 1989/2065 [1:48:03<03:46,  2.98s/it] 96%|█████████▋| 1990/2065 [1:48:06<03:44,  2.99s/it]                                                     {'loss': 2.2799, 'grad_norm': 0.3297625184059143, 'learning_rate': 6.988083918976762e-08, 'epoch': 4.82}
 96%|█████████▋| 1990/2065 [1:48:06<03:44,  2.99s/it] 96%|█████████▋| 1991/2065 [1:48:09<03:41,  2.99s/it] 96%|█████████▋| 1992/2065 [1:48:12<03:37,  2.98s/it] 97%|█████████▋| 1993/2065 [1:48:15<03:34,  2.99s/it] 97%|█████████▋| 1994/2065 [1:48:18<03:32,  2.99s/it] 97%|█████████▋| 1995/2065 [1:48:21<03:30,  3.01s/it] 97%|█████████▋| 1996/2065 [1:48:24<03:27,  3.00s/it] 97%|█████████▋| 1997/2065 [1:48:27<03:23,  3.00s/it] 97%|█████████▋| 1998/2065 [1:48:30<03:20,  3.00s/it] 97%|█████████▋| 1999/2065 [1:48:33<03:17,  2.99s/it] 97%|█████████▋| 2000/2065 [1:48:36<03:14,  2.99s/it]                                                     {'loss': 2.2267, 'grad_norm': 0.327665239572525, 'learning_rate': 5.292358856357704e-08, 'epoch': 4.84}
 97%|█████████▋| 2000/2065 [1:48:36<03:14,  2.99s/it][INFO|trainer.py:3512] 2024-06-25 00:13:09,393 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-25 00:13:09,393 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-25 00:13:09,393 >>   Batch size = 4

  0%|          | 0/11 [00:00<?, ?it/s][A
 18%|█▊        | 2/11 [00:01<00:06,  1.36it/s][A
 27%|██▋       | 3/11 [00:02<00:08,  1.05s/it][A
 36%|███▋      | 4/11 [00:04<00:08,  1.22s/it][A
 45%|████▌     | 5/11 [00:05<00:07,  1.31s/it][A
 55%|█████▍    | 6/11 [00:07<00:06,  1.37s/it][A
 64%|██████▎   | 7/11 [00:08<00:05,  1.41s/it][A
 73%|███████▎  | 8/11 [00:10<00:04,  1.44s/it][A
 82%|████████▏ | 9/11 [00:11<00:02,  1.46s/it][A
 91%|█████████ | 10/11 [00:13<00:01,  1.47s/it][A
100%|██████████| 11/11 [00:14<00:00,  1.47s/it][A                                                     
                                               [A{'eval_loss': 2.261593818664551, 'eval_runtime': 16.3928, 'eval_samples_per_second': 15.922, 'eval_steps_per_second': 0.671, 'epoch': 4.84}
 97%|█████████▋| 2000/2065 [1:48:53<03:14,  2.99s/it]
100%|██████████| 11/11 [00:15<00:00,  1.47s/it][A
                                               [A 97%|█████████▋| 2001/2065 [1:48:56<08:26,  7.91s/it] 97%|█████████▋| 2002/2065 [1:48:59<06:45,  6.43s/it] 97%|█████████▋| 2003/2065 [1:49:02<05:34,  5.40s/it] 97%|█████████▋| 2004/2065 [1:49:05<04:44,  4.67s/it] 97%|█████████▋| 2005/2065 [1:49:08<04:09,  4.16s/it] 97%|█████████▋| 2006/2065 [1:49:11<03:44,  3.81s/it] 97%|█████████▋| 2007/2065 [1:49:14<03:26,  3.56s/it] 97%|█████████▋| 2008/2065 [1:49:17<03:14,  3.42s/it] 97%|█████████▋| 2009/2065 [1:49:20<03:04,  3.29s/it] 97%|█████████▋| 2010/2065 [1:49:23<02:55,  3.20s/it]                                                     {'loss': 2.3068, 'grad_norm': 0.31498849391937256, 'learning_rate': 3.8313807844641e-08, 'epoch': 4.87}
 97%|█████████▋| 2010/2065 [1:49:23<02:55,  3.20s/it] 97%|█████████▋| 2011/2065 [1:49:26<02:51,  3.17s/it] 97%|█████████▋| 2012/2065 [1:49:29<02:44,  3.11s/it] 97%|█████████▋| 2013/2065 [1:49:32<02:39,  3.07s/it] 98%|█████████▊| 2014/2065 [1:49:35<02:35,  3.05s/it] 98%|█████████▊| 2015/2065 [1:49:38<02:31,  3.03s/it] 98%|█████████▊| 2016/2065 [1:49:41<02:27,  3.02s/it] 98%|█████████▊| 2017/2065 [1:49:44<02:24,  3.01s/it] 98%|█████████▊| 2018/2065 [1:49:47<02:21,  3.00s/it] 98%|█████████▊| 2019/2065 [1:49:50<02:17,  3.00s/it] 98%|█████████▊| 2020/2065 [1:49:53<02:15,  3.01s/it]                                                     {'loss': 2.2746, 'grad_norm': 0.32366785407066345, 'learning_rate': 2.6054944882275556e-08, 'epoch': 4.89}
 98%|█████████▊| 2020/2065 [1:49:53<02:15,  3.01s/it] 98%|█████████▊| 2021/2065 [1:49:56<02:11,  3.00s/it] 98%|█████████▊| 2022/2065 [1:49:59<02:08,  2.99s/it] 98%|█████████▊| 2023/2065 [1:50:02<02:05,  2.99s/it] 98%|█████████▊| 2024/2065 [1:50:05<02:03,  3.00s/it] 98%|█████████▊| 2025/2065 [1:50:08<01:59,  2.99s/it] 98%|█████████▊| 2026/2065 [1:50:11<01:56,  2.99s/it] 98%|█████████▊| 2027/2065 [1:50:14<01:53,  2.99s/it] 98%|█████████▊| 2028/2065 [1:50:17<01:50,  2.99s/it] 98%|█████████▊| 2029/2065 [1:50:19<01:47,  2.98s/it] 98%|█████████▊| 2030/2065 [1:50:22<01:44,  2.99s/it]                                                     {'loss': 2.2899, 'grad_norm': 0.3230664134025574, 'learning_rate': 1.61498927186865e-08, 'epoch': 4.92}
 98%|█████████▊| 2030/2065 [1:50:22<01:44,  2.99s/it] 98%|█████████▊| 2031/2065 [1:50:25<01:41,  2.98s/it] 98%|█████████▊| 2032/2065 [1:50:28<01:38,  2.99s/it] 98%|█████████▊| 2033/2065 [1:50:31<01:35,  2.99s/it] 98%|█████████▊| 2034/2065 [1:50:34<01:32,  3.00s/it] 99%|█████████▊| 2035/2065 [1:50:38<01:30,  3.02s/it] 99%|█████████▊| 2036/2065 [1:50:41<01:27,  3.01s/it] 99%|█████████▊| 2037/2065 [1:50:44<01:23,  3.00s/it] 99%|█████████▊| 2038/2065 [1:50:46<01:20,  2.99s/it] 99%|█████████▊| 2039/2065 [1:50:49<01:17,  2.99s/it] 99%|█████████▉| 2040/2065 [1:50:52<01:14,  2.99s/it]                                                     {'loss': 2.3075, 'grad_norm': 0.30106276273727417, 'learning_rate': 8.600988906231067e-09, 'epoch': 4.94}
 99%|█████████▉| 2040/2065 [1:50:52<01:14,  2.99s/it] 99%|█████████▉| 2041/2065 [1:50:56<01:12,  3.01s/it] 99%|█████████▉| 2042/2065 [1:50:58<01:09,  3.00s/it] 99%|█████████▉| 2043/2065 [1:51:01<01:05,  3.00s/it] 99%|█████████▉| 2044/2065 [1:51:04<01:02,  2.99s/it] 99%|█████████▉| 2045/2065 [1:51:07<00:59,  2.99s/it] 99%|█████████▉| 2046/2065 [1:51:10<00:56,  2.99s/it] 99%|█████████▉| 2047/2065 [1:51:13<00:53,  2.98s/it] 99%|█████████▉| 2048/2065 [1:51:16<00:50,  2.99s/it] 99%|█████████▉| 2049/2065 [1:51:19<00:47,  2.99s/it] 99%|█████████▉| 2050/2065 [1:51:22<00:44,  2.98s/it]                                                     {'loss': 2.305, 'grad_norm': 0.2932879328727722, 'learning_rate': 3.41001495575477e-09, 'epoch': 4.96}
 99%|█████████▉| 2050/2065 [1:51:22<00:44,  2.98s/it] 99%|█████████▉| 2051/2065 [1:51:25<00:41,  2.98s/it] 99%|█████████▉| 2052/2065 [1:51:28<00:38,  2.98s/it] 99%|█████████▉| 2053/2065 [1:51:31<00:35,  2.98s/it] 99%|█████████▉| 2054/2065 [1:51:34<00:32,  2.98s/it]100%|█████████▉| 2055/2065 [1:51:37<00:29,  2.98s/it]100%|█████████▉| 2056/2065 [1:51:40<00:26,  2.98s/it]100%|█████████▉| 2057/2065 [1:51:43<00:23,  2.98s/it]100%|█████████▉| 2058/2065 [1:51:46<00:20,  2.98s/it]100%|█████████▉| 2059/2065 [1:51:49<00:17,  2.98s/it]100%|█████████▉| 2060/2065 [1:51:52<00:14,  2.98s/it]                                                     {'loss': 2.2996, 'grad_norm': 0.3349313735961914, 'learning_rate': 5.781959161710316e-10, 'epoch': 4.99}
100%|█████████▉| 2060/2065 [1:51:52<00:14,  2.98s/it]100%|█████████▉| 2061/2065 [1:51:55<00:11,  2.98s/it]100%|█████████▉| 2062/2065 [1:51:58<00:08,  2.98s/it]100%|█████████▉| 2063/2065 [1:52:01<00:05,  2.98s/it]100%|█████████▉| 2064/2065 [1:52:04<00:02,  2.98s/it]100%|██████████| 2065/2065 [1:52:07<00:00,  2.98s/it][INFO|trainer.py:2231] 2024-06-25 00:16:40,155 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 6727.5923, 'train_samples_per_second': 3.68, 'train_steps_per_second': 0.307, 'train_loss': 2.3135273062865322, 'epoch': 5.0}
100%|██████████| 2065/2065 [1:52:07<00:00,  2.98s/it]100%|██████████| 2065/2065 [1:52:07<00:00,  3.26s/it]
[INFO|trainer.py:3203] 2024-06-25 00:16:40,162 >> Saving model checkpoint to LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /Yi-9B/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x2b63cb4b2c90>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 30354073-92c4-4d46-b179-a7dcd87972d7)') - silently ignoring the lookup for the file config.json in Yi-9B.
  warnings.warn(
/HOME/scz4074/.conda/envs/fine-tuning-LLM/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in Yi-9B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-06-25 00:16:50,221 >> tokenizer config file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-06-25 00:16:50,223 >> Special tokens file saved in LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/special_tokens_map.json
The OrderedVocab you are attempting to save contains holes for indices [3, 4, 5, 9, 10, 11, 12, 13], your vocabulary could be corrupted !
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     2.3135
  train_runtime            = 1:52:07.59
  train_samples_per_second =       3.68
  train_steps_per_second   =      0.307
Figure saved at: LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/training_loss.png
Figure saved at: LLaMA-Factory/LLaMA-Factory-main/saves/Yi-9B/post-pt-1/training_eval_loss.png
[INFO|trainer.py:3512] 2024-06-25 00:16:57,391 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-06-25 00:16:57,391 >>   Num examples = 261
[INFO|trainer.py:3517] 2024-06-25 00:16:57,391 >>   Batch size = 4
  0%|          | 0/11 [00:00<?, ?it/s] 18%|█▊        | 2/11 [00:01<00:05,  1.51it/s] 27%|██▋       | 3/11 [00:02<00:07,  1.06it/s] 36%|███▋      | 4/11 [00:04<00:07,  1.09s/it] 45%|████▌     | 5/11 [00:05<00:07,  1.18s/it] 55%|█████▍    | 6/11 [00:06<00:06,  1.24s/it] 64%|██████▎   | 7/11 [00:08<00:05,  1.27s/it] 73%|███████▎  | 8/11 [00:09<00:03,  1.30s/it] 82%|████████▏ | 9/11 [00:10<00:02,  1.32s/it] 91%|█████████ | 10/11 [00:12<00:01,  1.34s/it]100%|██████████| 11/11 [00:13<00:00,  1.35s/it]100%|██████████| 11/11 [00:14<00:00,  1.28s/it]
***** eval metrics *****
  epoch                   =        5.0
  eval_loss               =     2.2616
  eval_runtime            = 0:00:14.94
  eval_samples_per_second =     17.464
  eval_steps_per_second   =      0.736
  perplexity              =     9.5985
[INFO|modelcard.py:450] 2024-06-25 00:17:12,346 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
