## 微调Yi-9B模型增强模型购物概念理解能力
### 0. 实验基本说明
本次实验是深度学习实验大作业，团队成员分别为：
- 陈鸿绪 （负责GPT生成数据异步并发实现、构建微调数据集与数据处理、模型训练微调与测试、实验报告撰写）
- 黄祈铭 （负责生成GPT数据、构建微调数据集、数据处理、实验报告撰写）
- 林博文 （构建微调数据集、数据处理、实验报告撰写）

我们团队的选题为"Amazon KDD Cup 24: Understanding Shopping Concepts"，由于我们使用了9B的大模型Yi-9B，单个样本推理（大致16GB）加上模型所占显存（大概18GB）在2张T4显卡（32GB显存）在不采取量化操作的情况下明显会OOM，然而量化会导致模型性能的下降，所以本次实验采取自行划分数据集与任务类型进行评测。
### 1. 项目文件夹说明
- bash：存放linux系统训练、测试、导出模型的.sh脚本
- data：存放post-pre-train数据集（三个不同语料数据集下载地址），sft数据集（GPT生成数据集代码与huggingface数据集下载地址），测试集合（包括6种不同大类型任务）
- fig：训练时的测试集与验证集loss图像
- LLaMA-Factory：微调框架
- report：任务报告
- slurm-out：提交作业后的计算节点输出
- test-result：测试输出结果处理，包含共6种大类型任务与原模型的评测比较。
### 2. 实验过程简单说明
1. 构建数据集：分为post-pre-trainning的数据集与SFT数据集，两种数据集的组成在文件夹中可见。
2. 数据处理：这部分包括了数据去空、SFT数据构建、数据筛查、输出规范化、数据集划分、数据标注等操作。
3. 训练微调：使用相应的数据集进行两步训练，分别为post pre-trainning（特定领域再次lora预训练以学习相关知识）与self-supervised trainnning（对于一些特定下游任务进行lora微调），我们使用了6张3090显卡、利用LLaMa-Factory框架进行了分布式微调。
4. 模型测试：对于不同的任务类型我们将采用不同的指标进行评测，这里还需要对于大模型输出需要进行基于规则式的答案提取。

### 3. 微调后模型性能与微调前的性能比较


### Huggingface
我们的模型已经上传至huggingFace：
[Model URL](https://huggingface.co/Daxuxu36/Yi-9B-Understanding-Shopping-Concepts)